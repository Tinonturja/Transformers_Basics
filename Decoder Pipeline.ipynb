{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dadaa06c-70fc-4cc0-8a29-b641052b7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.datasets import multi30k,Multi30k\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.datasets import IMDB,PennTreebank\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa9e745-a9d6-4959-a26b-a3458bfbfe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "train_iter, test_iter = IMDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a937360-069b-450d-9232-e90f35cc15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label,sample = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9626d762-4915-47c4-b773-8dce592879d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label,sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e8397d-5695-4c02-b07c-3f7ad8ee5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list =([label for label,_ in train_iter])\n",
    "text_list = ([text for _,text in train_iter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da1e7fe-9975-4626-952a-163462e2ebb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac821e3b-2af2-4d70-9484-9af971f462fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa66eb9-a850-456a-8f70-0f5c9b7e1e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sorry, I just didn\\'t find the subject matter as compelling as the filmmaker did. The robot guy and the mole rat guy were pretty interesting, although Morris didn\\'t really tell us much about them. The other two subjects were a bore. And the supposed \"connections\" between them didn\\'t hold up.',\n",
       " 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0,len(text_list))\n",
    "\n",
    "text_list[idx],label_list[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662332be-c0be-4593-9f39-cea907cdd7c0",
   "metadata": {},
   "source": [
    "The data has 12500 test sample data. Here each one is labelled as one. So each movie got a positive review somehow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee633eed-98d4-4ade-aafb-01ba48f1bab3",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6722de-e6cb-4c73-99bc-4c19731247c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, EOS_IDX = 0,1,2\n",
    "special_symbols = ['<unk>','<pad>','<|endoftext|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82555d7f-a1fa-4178-8f3d-b80607220c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_token(dataset):\n",
    "    for _,text in dataset:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Create the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_token(train_iter), specials = special_symbols,special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e5c863-e48e-4453-a190-5ee95c297e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to index, Index to text\n",
    "\n",
    "# input--> token; output--> index\n",
    "text_to_idx = lambda text: [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "# input --> index; output--> token\n",
    "idx_to_text = lambda seq_en: \" \".join([vocab.get_itos()[idx] for idx in seq_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9271315a-a236-42c9-a2de-5efb9359381c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it t ! or has my been'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([12,24,36,48,60,72,84],dtype = torch.int64)\n",
    "#index_to_text = idx_to_text([12,24,36,48,60,72,84])\n",
    "index_to_text = idx_to_text(a)\n",
    "\n",
    "index_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa0f1cce-a605-4cf9-8108-c291bb86fa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<|endoftext|>', '.', 'the', ',', 'a', 'and', \"'\", 'of']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = list(vocab.get_itos())[:10]\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0546cc3e-2ee1-4695-a64a-b9fa2cbf2af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> <pad> <|endoftext|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = idx_to_text(torch.tensor([0,1,2]))\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f2ec0-f865-474f-9143-4bfe01c0e853",
   "metadata": {},
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ffc46-1041-4764-b7cf-21f18fa8328a",
   "metadata": {},
   "source": [
    "Collate function shapes how the dataloader perceives the data. To pass the data through the model each string should have equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d72b192-5202-4d74-8b3b-569e7e1c4878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(block_size, text):\n",
    "    \"\"\"The goal of the function is to generate a training sample. The input and the target dataset. It's done because the model learns from the input\n",
    "    and tries to predict the output.\n",
    "    \n",
    "    Parameters(block_size,text):\n",
    "        block_size: actually indicates the context size. At how many tokens the model can look at once. \n",
    "        text: the full dataset. A list of long tokens\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    src_seq,trg_seq = [],[]\n",
    "    sample_len = len(text) # gives the length of the total length of the input\n",
    "\n",
    "    # Calculating the stopping point for randomly selecting a sample\n",
    "    # This ensures the selected sample doesn't exceed the text\n",
    "    random_sample_stop = sample_len - block_size\n",
    "\n",
    "    # random sampling should be starting in this limit\n",
    "\n",
    "    if random_sample_stop >=1:\n",
    "\n",
    "        # pick the starting point\n",
    "        random_start = torch.randint(0,random_sample_stop, size = (1,)).item()\n",
    "\n",
    "        stop = random_start + block_size\n",
    "\n",
    "        # Create the input and the target sequence\n",
    "        src_seq = text[random_start:stop]\n",
    "\n",
    "        trg_seq = text[random_start+1:stop+1]\n",
    "\n",
    "\n",
    "\n",
    "    # Handle the case where the text sequence is exactly same as the context size\n",
    "    elif random_sample_stop <= 0:\n",
    "\n",
    "        random_start = 0\n",
    "        stop = sample_len\n",
    "\n",
    "        src_seq = text[random_start:stop]\n",
    "\n",
    "        trg_seq = text[random_start+1:stop]\n",
    "\n",
    "        # Append an empty string to maintain the sequence alignment\n",
    "        trg_seq.append('<|endoftext|>')\n",
    "\n",
    "    return src_seq,trg_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72929ad2-160c-4362-aef2-060bb53332be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the training data tokens into a list\n",
    "BATCH_SIZE = 1 # for each batch its return a list of tokens which could be used for training\n",
    "\n",
    "batch_of_tokens = []\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    label,text = next(iter(train_iter))\n",
    "    batch_of_tokens.append(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "619092a1-d470-4290-9db2-4c31f884a760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_of_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4847764-4b6b-4b43-8268-4a1c5f868fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how is the input and the output sequence would be feed to the training model\n",
    "\n",
    "# create the whole dataset for training of 100 tokens\n",
    "text = batch_of_tokens[0][0:100]\n",
    "block_size = 10\n",
    "\n",
    "src_seq, trg_seq = get_sample(block_size,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1104c172-bacc-41df-a368-cc482ef3ecc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['plot',\n",
       "  'is',\n",
       "  'centered',\n",
       "  'around',\n",
       "  'a',\n",
       "  'young',\n",
       "  'swedish',\n",
       "  'drama',\n",
       "  'student',\n",
       "  'named'],\n",
       " ['is',\n",
       "  'centered',\n",
       "  'around',\n",
       "  'a',\n",
       "  'young',\n",
       "  'swedish',\n",
       "  'drama',\n",
       "  'student',\n",
       "  'named',\n",
       "  'lena'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_seq,trg_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb8e5eb3-14da-4e2e-8b37-58512fe69925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 11, 5505, 191, 6, 266, 3994, 615, 1272, 831],\n",
       " [11, 5505, 191, 6, 266, 3994, 615, 1272, 831, 6788])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(src_seq),vocab(trg_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44993ecf-7e07-4db9-a83a-d87c34121c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 0\n",
      "Source Sequence (Text): ['u', '.', 's', '.', 'customs', 'if', 'it', 'ever', 'tried', 'to']\n",
      "Source Sequence (Indices): [1466, 3, 17, 3, 11063, 51, 12, 124, 608, 10]\n",
      "Source Sequence (Shape): 10\n",
      "Target Sequence (Text): ['.', 's', '.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter']\n",
      "Target Sequence (Indices): [3, 17, 3, 11063, 51, 12, 124, 608, 10, 2636]\n",
      "Target Sequence (Shape): 10\n",
      "Sample: 1\n",
      "Source Sequence (Text): ['sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes']\n",
      "Source Sequence (Indices): [338, 693, 14, 4, 25, 11, 693, 20, 1668, 4919]\n",
      "Source Sequence (Shape): 10\n",
      "Target Sequence (Text): ['shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather']\n",
      "Target Sequence (Indices): [693, 14, 4, 25, 11, 693, 20, 1668, 4919, 253]\n",
      "Target Sequence (Shape): 10\n"
     ]
    }
   ],
   "source": [
    "src_batch,trg_batch = [],[]\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 2\n",
    "block_size = 10\n",
    "# Loop to create src batch and target batch\n",
    "for i in range(BATCH_SIZE):\n",
    "    _,text = next(iter(train_iter)) # Get the first sample every time, as we convert the train data iterable in every turn\n",
    "\n",
    "\n",
    "    # turn the text into tokenizer and then send them\n",
    "    text_tokens = tokenizer(text)\n",
    "\n",
    "    # Generate source and target tokens\n",
    "    src_tokens,trg_tokens = get_sample(block_size,text_tokens) # block_size is previously declared; block_size = 10\n",
    "\n",
    "    # Get the indices of that tokens\n",
    "    src_indices,trg_indices = vocab(src_tokens),vocab(trg_tokens)\n",
    "\n",
    "    # Turn the indices into tensors\n",
    "    src_sequence = torch.tensor(src_indices)\n",
    "    trg_sequence = torch.tensor(trg_indices)\n",
    "    \n",
    "\n",
    "    # print the output \n",
    "    print(f\"Sample: {i}\")\n",
    "    print(f\"Source Sequence (Text): {src_tokens}\")\n",
    "    print(f\"Source Sequence (Indices): {src_indices}\")\n",
    "    print(f\"Source Sequence (Shape): {len(src_tokens)}\")\n",
    "    print(f\"Target Sequence (Text): {trg_tokens}\")\n",
    "    print(f\"Target Sequence (Indices): {trg_indices}\")\n",
    "    print(f\"Target Sequence (Shape): {len(trg_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ec95a-2697-4e7c-8b67-13b70eb86544",
   "metadata": {},
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d23e5385-6202-47f9-87f2-1d99fb8e9f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'mps' if torch.backends.mps.is_available() else 'cuda'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3a584cb-4e9c-4510-8e90-7414129eb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 30\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch,trg_batch = [],[]\n",
    "\n",
    "    for _,text in batch:\n",
    "\n",
    "        tokens = tokenizer(text)\n",
    "\n",
    "        src_tokens,trg_tokens = get_sample(BLOCK_SIZE,tokens)\n",
    "\n",
    "        src_indices,trg_indices = vocab(src_tokens),vocab(trg_tokens)\n",
    "\n",
    "        src_seq,trg_seq = torch.tensor(src_indices,dtype = torch.int64),torch.tensor(trg_indices,dtype = torch.int64)\n",
    "\n",
    "        src_batch.append(src_seq)\n",
    "        trg_batch.append(trg_seq)\n",
    "\n",
    "\n",
    "    src_batch = pad_sequence(src_batch,padding_value = PAD_IDX, batch_first = False)\n",
    "    trg_batch = pad_sequence(trg_batch,padding_value = PAD_IDX, batch_first = False)\n",
    "\n",
    "    return src_batch.to(DEVICE),trg_batch.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd57b2-30df-40b9-80ac-b856a92d5434",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dcb59e1-e6ce-4a85-bcb5-d09047012cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "train_dataloader = DataLoader(train_iter,\n",
    "                       batch_size = BATCH_SIZE,\n",
    "                       shuffle = True,\n",
    "                       collate_fn = collate_batch) # collate batch will define how the data will be retuerned\n",
    "\n",
    "test_dataloader = DataLoader(test_iter,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            shuffle = True,\n",
    "                            collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9032cc-89e4-47ea-bdc1-b48da0da1a93",
   "metadata": {},
   "source": [
    "## Iterating Through Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddae662f-60f9-4f83-840f-b8835c55a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 0\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: . the same clothes do not make it the same movie . in my opinion they didn ' t watch the 1995 version , which even though it had it\n",
      "\n",
      "\n",
      "Target: the same clothes do not make it the same movie . in my opinion they didn ' t watch the 1995 version , which even though it had it '\n",
      "\n",
      "\n",
      "sample: 0\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: ( voiced by nathan lane ) . stuart embarks on the experience of family loyalty and overall friendship . george will finally accept his tiny new brother when the dapper\n",
      "\n",
      "\n",
      "Target: voiced by nathan lane ) . stuart embarks on the experience of family loyalty and overall friendship . george will finally accept his tiny new brother when the dapper dressed\n",
      "\n",
      "\n",
      "sample: 1\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: , in which there are no thrills and the acting is incredibly phony . new best friend is boring , and the events during the movie are the same .\n",
      "\n",
      "\n",
      "Target: in which there are no thrills and the acting is incredibly phony . new best friend is boring , and the events during the movie are the same . skip\n",
      "\n",
      "\n",
      "sample: 1\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: watch it was because of the ' actors ' . firstly , for the most part they just looked stiff and i ' m sure their scripts were in their\n",
      "\n",
      "\n",
      "Target: it was because of the ' actors ' . firstly , for the most part they just looked stiff and i ' m sure their scripts were in their hands\n",
      "\n",
      "\n",
      "sample: 2\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: still living in it . jennifer decides to sell it as she ' s in dire need of the money much to granny ' s chagrin . she also begins\n",
      "\n",
      "\n",
      "Target: living in it . jennifer decides to sell it as she ' s in dire need of the money much to granny ' s chagrin . she also begins to\n",
      "\n",
      "\n",
      "sample: 2\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: revolting that it wasn ' t even entertaining . some of the highlights of this film include the absurd music which is constantly playing throughout the movie , the hideous\n",
      "\n",
      "\n",
      "Target: that it wasn ' t even entertaining . some of the highlights of this film include the absurd music which is constantly playing throughout the movie , the hideous special\n",
      "\n",
      "\n",
      "sample: 3\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: in a lead box ! elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings . . but if it weren ' t for the censorship\n",
      "\n",
      "\n",
      "Target: a lead box ! elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings . . but if it weren ' t for the censorship scandal\n",
      "\n",
      "\n",
      "sample: 3\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: otherwise people older than that will have no way to relate ( even 8-year-olds wouldn ' t want to see a movie about a kid who is whole year younger\n",
      "\n",
      "\n",
      "Target: people older than that will have no way to relate ( even 8-year-olds wouldn ' t want to see a movie about a kid who is whole year younger than\n",
      "\n",
      "\n",
      "sample: 4\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: burton ' s face that had been cast by the wave . was it only burton ' s apocalypse ? heck , that happens every day to people who lose\n",
      "\n",
      "\n",
      "Target: ' s face that had been cast by the wave . was it only burton ' s apocalypse ? heck , that happens every day to people who lose it\n",
      "\n",
      "\n",
      "sample: 4\n",
      "Source shape: torch.Size([30, 2])\n",
      "source: when james stewart , solid and convincing as always , solves all the stories from dillinger to 5th column more or less singlehandedly . june allyson as his regular love\n",
      "\n",
      "\n",
      "Target: james stewart , solid and convincing as always , solves all the stories from dillinger to 5th column more or less singlehandedly . june allyson as his regular love interest\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = iter(train_dataloader)\n",
    "\n",
    "for sample in range(5):\n",
    "    src, tgr = next(dataset)\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        print(f\"sample: {sample}\")\n",
    "        print(f\"Source shape: {src.shape}\")\n",
    "        print(f\"source: {idx_to_text(src[:,i])}\") # take the indices as the intput and return the word as the output\n",
    "        print(\"\\n\")\n",
    "        print(f\"Target: {idx_to_text(tgr[:,i])}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f7083e-ab92-411b-9940-2fb3530c9b5d",
   "metadata": {},
   "source": [
    "## **MASKING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081ed33-5c4b-4476-8bfe-9aa918c69515",
   "metadata": {},
   "source": [
    "Create masking so that the decoder while predicts the next word can only take words before the output words as the context, not the whole dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7bb5739-f61e-4d93-9a02-b2fcc0b5ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sqr_mask(sz,device = DEVICE):\n",
    "    mask = (torch.triu(torch.ones((sz,sz),device = device))==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22145155-405d-46c5-a476-c5b492b45888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf],\n",
       "        [0., 0., -inf],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3,3)\n",
    "a = (torch.triu(a)==1).transpose(0,1)\n",
    "a = a.float().masked_fill( a ==0,float('-inf')).masked_fill(a ==1,float(0.0))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "760adca6-915d-4c1b-bc09-d0781294474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(sz,device):\n",
    "    mask = (torch.triu(torch.ones((sz,sz),device =device))==1).transpose(0,1)\n",
    "    mask = mask.float().masked_fill(mask ==0,float('-inf')).masked_fill(mask==1,float(0.0))\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bba4dddb-7772-4a25-a369-f03ceae2a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src,device = DEVICE):\n",
    "    src_seq_len = src.shape[0]\n",
    "    src_mask = generate_mask(src_seq_len,device = DEVICE)\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0,1)\n",
    "    return src_mask,src_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f743b27a-da0c-4ee6-acf4-8dbb7bfb6f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf],\n",
       "         [0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., 0.]], device='mps:0'),\n",
       " tensor([[False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "src_t = torch.rand(5,5)\n",
    "m =create_mask(src_t,device = DEVICE)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322e0ea-b453-4200-b75f-cd0541b45681",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25d8889a-c7d1-4372-8f8a-09a04f1508e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                emb_dim: int,\n",
    "                dropout: float,\n",
    "                maxlen: int = 5000):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        den = torch.exp(-torch.arange(0,emb_dim,2)*math.log(10000)/emb_dim)\n",
    "\n",
    "        pos = torch.arange(0,maxlen).reshape(maxlen,1)\n",
    "\n",
    "        pos_embedding = torch.zeros(size = (maxlen,emb_dim))\n",
    "\n",
    "\n",
    "        pos_embedding[:,0::2] = torch.sin(pos*den)\n",
    "        pos_embedding[:,1::2] = torch.cos(pos*den)\n",
    "\n",
    "\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('pos_embedding',pos_embedding)\n",
    "\n",
    "    def forward(self,token_embedding):\n",
    "        return self.dropout(token_embedding+self.pos_embedding[:token_embedding.size(0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646b7aa-bddf-4e05-bb20-486f7cfbf22c",
   "metadata": {},
   "source": [
    "## Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90f9331a-9857-40b9-ad2c-90a83edb0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                vocab_size,\n",
    "                emb_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.embedding(vocab_size,emb_dim)\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def forward(self,tokens):\n",
    "        return self.embedding(tokens.long())*math.sqrt(self.emb_dim)\n",
    "        \"\"\"PyTorch nn.Embedding expects input as LongTensor (i.e., dtype=torch.int64).\n",
    "        This ensures that the token indices are valid integer indices for lookup.\n",
    "        If your tokens are accidentally float32, you'll get a type error.\"\"\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66220284-438c-4165-831a-b1b747c1861a",
   "metadata": {},
   "source": [
    "## Custom GPT Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ab7fd-c946-41d1-8d85-77d458c4ea19",
   "metadata": {},
   "source": [
    "* Initialization(init): embedding_dimension, vocab_size, num_heads, num_layers, max_sequence_length, and dropout\n",
    "* lm_head: Generates logits over the vocabulary\n",
    "\n",
    "* weight_initialization: Initializes the weights for better training convergence. The Xavier uniform initialization is used, which is a common practice for initializing weights in deep learning.\n",
    "* Decoder: method currently functions the forward pass, through the transformer encoder layers, followed by the generation of logits for the language modeling task.\n",
    "1) got the output from the transformer encoder layers\n",
    "2) Generation of logits for the language modelling task\n",
    "\n",
    "* Forward Pass: This method is similar to Decoder method, and defines the forward computation of the model. It produces the input through embedding layers, positional encoding, transformer encoded layers, and produces the final output using lm_head\n",
    "* mask generation: Both decoder and forward methods contain logic to generate a square casual mask if no source mask is provided. Mask ensures that the prediction for a position does not depend on the future tokens in the sequences, which is important for auto regressive nature of gpt models\n",
    "* Commented out decoder: A section of the code is commented out, suggesting an initial design where a transformer decoder layer was considered. However, the final implementation uses only encoder layers only, which is a common simplification for models focusing on language modeling and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a201b591-a5e9-47e7-bc0b-a8867628049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                emb_dim: int,\n",
    "                vocab_size: int,\n",
    "                num_head: int,\n",
    "                num_layers: int,\n",
    "                max_seq_len: 500,\n",
    "                dropout = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size,emb_dim)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(emb_dim = emb_dim,dropout = dropout)\n",
    "\n",
    "        print(f\"EMBEDDING DIMENSION: {emb_dim}\")\n",
    "\n",
    "        # Remaining layers are part of the TransformerDecoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model = emb_dim,\n",
    "                                              nhead = num_head,\n",
    "                                              dropout = dropout)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer = encoder_layers,\n",
    "                                                        num_layers = num_layers)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size) # produce the final output, the final logits over the vocabulary\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim()>1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    def create_mask(src,device = DEVICE):\n",
    "        src_seq_len = src.shape[0] # src_shape: [seq_len,batch_size,emb_dim]\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src_seq_len)\n",
    "        src_padding_mask = (src ==PAD_IDX).transpose(0,1)\n",
    "        return src_mask ,src_padding_mask\n",
    "\n",
    "    def decoder(self,x,src_mask):\n",
    "\n",
    "        seq_len = x.size(0)\n",
    "\n",
    "\n",
    "        # Add positional arguments to the input embeddings\n",
    "\n",
    "        x = self.embed(x)*math.sqrt(self.emb_dim)\n",
    "\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        if src_mask is None:\n",
    "            \"Generate a square casual mask for the sequence. The masked positions are filled with -inf and the unmasked positions will be filled with 0\"\n",
    "\n",
    "            src_mask,src_padding_mask = create_mask(x)\n",
    "\n",
    "        output = self.transformer_encoder(x,src_mask)\n",
    "        logits = self.lm_head(output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def forward(self,x,mask = None, key_paddig_mask = None):\n",
    "\n",
    "        seq_len = x.size(0)\n",
    "\n",
    "        embedding = self.embed(x) * math.sqrt(self.emb_dim)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        if src_mask is None:\n",
    "            src_mask, src_padding_mask= create_mask(x)\n",
    "\n",
    "        output =self.transformer_encoder(x,src_mask,src_padding_mask)\n",
    "\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc55c4f-f4e1-4aac-973b-2fba1b4917f0",
   "metadata": {},
   "source": [
    "## Model Configaration and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd2c30-d420-42ae-abaa-9f27b491b011",
   "metadata": {},
   "source": [
    "* **`STEPS`**:\n",
    "  1) Create the tokens of the datasets, and vocabularies\n",
    "  2) Create the collate funcion; which will return `source` and `target`\n",
    "  3) create positional encodings (source)\n",
    "  4) create embeddings (source)\n",
    "  5) create mask\n",
    "  6) create transformer layers\n",
    "  7) create transformer encoder\n",
    "  8) pass the output of the transformer encoder throught the linear logit\n",
    "\n",
    "\n",
    "**`Configure and instantiate a custom gpt model with the following specification`**\n",
    "* ntokens: Its basically the `vocab_size`, total number of unique tokens in the vocabulary, which the model will use to represent the word\n",
    "* emsize: The size of each embedding vector. In this model, each word will be represented by a 200 dimensional vector.\n",
    "* n_players: the number of transformer encoder layers in the model. We are using two layers in this configuaration\n",
    "* n_head: the number of attention heads in the multi head attention mechanism. The model will use two attention heads.\n",
    "* dropout: A regularization technique which randomly select neurons are ignored during training to prevent overfitting.  Here, we set the dropout to 0.2\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b20ad03-7dbc-42e9-89a9-aec2b025dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING DIMENSION: 200\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab) \n",
    "emb_dim = 200\n",
    "n_layers = 2\n",
    "nhead = 2\n",
    "dropout = 0.2\n",
    "\n",
    "model =  CustomGPTModel(emb_dim = emb_dim,\n",
    "                       vocab_size=ntokens,\n",
    "                       num_head = nhead,\n",
    "                       num_layers = n_layers,\n",
    "                       max_seq_len=5000).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78936f17-68c2-4dde-8c3f-d624a366626c",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc60ff-d7c4-478b-a6cb-22790e64f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(prompt,block_size = BLOCK_SIZE:\n",
    "\n",
    "    \"\"\"take the blocksize and fit it for proper input\"\"\"\n",
    "    while prompt is None:\n",
    "        prompt = input(\"Sorry, prom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0bcc7-15fd-458b-99ab-db8384a8056d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92573df-e264-4803-abc5-b641ca0289f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824b69a-b26f-4151-bd86-103b604582e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b7f0d-da26-48c1-abec-79ded3b99d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8388e-f3d0-47b2-b18e-6b400b2aa6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5f4b1-d1c3-431f-81ce-2e4a8c070690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bba28-8fd2-4863-80b2-2d982ad05531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e027933-916c-4580-ab35-585ae41a8f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32027e-ec5b-441d-80cc-d807f6f7ba52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4a6a0-b4ae-4060-9c01-6ecbd04e249a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2568c-4781-4c3c-adc0-469334fc3a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0694578-8530-4a5d-b2c6-1cb47916ba1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046981a3-ca52-4e6f-8194-a46f1a027947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
