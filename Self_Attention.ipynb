{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd30be4-3a18-4cb0-bdfc-d26681c26eb8",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5485c2-a931-4573-99cd-b97af7b76756",
   "metadata": {},
   "source": [
    "Self attention is a mechanism used in neural networks to help the model to focus on different parts of the input data when generating the output. \n",
    "\n",
    "Transformer achitecture mainly used in following jobs:\n",
    "\n",
    "* Machine Translation\n",
    "* Sentiment Analysis\n",
    "* Text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14bc6b-e74b-4b2e-98ce-b2147ddb997c",
   "metadata": {},
   "source": [
    "The unique library that we use in this structure\n",
    "\n",
    "*`Levenshtein:`* Use for calculating the `Levenshtein distance`, which can be useful for evaluating model performance in tasks like text generation, or translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a940d18-891d-40ef-9936-0b82da4c407a",
   "metadata": {},
   "source": [
    "## Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e354f9-8981-448c-9fc5-dd575017b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Levenshtein in /opt/anaconda3/lib/python3.12/site-packages (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from Levenshtein) (3.13.0)\n",
      "Collecting torch==2.3.0\n",
      "  Downloading torch-2.3.0-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting torchtext==0.18.0\n",
      "  Using cached torchtext-0.18.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.3.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.3.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.3.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.3.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.3.0) (2024.6.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from torchtext==0.18.0) (4.66.5)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from torchtext==0.18.0) (2.32.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchtext==0.18.0) (1.26.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.3.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torchtext==0.18.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torchtext==0.18.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torchtext==0.18.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torchtext==0.18.0) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Downloading torch-2.3.0-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torchtext-0.18.0-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.17.2\n",
      "    Uninstalling torchtext-0.17.2:\n",
      "      Successfully uninstalled torchtext-0.17.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.22.0 requires torch==2.7.0, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.3.0 torchtext-0.18.0\n"
     ]
    }
   ],
   "source": [
    "! pip install Levenshtein\n",
    "\n",
    "! pip install torch==2.3.0 torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c91d1-e005-40cf-ad51-e4d0c0031d6f",
   "metadata": {},
   "source": [
    "## Importing Required Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db70f943-6b6e-41c7-8704-442531a6b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "\n",
    "from Levenshtein import distance\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8e6242-680b-4e37-8553-824600616b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d33b4f-3a35-4be2-b41b-38f64a33a86d",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30461968-6855-4050-8bcc-af616a7b3c79",
   "metadata": {},
   "source": [
    "`Learning_rate:` step size at each iteration while moving toward a minimum of the loss function\n",
    "\n",
    "`batch_size:` The number of samples that will be propagated throught ht network in one forward/ backward pass. \n",
    "\n",
    "`max iters:` the total number of training iterations we plan to run. Set to 5000 to allow the model ample opportunity to learn from the data.\n",
    "\n",
    "`eval_interval and eval_iters:` parameters defining how frequently we evaluate the models performance on a set number of batches to approximate loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9d5b8-a5a3-4fa2-9065-d10d31dbc1d6",
   "metadata": {},
   "source": [
    "## Architecture Parameters\n",
    "\n",
    "* Max_vocab_size: This represents the maximum number of tokens in our vocabulary. It's set to 256. meaning that we will only consider the most frequent 256 tokens\n",
    "\n",
    "* Vocab_size: The actual number of tokens in the vocabulary, which may be less than the max due to the variable length of tokens in subword tokenization like BPE (Byte Pair Encoding)\n",
    "\n",
    "* Block_size: The length of the input sequence that the model is designed to handle. Here it's 16\n",
    "\n",
    "* n_embed: The size of each embedding vector, set to 32\n",
    "\n",
    "* Num_heads: the number of heads in the multi-headed self-attention mechanism,2 in this case, which allows the the model to jointly attend to information from different representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d55f6db-d9dd-460a-bdf7-2960a769d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d963824-77e3-4444-89e6-bfefdd6cf1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Parameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "max_iters = 5000 ## Maximum training iterations\n",
    "eval_interval = 200 ## Evaluate model every 'eval_interval' iterations in the training loop\n",
    "eval_iters = 100 ## When evaluating, approximate loss using 'eval_iters' batch\n",
    "\n",
    "\n",
    "## Architecture Parameters\n",
    "max_vocab_size = 256  # Maximum vocabulary size\n",
    "vocab_size = max_vocab_size # Real vocabulary size (e.g. BPE has a variable length, so it can be less than the max vocab size)\n",
    "block_size = 16  # Context length for predictions\n",
    "n_embed = 32 # Embedding Size\n",
    "num_heads = 2 # Number of head in multi headed attention\n",
    "n_layer = 2 # Number of blocks\n",
    "ff_scale_factor = 4\n",
    "dropout = 0.0\n",
    "\n",
    "\n",
    "head_size = n_embed//num_heads\n",
    "assert (num_heads*head_size) ==n_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db81f6b-7440-4892-b31c-bd93e2c2b63a",
   "metadata": {},
   "source": [
    "Following the parameter setup, you will create a function defined as `plot_embeddings` which is designed to visualize the learned embeddings in a 3D space using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1100ca-c6b9-4c14-ac7a-79ff8cb04e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(my_embeddings,name,vocab):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111,projection = '3d')\n",
    "\n",
    "    # plot the data points\n",
    "    ax.scatter(my_embeddings[:,0], my_embeddings[:,1], my_embeddings[:,2])\n",
    "\n",
    "\n",
    "    # label the points\n",
    "    for j,label in en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30034f-faa9-47b5-977d-2c833d1eb9b7",
   "metadata": {},
   "source": [
    "## Program for Literal transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6002b435-0218-4556-bc9e-dfb62c563549",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    'le': 'the',\n",
    "    'chat': 'cat',\n",
    "    'est': \"is\",\n",
    "    'sous': \"under\",\n",
    "    'la': 'the',\n",
    "    'table': 'table'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40b6f7a6-b9ea-4612-96a6-5747f146b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def translate(sentence):\n",
    "\n",
    "    out = '' # Initialize the output string\n",
    "    for token in tokenize(sentence):\n",
    "        out += dictionary[token]+ \" \"\n",
    "\n",
    "    return out.strip() # Return the translated sentence, stripping any extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "565e269c-3594-4ec9-9607-03a9914aef77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat is under the table'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"le chat est sous la table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "befbc2e4-50ab-4f6d-a071-a288b5750860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_key(query):\n",
    "\n",
    "    \"\"\"This function computes the Levenstein distance between the query and each key in the dictionary\n",
    "    The Levenshtein distance is a measure of the number of single-character edits required to change one word into the other \"\"\"\n",
    "\n",
    "    closest_key,min_dist = None,float('inf') # Initialize the closest key and minimum distance to inifinity\n",
    "\n",
    "    for key in dictionary.keys():\n",
    "\n",
    "        dist = distance(query,key)\n",
    "\n",
    "        if dist<min_dist:\n",
    "            min_dist,closest_key = dist,key\n",
    "\n",
    "    return closest_key\n",
    "\n",
    "def translet(sentence):\n",
    "\n",
    "    \"\"\"This function tokenizes the input sentence into words and finds the closest translation for each word.\n",
    "    \"\"\"\n",
    "\n",
    "    out = \"\" # Initialize the output string\n",
    "    for query in tokenize(sentence):\n",
    "        key =  find_closest_key(query)\n",
    "        out +=dictionary[key] +' '\n",
    "    return out.strip()\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f45ba12-9333-495c-bd00-7f5180a44316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'table'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translet('tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b2aaf-d829-4111-b8fa-4b3f829dc574",
   "metadata": {},
   "source": [
    "## Define Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18e23a2e-3936-4e77-a93e-3816d3c2c910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary input 6: ['chat', 'est', 'la', 'le', 'sous', 'table']\n",
      "Vocabulary output 5: ['cat', 'is', 'table', 'the', 'under']\n"
     ]
    }
   ],
   "source": [
    "# Create and sort the input vocabulary from the dictionary 's keys\n",
    "vocabulary_in = sorted(list((set(dictionary.keys()))))\n",
    "\n",
    "# Display the size and the sorted vocabulary for the input language\n",
    "print(f\"Vocabulary input {len(vocabulary_in)}: {vocabulary_in}\")\n",
    "\n",
    "# convert and sort the input vocabulary from the dictionary's values \n",
    "vocabulary_out  = sorted(list(set(dictionary.values())))\n",
    "\n",
    "# Display the size and the sorted vocabulary for the output language\n",
    "print(f\"Vocabulary output {len(vocabulary_out)}: {vocabulary_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cb90de4-607b-465c-9ab3-cadeca570aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7eb51c-0441-4545-936d-96a07a778b36",
   "metadata": {},
   "source": [
    "## Encode Tokens using 'one hot' encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f01465d-e415-4fb1-a859-cbe75d4bd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_one_hot(vocabulary):\n",
    "\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    one_hot = dict()\n",
    "\n",
    "    LEN = len(vocabulary)\n",
    "\n",
    "    for i, key in enumerate(vocabulary):\n",
    "\n",
    "        one_hot_encod = torch.zeros(len(vocabulary))\n",
    "\n",
    "        one_hot_encod[i] =1\n",
    "        one_hot[key] = one_hot_encod\n",
    "\n",
    "        print(f\"{key}\\t: {one_hot[key]}\")\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f5e19c0-9f79-48de-8b63-15e3c08d9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat\t: tensor([1., 0., 0., 0., 0., 0.])\n",
      "est\t: tensor([0., 1., 0., 0., 0., 0.])\n",
      "la\t: tensor([0., 0., 1., 0., 0., 0.])\n",
      "le\t: tensor([0., 0., 0., 1., 0., 0.])\n",
      "sous\t: tensor([0., 0., 0., 0., 1., 0.])\n",
      "table\t: tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat': tensor([1., 0., 0., 0., 0., 0.]),\n",
       " 'est': tensor([0., 1., 0., 0., 0., 0.]),\n",
       " 'la': tensor([0., 0., 1., 0., 0., 0.]),\n",
       " 'le': tensor([0., 0., 0., 1., 0., 0.]),\n",
       " 'sous': tensor([0., 0., 0., 0., 1., 0.]),\n",
       " 'table': tensor([0., 0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = encode_one_hot(vocabulary_in)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ede3b35c-08f8-4039-a9da-9aa700fbd9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_{ chat } = tensor([1., 0., 0., 0., 0., 0.])\n",
      "E_{ est } = tensor([0., 1., 0., 0., 0., 0.])\n",
      "E_{ la } = tensor([0., 0., 1., 0., 0., 0.])\n",
      "E_{ le } = tensor([0., 0., 0., 1., 0., 0.])\n",
      "E_{ sous } = tensor([0., 0., 0., 0., 1., 0.])\n",
      "E_{ table } = tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "for k,v in one_hot.items():\n",
    "    print(f\"E_{{ {k} }} = {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c1c9b19-ae13-4060-ac3a-8a0cdb73af99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "## Stacking the one-hot encoded vector for input vocabulary to form a tensor\n",
    "\n",
    "k = torch.stack([one_hot[k] for k in one_hot.keys()])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d988fda6-166f-4c6d-b539-fe46819f0a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7fc10b6-2838-47b0-992c-183da812ffd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chat', 'est', 'la', 'le', 'sous', 'table'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ca4a99f-3358-42f7-9302-e73d259b66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vector = torch.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc28fac-4d5c-442a-9947-c64cef12157d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b838e462-cb8e-40cb-b7a2-b948a4868fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_vector[1] =1\n",
    "one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd221d4c-b8b4-406a-bfa8-a996a49cbcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer to convert input token indices to vectors of fixed size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,n_embed)\n",
    "\n",
    "        # Linear layers to compute the queries, keys, and values from the embeddings\n",
    "        self.key = nn.Linear(n_embed,n_embed,bias = False)\n",
    "        self.query = nn.Linear(n_embed,n_embed,bias = False)\n",
    "        self.value = nn.Linear(n_embed,n_embed,bias = False)\n",
    "\n",
    "\n",
    "    def attention(self,x):\n",
    "\n",
    "        embedded_x = self.embedding(x)\n",
    "\n",
    "        k = self.key(embedded_x)\n",
    "        q = self.query(embedded_x)\n",
    "        v = self.value(embedded_x)\n",
    "\n",
    "        # Attention Score\n",
    "        # key shape: [batch_size, seq_len, embed_dim]\n",
    "        # query shape: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # the attention score we got from the dot product of key and query is w\n",
    "        # w shape: [batch_size, seq_len, seq_len]\n",
    "        # attention score of each token against every other token, including itself.\n",
    "        \n",
    "        w = q @ k.transpose(-2,-1) * k.shape[-1] ** -0.5 # transpose(-2,-1)--> swaps the last two dimension between them\n",
    "        w = torch.nn.functional.softmax(w,dim = 1) # do a softmax across the last dimenstion\n",
    "        return embedded_x,k,q,v,w\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        embedded_x = self.embedding(x)\n",
    "\n",
    "        k = self.key(embedded_x)\n",
    "        q = self.query(embedded_x)\n",
    "        v = self.value(embedded_x)\n",
    "\n",
    "        # Attention score\n",
    "        w = q @ k.transpose(-2,-1) * k.shape[-1] ** -0.5\n",
    "        w = nn.functional.softmax(w,dim = 1)\n",
    "\n",
    "        # add weighted values\n",
    "        out = w@v\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e26af9-2d1c-4070-af04-6eae4c7e3cc3",
   "metadata": {},
   "source": [
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7db932ce-517c-4564-8ea4-d4d4122fc6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \"),\n",
    "    (3,\"he painted the car red\"),\n",
    "    (1,\"he painted the red car\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3656e-5435-44cf-942f-56edd62b6819",
   "metadata": {},
   "source": [
    "## TOKENIZATION SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "55e0cdfa-d9ad-493f-b738-7d5164a76dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "tokens = [tokenizer(lines) for _,lines in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89d257a7-0bbf-4e18-8fc6-35363b398926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['introduction', 'to', 'nlp'],\n",
       " ['basics', 'of', 'pytorch'],\n",
       " ['nlp', 'techniques', 'for', 'text', 'classification'],\n",
       " ['named', 'entity', 'recognition', 'with', 'pytorch'],\n",
       " ['sentiment', 'analysis', 'using', 'pytorch'],\n",
       " ['machine', 'translation', 'with', 'pytorch'],\n",
       " ['nlp',\n",
       "  'named',\n",
       "  'entity',\n",
       "  ',',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  ',',\n",
       "  'machine',\n",
       "  'translation'],\n",
       " ['machine', 'translation', 'with', 'nlp'],\n",
       " ['named', 'entity', 'vs', 'sentiment', 'analysis', 'nlp'],\n",
       " ['he', 'painted', 'the', 'car', 'red'],\n",
       " ['he', 'painted', 'the', 'red', 'car']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "801b6bd4-98fa-46c4-9f1f-7ac689d7cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(vocabulary):\n",
    "    for _,lines in vocabulary:\n",
    "        yield tokenizer(lines)\n",
    "\n",
    "vocabulary = iter(dataset)\n",
    "\n",
    "tokens_data = yield_tokens(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a786614b-5f64-4908-82c9-462de2fcdb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['introduction', 'to', 'nlp'],\n",
       " ['basics', 'of', 'pytorch'],\n",
       " ['nlp', 'techniques', 'for', 'text', 'classification'],\n",
       " ['named', 'entity', 'recognition', 'with', 'pytorch'],\n",
       " ['sentiment', 'analysis', 'using', 'pytorch'],\n",
       " ['machine', 'translation', 'with', 'pytorch'],\n",
       " ['nlp',\n",
       "  'named',\n",
       "  'entity',\n",
       "  ',',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  ',',\n",
       "  'machine',\n",
       "  'translation'],\n",
       " ['machine', 'translation', 'with', 'nlp'],\n",
       " ['named', 'entity', 'vs', 'sentiment', 'analysis', 'nlp'],\n",
       " ['he', 'painted', 'the', 'car', 'red'],\n",
       " ['he', 'painted', 'the', 'red', 'car']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_func = [token for token in tokens_data]\n",
    "tokens_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "85942da4-0e1e-4bc4-91f4-339e4ea2c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "506fe706-48a6-4101-979d-45c782f58fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(tokens_func,specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7245342e-192f-4476-9f08-33bd73adc0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " 'nlp',\n",
       " 'pytorch',\n",
       " 'analysis',\n",
       " 'entity',\n",
       " 'machine',\n",
       " 'named',\n",
       " 'sentiment',\n",
       " 'translation',\n",
       " 'with',\n",
       " ',',\n",
       " 'car',\n",
       " 'he',\n",
       " 'painted',\n",
       " 'red',\n",
       " 'the',\n",
       " 'basics',\n",
       " 'classification',\n",
       " 'for',\n",
       " 'introduction',\n",
       " 'of',\n",
       " 'recognition',\n",
       " 'techniques',\n",
       " 'text',\n",
       " 'to',\n",
       " 'using',\n",
       " 'vs']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170f015-55ed-4142-94e1-81f2866290ee",
   "metadata": {},
   "source": [
    "## Text Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40b23f99-85e5-45e1-8fbb-07a584ab63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    \"\"\"Converts a text string to a list of token indices\"\"\"\n",
    "\n",
    "    return vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c1b1ac-baeb-45ee-bb93-39f1b3ad773b",
   "metadata": {},
   "source": [
    "## Hyperparameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d328c78-4564-469b-a380-43d825cebdd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Head.__init__() missing 2 required positional arguments: 'embed_dim' and 'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m n_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create the attention head with the integrated embedding layer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m attention_head \u001b[38;5;241m=\u001b[39m Head()\n",
      "\u001b[0;31mTypeError\u001b[0m: Head.__init__() missing 2 required positional arguments: 'embed_dim' and 'vocab_size'"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "n_embed = 3\n",
    "\n",
    "# Create the attention head with the integrated embedding layer\n",
    "attention_head = Head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaed3c8-6bd7-4839-9252-07d1b91b2f2f",
   "metadata": {},
   "source": [
    "## Dummy data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90ea25b2-7326-493b-964e-23ebcb228ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "tensor([12, 13, 15, 11, 14])\n"
     ]
    }
   ],
   "source": [
    "my_tokens = \"he painted the car red\"\n",
    "\n",
    "# Apply the text pipeline to the sentence to get token indices\n",
    "input_data = torch.tensor(text_pipeline(my_tokens),dtype = torch.long)\n",
    "\n",
    "# print out the shape and the token indices tensor\n",
    "print(input_data.shape)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc83c786-6687-40d3-af90-6f50f912fa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "embedded_x:\n",
      " tensor([[-0.3683, -0.6410, -0.6404],\n",
      "        [-0.1384,  0.1150, -0.0478],\n",
      "        [ 0.5578, -1.0002, -0.1047],\n",
      "        [ 0.3659, -0.0492,  0.6612],\n",
      "        [ 0.0702,  1.8551,  0.4558]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([5, 3])\n",
      "key:\n",
      " tensor([[ 0.2685,  0.2143, -0.2498],\n",
      "        [ 0.0078, -0.0442, -0.0450],\n",
      "        [ 0.1122,  0.3683,  0.1077],\n",
      "        [-0.2823, -0.1134,  0.1527],\n",
      "        [-0.2070, -0.4332,  0.2876]], grad_fn=<MmBackward0>)\n",
      "torch.Size([5, 3])\n",
      "Query:\n",
      " tensor([[-0.4027,  0.2417,  0.3353],\n",
      "        [ 0.1054, -0.0782,  0.1150],\n",
      "        [-0.8143,  0.5693, -0.4430],\n",
      "        [ 0.0641,  0.1180, -0.5265],\n",
      "        [ 1.0843, -0.9183,  0.2100]], grad_fn=<MmBackward0>)\n",
      "torch.Size([5, 3])\n",
      "Value:\n",
      "tensor([[-0.1004,  0.0797,  0.1339],\n",
      "        [-0.0796, -0.0511,  0.0192],\n",
      "        [ 0.3836,  0.3278, -0.0294],\n",
      "        [ 0.1975,  0.1075, -0.1316],\n",
      "        [-0.2107, -0.4564, -0.0877]], grad_fn=<MmBackward0>)\n",
      "torch.Size([5, 5])\n",
      "Attention weight:\n",
      "tensor([[0.1816, 0.1936, 0.2062, 0.2131, 0.2056],\n",
      "        [0.1976, 0.1994, 0.1990, 0.1992, 0.2048],\n",
      "        [0.2021, 0.1991, 0.2088, 0.2121, 0.1780],\n",
      "        [0.2241, 0.2049, 0.2020, 0.1900, 0.1789],\n",
      "        [0.2052, 0.2049, 0.1791, 0.1816, 0.2292]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedded_x,k,q,v,w = attention_head.attention(input_data)\n",
    "\n",
    "# print the size of the resulting embedded vector for verification\n",
    "print(embedded_x.shape)\n",
    "print(f\"embedded_x:\\n {embedded_x}\")\n",
    "print(k.shape)\n",
    "print(f\"key:\\n {k}\")\n",
    "print(q.shape)\n",
    "print(f\"Query:\\n {q}\")\n",
    "print(v.shape)\n",
    "print(f\"Value:\\n{v}\")\n",
    "print(w.shape)\n",
    "print(f\"Attention weight:\\n{w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab339fe-30b2-4a30-938a-dcc6772d3049",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b02f00c6-75e4-42a3-afc1-767abd20d763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.],\n",
       "         [ 1.],\n",
       "         [ 2.],\n",
       "         [ 3.],\n",
       "         [ 4.],\n",
       "         [ 5.],\n",
       "         [ 6.],\n",
       "         [ 7.],\n",
       "         [ 8.],\n",
       "         [ 9.],\n",
       "         [10.],\n",
       "         [11.],\n",
       "         [12.],\n",
       "         [13.],\n",
       "         [14.],\n",
       "         [15.],\n",
       "         [16.],\n",
       "         [17.],\n",
       "         [18.],\n",
       "         [19.],\n",
       "         [20.],\n",
       "         [21.],\n",
       "         [22.],\n",
       "         [23.],\n",
       "         [24.],\n",
       "         [25.],\n",
       "         [26.]]),\n",
       " torch.Size([27, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0,vocab_size,dtype = torch.float).unsqueeze(1)\n",
    "position,position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4c8fd84-63d2-4c73-8956-e7730f661c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of words from the vocabulary object\n",
    "vocab_list = list(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "375612d1-db59-49ec-a89a-131ca7fbac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " 'nlp',\n",
       " 'pytorch',\n",
       " 'analysis',\n",
       " 'entity',\n",
       " 'machine',\n",
       " 'named',\n",
       " 'sentiment',\n",
       " 'translation',\n",
       " 'with',\n",
       " ',',\n",
       " 'car',\n",
       " 'he',\n",
       " 'painted',\n",
       " 'red',\n",
       " 'the',\n",
       " 'basics',\n",
       " 'classification',\n",
       " 'for',\n",
       " 'introduction',\n",
       " 'of',\n",
       " 'recognition',\n",
       " 'techniques',\n",
       " 'text',\n",
       " 'to',\n",
       " 'using',\n",
       " 'vs']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55531959-4c12-4bc3-810a-baf1b3420399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5569799-c6af-452d-a466-c75fb403c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: <unk>, position index: 0.0\n",
      "Word: nlp, position index: 1.0\n",
      "Word: pytorch, position index: 2.0\n",
      "Word: analysis, position index: 3.0\n",
      "Word: entity, position index: 4.0\n",
      "Word: machine, position index: 5.0\n",
      "Word: named, position index: 6.0\n",
      "Word: sentiment, position index: 7.0\n",
      "Word: translation, position index: 8.0\n",
      "Word: with, position index: 9.0\n",
      "Word: ,, position index: 10.0\n",
      "Word: car, position index: 11.0\n",
      "Word: he, position index: 12.0\n",
      "Word: painted, position index: 13.0\n",
      "Word: red, position index: 14.0\n",
      "Word: the, position index: 15.0\n",
      "Word: basics, position index: 16.0\n",
      "Word: classification, position index: 17.0\n",
      "Word: for, position index: 18.0\n",
      "Word: introduction, position index: 19.0\n",
      "Word: of, position index: 20.0\n",
      "Word: recognition, position index: 21.0\n",
      "Word: techniques, position index: 22.0\n",
      "Word: text, position index: 23.0\n",
      "Word: to, position index: 24.0\n",
      "Word: using, position index: 25.0\n",
      "Word: vs, position index: 26.0\n"
     ]
    }
   ],
   "source": [
    "for idx in range(vocab_size):\n",
    "    word = vocab_list[idx] # get the word from the vocabulary list at the current index\n",
    "    pos = position[idx][0].item() # Extract the numerical value of the position index from the tensor\n",
    "    print(f\"Word: {word}, position index: {pos}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb7836-eb4c-48f5-8719-94809c465266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a matrix of zeros with dimensions [vocab_size, n_embed]\n",
    "# this will be used to hold the positional encodings for each word in the vocabulary\n",
    "\n",
    "pe = torch.zeros(vocab_size,n_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d42f522-051e-45e5-bda9-1e59268fc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_dim)\n",
    "    \n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.query = nn.Linear(embed_dim ,embed_dim, bias = False)\n",
    "        self.value = nn.Linear(embed_dim , embed_dim ,bias = False)\n",
    "    \n",
    "\n",
    "\n",
    "    def attention(self,x):\n",
    "        embed_x = self.embedding(x)\n",
    "        k = self.key(embed_x)\n",
    "        q = self.query(embed_x)\n",
    "        v = self.value(embed_x)\n",
    "        \n",
    "\n",
    "        ## attention score\n",
    "        # k_Shape : [batch_size, seq_len, embed_dim]\n",
    "        w = q @ k.transpose(-2,-1) * k.shape[-1] ** -0.5\n",
    "        w = torch.nn.softmax(w, dim = 1)\n",
    "    \n",
    "        return embed_x,k,q,v,w\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        embed_x = self.embedding(x)\n",
    "        k = self.key(embed_x)\n",
    "        q = self.query(embed_x)\n",
    "        v = self.value(embed_x)\n",
    "        \n",
    "\n",
    "        ## attention score\n",
    "        # k_Shape : [batch_size, seq_len, embed_dim]\n",
    "        w = q @ k.transpose(-2,-1) * k.shape[-1] ** -0.5\n",
    "        w = torch.nn.softmax(w, dim = 1)\n",
    "\n",
    "        # add weight values\n",
    "        out = w @ v\n",
    "        return out\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f44f5dc3-aebb-4f23-82e7-d62fd8e1fb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5865, 0.8069, 0.6474, 0.9974, 0.4893],\n",
       "         [0.0725, 0.8308, 0.6384, 0.0457, 0.5676],\n",
       "         [0.4795, 0.1701, 0.9509, 0.5975, 0.7179]],\n",
       "\n",
       "        [[0.6809, 0.2962, 0.5539, 0.7785, 0.8854],\n",
       "         [0.7682, 0.0020, 0.0525, 0.7021, 0.0929],\n",
       "         [0.8760, 0.6301, 0.4544, 0.3794, 0.1651]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(size = (2,3,5))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1901fc88-93e0-4149-a027-59090991e61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ac5be86-6e5e-4746-8661-eb80aca97e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.transpose(-2,-1)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de7b70fc-d56a-4b0a-b6a5-146630941b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5865, 0.0725, 0.4795],\n",
       "         [0.8069, 0.8308, 0.1701],\n",
       "         [0.6474, 0.6384, 0.9509],\n",
       "         [0.9974, 0.0457, 0.5975],\n",
       "         [0.4893, 0.5676, 0.7179]],\n",
       "\n",
       "        [[0.6809, 0.7682, 0.8760],\n",
       "         [0.2962, 0.0020, 0.6301],\n",
       "         [0.5539, 0.0525, 0.4544],\n",
       "         [0.7785, 0.7021, 0.3794],\n",
       "         [0.8854, 0.0929, 0.1651]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f85cbf-4274-4c30-9226-4f47e3f12614",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6e5d072-0ca6-4d69-b5e0-51fa47d3ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    \"\"\"Positional encoding module injects some information about the relative or absolute position of the tokens in the sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,n_embed,vocab_size, dropout = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize a buffer for the positional encodings (not a parameter, so it's not updated during training)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, n_embed)\n",
    "\n",
    "        position = torch.arange(0, vocab_size, dtype = torch.float).unsqueeze(dim = 1)\n",
    "\n",
    "        ## Calculate the positional encodings once in log space\n",
    "        pe = torch.cat((torch.cos(2 * 3.14 * position/25),torch.sin(2*3.14*position/25),torch.sin(2*3.14*position/5)),dim =1)\n",
    "\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # add positional encoding to each embedding vector x, assuming x is [seq_len,batch_size,embed_dim]\n",
    "        # pe is a registered buffer, and doesn't require gradients\n",
    "\n",
    "        pos = x + self.pe[:x.size(0),:]\n",
    "        \n",
    "        \n",
    "        return pos\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim, vocab_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # positional encoding layer\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim,vocab_size)\n",
    "\n",
    "        ## Layers to transform the position encoded embeddings into queries, keys and values\n",
    "\n",
    "        self.key =nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.query =nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.value =nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        \"\"\"Self Attention Head\"\"\"\n",
    "\n",
    "        embedded_x = self.embedding(x)\n",
    "\n",
    "        # Add the positional embedding\n",
    "        p_embedded = self.pos_encoder(embedded_x)\n",
    "\n",
    "        q = self.query(p_embedded)\n",
    "        k = self.key(p_embedded) ## key: [batch_size, seq_len, embed_dim]\n",
    "        v = self.value(p_embedded)\n",
    "\n",
    "        w = q @ k.transpose(-2,-1) * k.shape[-1] ** -0.5 # Queries * Keys / normalizations\n",
    "\n",
    "        # apply the softmax function to the attention scores to get probabilities\n",
    "        w = torch.nn.functional.softmax(w,dim =1)\n",
    "\n",
    "        # Multiply the attention weights with values to get the output\n",
    "        out = w @ v\n",
    "        return out\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "282e0c80-b7d6-4227-a23f-350e03128d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:,torch.Size([5, 3])\n",
      "Output: tensor([[-0.4442, -0.0455, -0.7745],\n",
      "        [-0.4178, -0.0963, -0.7719],\n",
      "        [-0.4369, -0.0753, -0.8054],\n",
      "        [-0.4862,  0.0094, -0.8236],\n",
      "        [-0.5000,  0.0320, -0.8273]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# instantiate the head class with embedding dimension and vocabulary size as parameters\n",
    "transformer = Head(n_embed,vocab_size)\n",
    "\n",
    "# Pass the input data through the transformer model to obtain the output data\n",
    "\n",
    "out = transformer(input_data)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "# The shape will provide insight into how the data has been transformed through the model\n",
    "print(f\"output shape:,{out.shape}\")\n",
    "\n",
    "print(f\"Output: {out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c678ad50-0ae1-4273-97f7-5bcc0273f9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, tensor([12, 13, 15, 11, 14]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embed,input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c8e4906-1035-4377-a07d-18f5c083aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b79752a-ef46-4115-a635-55e7e05430d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '.',\n",
       " 'a',\n",
       " 'cars',\n",
       " 'different',\n",
       " 'driving',\n",
       " 'give',\n",
       " 'i',\n",
       " 'level',\n",
       " 'love',\n",
       " 'me',\n",
       " 'of',\n",
       " 'pleasure',\n",
       " 'them']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330e30b-fd84-465c-94f7-fda0bd83ebe4",
   "metadata": {},
   "source": [
    "## **Transformer in PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5b5e7475-a798-457b-8a3e-9cb451d771db",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead = 16, num_encoder_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "232f9300-6917-410a-92b3-be140a1d1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.rand((10,32,512)) # batch_size, seq_len, emb_dim\n",
    "trg = torch.rand((20,32,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1c54ac4d-bbd3-4fb7-924a-f470f9b3144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer_model(src,trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "43f443ab-aab8-4417-ba23-0d0476bb05bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28,  7, 21,  ...,  7, 17, 24],\n",
       "        [ 4,  2,  6,  ..., 26, 28, 31],\n",
       "        [ 6, 31,  7,  ..., 23, 11,  2],\n",
       "        ...,\n",
       "        [ 0, 11, 14,  ..., 13, 18,  2],\n",
       "        [14, 28,  8,  ..., 16, 17, 24],\n",
       "        [ 2, 21, 14,  ..., 13,  4, 11]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.argmax(dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36becabc-23d6-4ad7-91c3-a3b24a613400",
   "metadata": {},
   "source": [
    "## MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9e2fe99-5b96-49a3-8452-7fa30ec3b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should be zero:0\n"
     ]
    }
   ],
   "source": [
    "# embedding_dim\n",
    "embed_dim = 4\n",
    "\n",
    "# Number of attention heads\n",
    "num_heads = 2\n",
    "\n",
    "print(f\"should be zero:{embed_dim%num_heads}\")\n",
    "\n",
    "# Initialize Multihead Attention\n",
    "multihead_attention = nn.MultiheadAttention(embed_dim = embed_dim , num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6c8ab65b-d3c0-4448-8322-a2551ea22411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output Shape: torch.Size([10, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10 # 10 words per sentence\n",
    "batch_size = 5 # 5 sentences in a batch\n",
    "query = torch.rand((seq_len,batch_size, embed_dim))\n",
    "key = torch.rand((seq_len,batch_size, embed_dim))\n",
    "value = torch.rand((seq_len,batch_size,embed_dim))\n",
    "\n",
    "# Perform multi head attention\n",
    "attn_output, _ = multihead_attention(query,key,value)\n",
    "\n",
    "print(f\"Attention Output Shape: {attn_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "adce4270-92a4-434d-9b39-db7ddcdb5eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be zero: 0\n"
     ]
    }
   ],
   "source": [
    "# embedding dimension\n",
    "embed_dim = 4\n",
    "\n",
    "# Number of attention head\n",
    "num_heads = 2\n",
    "\n",
    "# Checking if the embed_dim is divisible by num_heads or not\n",
    "print(f\"Should be zero: {embed_dim%num_heads}\")\n",
    "\n",
    "# Number of encoder layers\n",
    "num_layers = 6\n",
    "\n",
    "# Initialize the encoder layer with specified embedding dimension and number of heads\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model = embed_dim,\n",
    "                                          nhead = num_heads)\n",
    "\n",
    "## Build the transformer encoder by stacking the encoder layer 6\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=num_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6221ff-a616-49ed-8739-19047bd74864",
   "metadata": {},
   "source": [
    "Let's now test it with a random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2e1c5c50-4513-45ea-8bc7-58be824c3e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tensor Shape: torch.Size([10, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Define sequence length as 10 and batch_size as 5 for the input data\n",
    "seq_len = 10\n",
    "batch_size = 5\n",
    "\n",
    "# Generate random input tensor to simulate input embeddings for the transformer encoder\n",
    "x = torch.rand((seq_len,batch_size,embed_dim))\n",
    "\n",
    "# Apply the transformer encoder to the input\n",
    "encoded = transformer_encoder(x)\n",
    "\n",
    "print(f\"Encoded Tensor Shape: {encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19daad-439b-4a51-b361-7532844083a7",
   "metadata": {},
   "source": [
    "* embedding size = 240\n",
    "* number of layers = 12\n",
    "* number of attention heads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d73cecd3-48f3-4f0d-97e7-e61a5422a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 240\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model = embed_dim,\n",
    "                                          nhead=num_heads)\n",
    "\n",
    "encoder = nn.TransformerEncoder(encoder_layer,num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "817ac7f4-86b5-47a0-8cda-96ef9e681727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 240])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 20\n",
    "batch_size = 1\n",
    "\n",
    "src = torch.rand((seq_len,batch_size,embed_dim))\n",
    "\n",
    "output = encoder(src)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b30373-9077-42eb-9641-d1309ae671a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0be49a0-d2fc-4d0a-a652-9613a45f7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I love cars. Driving them give me a different level of pleasure'\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator([tokens], specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87f4ee1d-f7a9-422a-a11d-40caea7f6bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(vocab.get_itos()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f53af43-3853-4b74-a494-95193eb0fada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'using': 25,\n",
       " 'introduction': 19,\n",
       " 'classification': 17,\n",
       " 'for': 18,\n",
       " 'basics': 16,\n",
       " 'the': 15,\n",
       " 'text': 23,\n",
       " 'recognition': 21,\n",
       " 'to': 24,\n",
       " 'machine': 5,\n",
       " 'he': 12,\n",
       " 'painted': 13,\n",
       " 'car': 11,\n",
       " 'with': 9,\n",
       " 'translation': 8,\n",
       " 'entity': 4,\n",
       " 'sentiment': 7,\n",
       " 'vs': 26,\n",
       " 'red': 14,\n",
       " 'nlp': 1,\n",
       " ',': 10,\n",
       " 'named': 6,\n",
       " 'techniques': 22,\n",
       " '<unk>': 0,\n",
       " 'of': 20,\n",
       " 'analysis': 3,\n",
       " 'pytorch': 2}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b455da8-d97b-4f97-84ff-5d5192103aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 3\n",
    "embed_vocab = nn.Embedding(len(vocab),embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "014317b9-866f-42b9-87a9-d0aecceb09a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(14, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf9d7b20-8bfe-420c-b38d-e8c05f3bf075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4229, -1.6996,  0.1708],\n",
       "        [ 0.0846, -0.6866,  1.8081],\n",
       "        [-0.3060, -0.4981, -0.2505],\n",
       "        [-0.2914,  1.7543,  1.2794],\n",
       "        [-0.8538,  0.1899, -1.2747],\n",
       "        [ 0.9335, -1.0490, -0.3024],\n",
       "        [-0.2193,  0.3524, -0.4378],\n",
       "        [-0.5833, -0.5402,  1.1046],\n",
       "        [ 1.2563,  0.7371,  1.4163],\n",
       "        [ 1.4141, -0.5095,  1.2078],\n",
       "        [-0.7427,  1.2219,  0.2621],\n",
       "        [ 0.5857,  1.1733, -2.3439],\n",
       "        [-1.4810,  0.8754, -0.2977],\n",
       "        [-0.8680, -0.1756, -1.2657]], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_vocab.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c122b2f8-56e2-41c9-9d75-34eef34b8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "pe = torch.zeros(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2031193-c24d-48ae-b393-9fd28b62784b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c8b2b35f-a596-4799-9638-d49463012dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.],\n",
       "        [11.],\n",
       "        [12.],\n",
       "        [13.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0,vocab_size, dtype = torch.float).unsqueeze(dim =1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f3347a1-4893-4273-abf3-aac429f65bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 3.,  3.,  3.],\n",
       "        [ 4.,  4.,  4.],\n",
       "        [ 5.,  5.,  5.],\n",
       "        [ 6.,  6.,  6.],\n",
       "        [ 7.,  7.,  7.],\n",
       "        [ 8.,  8.,  8.],\n",
       "        [ 9.,  9.,  9.],\n",
       "        [10., 10., 10.],\n",
       "        [11., 11., 11.],\n",
       "        [12., 12., 12.],\n",
       "        [13., 13., 13.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.cat((position, position, position),dim = 1)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f7faa-e27b-49ca-a6d4-ba01f9ea7f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
