{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0ed321-5e60-4880-9e04-aecebc38d024",
   "metadata": {},
   "source": [
    "## Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5f741b-5883-4d06-bb45-819eeec15315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029e097-a72f-4980-917c-36e3835dd6dc",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0220549-5587-4407-bdc4-d82698f2d510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ShardingFilterIterDataPipe, ShardingFilterIterDataPipe)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading dataset\n",
    "train_iter,test_iter = IMDB()\n",
    "train_iter,test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2eeae6-660d-4956-b3a8-b7bbee41ceed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "[1]\n",
      "Total sample:12500\n",
      "25000\n",
      "[1, 2]\n",
      "Total sample in testdataset:25000\n",
      "Time required: 0.92 seconds\n"
     ]
    }
   ],
   "source": [
    "# Quantitative and qualitative information about the training data\n",
    "label,text_list = [],[]\n",
    "test_label,test_text_list = [],[]\n",
    "start_time = time.time()\n",
    "for _,text in train_iter:\n",
    "    label.append(_)\n",
    "    text_list.append(text)\n",
    "\n",
    "print(len(label))\n",
    "print(list(set(label)))\n",
    "print(f\"Total sample:{len(text_list)}\")\n",
    "\n",
    "\n",
    "for _,text in test_iter:\n",
    "    test_label.append(_)\n",
    "    test_text_list.append(text)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time-start_time\n",
    "print(len(test_label))\n",
    "print(list(set(test_label)))\n",
    "print(f\"Total sample in testdataset:{len(test_text_list)}\")\n",
    "print(f\"Time required: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870dde1-c070-4a01-8413-397cc3c1cc25",
   "metadata": {},
   "source": [
    "The training dataset has 12500 text block and the test dataset has 25000 text block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86751d57-8e01-4888-a328-829544001f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Train Text example: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\n",
      "First Test Text example: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n"
     ]
    }
   ],
   "source": [
    "_,text = next(iter(train_iter))\n",
    "_test,text_test = next(iter(test_iter))\n",
    "\n",
    "print(f\"First Train Text example: {text}\\n\")\n",
    "print(f\"First Test Text example: {text_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c26f1-9231-4d3b-9f60-4e89c39e44c6",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a37fc8-fc79-400f-bec3-a12be86ddc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tokens\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0,1,2\n",
    "\n",
    "special_symbols = ['<unk>','<pad>','<|endoftext|>']\n",
    "\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,[text for _,text in train_iter]), specials = special_symbols,special_first = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d3c174-441c-4f4c-b7de-0e7484ce5b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68813"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04356656-1d0c-4596-986c-43b6ae407084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2435"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['drink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff38171-49f5-4ff0-9d2f-2d1bc8d85131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample accumulation\n",
    "\"\"\"In decoder you have to define at how many blocks the model will look at once, This could be mentioned as context.\"\"\"\n",
    "\n",
    "def get_sample(context_size, text):\n",
    "    \"\"\"Context_size: The number of tokens the model will look at once\n",
    "    text: the whole text_tokens, From where the model will retrieve the tokens and assign to the source and target\"\"\"\n",
    "    sample_len = len(text)\n",
    "\n",
    "    src,trg = [],[]\n",
    "    if (sample_len-context_size)>=1:\n",
    "        start = torch.randint(low = 0,high = sample_len-context_size, size = (1,)).item()\n",
    "        end = start+context_size\n",
    "\n",
    "        src = text[start:end]\n",
    "        trg = text[start+1:end+1]\n",
    "\n",
    "\n",
    "    elif (sample_len-context_size)<=0:\n",
    "        start = torch.randint(0,sample_len,size = (1,)).item()\n",
    "        end = start+context_size\n",
    "\n",
    "        src = text[start:end]\n",
    "        trg = text[start+1:end]\n",
    "\n",
    "        trg.append('<|endoftext|>')\n",
    "\n",
    "    return src, trg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d941fab4-080f-4a5b-a3fd-418634242533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1\n",
      "Source: ['to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions', 'to', 'making', 'some', 'sort']\n",
      "Target: ['learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions', 'to', 'making', 'some', 'sort', 'of']\n",
      "Sample: 1\n",
      "Source: ['years', 'ago', ',', 'this', 'was', 'considered', 'pornographic', '.', 'really', ',', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between']\n",
      "Target: ['ago', ',', 'this', 'was', 'considered', 'pornographic', '.', 'really', ',', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between', ',']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "CONTEXT_SIZE = 20\n",
    "for _ in range(BATCH_SIZE):\n",
    "    _,text = next(iter(train_iter))\n",
    "\n",
    "    src,trg = get_sample(context_size = CONTEXT_SIZE, text = tokenizer(text))\n",
    "\n",
    "    print(f\"Sample: {_}\")\n",
    "    print(f\"Source: {src}\")\n",
    "    print(f\"Target: {trg}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c6edb-7886-409d-82c2-760f812c8a32",
   "metadata": {},
   "source": [
    "## INDEX TO ENGLISH & ENGLISH TO INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71356af-3234-46c0-b3f2-efbbebe7bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_eng = lambda seq : \" \".join([vocab.get_itos()[idx] for idx in seq])\n",
    "eng_to_idx = lambda text: [vocab[token] for token in tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8fe9d0c-c892-4ea9-a91f-119e456bfedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 0\n",
      "Source: [tensor([   58, 12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,\n",
      "          494,   386,  1978,     5,    70,   338,   144,    14,    39,   129])]\n",
      "Target: [tensor([12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,   494,\n",
      "          386,  1978,     5,    70,   338,   144,    14,    39,   129,     3])]\n",
      "sample: 1\n",
      "Source: [tensor([   58, 12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,\n",
      "          494,   386,  1978,     5,    70,   338,   144,    14,    39,   129]), tensor([   25,    20,   248,  1798,    10,  2307,     4,  2876,     7, 14661,\n",
      "           29,    56,  4419,  1218,    27,     9,  3994,   534,     3,    21])]\n",
      "Target: [tensor([12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,   494,\n",
      "          386,  1978,     5,    70,   338,   144,    14,    39,   129,     3]), tensor([   20,   248,  1798,    10,  2307,     4,  2876,     7, 14661,    29,\n",
      "           56,  4419,  1218,    27,     9,  3994,   534,     3,    21,    69])]\n",
      "sample: 2\n",
      "Source: [tensor([   58, 12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,\n",
      "          494,   386,  1978,     5,    70,   338,   144,    14,    39,   129]), tensor([   25,    20,   248,  1798,    10,  2307,     4,  2876,     7, 14661,\n",
      "           29,    56,  4419,  1218,    27,     9,  3994,   534,     3,    21]), tensor([  788,   338,     7,   719,    32,     6,   638,  9032,    14,  3994,\n",
      "          534,     3,    58, 12567,  3389,     5,  5375,    80,  1311,    10])]\n",
      "Target: [tensor([12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,   494,\n",
      "          386,  1978,     5,    70,   338,   144,    14,    39,   129,     3]), tensor([   20,   248,  1798,    10,  2307,     4,  2876,     7, 14661,    29,\n",
      "           56,  4419,  1218,    27,     9,  3994,   534,     3,    21,    69]), tensor([  338,     7,   719,    32,     6,   638,  9032,    14,  3994,   534,\n",
      "            3,    58, 12567,  3389,     5,  5375,    80,  1311,    10,    61])]\n",
      "sample: 3\n",
      "Source: [tensor([   58, 12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,\n",
      "          494,   386,  1978,     5,    70,   338,   144,    14,    39,   129]), tensor([   25,    20,   248,  1798,    10,  2307,     4,  2876,     7, 14661,\n",
      "           29,    56,  4419,  1218,    27,     9,  3994,   534,     3,    21]), tensor([  788,   338,     7,   719,    32,     6,   638,  9032,    14,  3994,\n",
      "          534,     3,    58, 12567,  3389,     5,  5375,    80,  1311,    10]), tensor([ 9032,    14,  3994,   534,     3,    58, 12567,  3389,     5,  5375,\n",
      "           80,  1311,    10,    61,   186,   494,   386,  1978,     5,    70])]\n",
      "Target: [tensor([12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,   494,\n",
      "          386,  1978,     5,    70,   338,   144,    14,    39,   129,     3]), tensor([   20,   248,  1798,    10,  2307,     4,  2876,     7, 14661,    29,\n",
      "           56,  4419,  1218,    27,     9,  3994,   534,     3,    21,    69]), tensor([  338,     7,   719,    32,     6,   638,  9032,    14,  3994,   534,\n",
      "            3,    58, 12567,  3389,     5,  5375,    80,  1311,    10,    61]), tensor([   14,  3994,   534,     3,    58, 12567,  3389,     5,  5375,    80,\n",
      "         1311,    10,    61,   186,   494,   386,  1978,     5,    70,   338])]\n",
      "sample: 4\n",
      "Source: [tensor([   58, 12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,\n",
      "          494,   386,  1978,     5,    70,   338,   144,    14,    39,   129]), tensor([   25,    20,   248,  1798,    10,  2307,     4,  2876,     7, 14661,\n",
      "           29,    56,  4419,  1218,    27,     9,  3994,   534,     3,    21]), tensor([  788,   338,     7,   719,    32,     6,   638,  9032,    14,  3994,\n",
      "          534,     3,    58, 12567,  3389,     5,  5375,    80,  1311,    10]), tensor([ 9032,    14,  3994,   534,     3,    58, 12567,  3389,     5,  5375,\n",
      "           80,  1311,    10,    61,   186,   494,   386,  1978,     5,    70]), tensor([  15,  800,    5, 1487,  118,    6,  372,    9,  129, 1206, 3535,   13,\n",
      "          69,   70,   10,   77,   15,   20,  479,    3])]\n",
      "Target: [tensor([12567,  3389,     5,  5375,    80,  1311,    10,    61,   186,   494,\n",
      "          386,  1978,     5,    70,   338,   144,    14,    39,   129,     3]), tensor([   20,   248,  1798,    10,  2307,     4,  2876,     7, 14661,    29,\n",
      "           56,  4419,  1218,    27,     9,  3994,   534,     3,    21,    69]), tensor([  338,     7,   719,    32,     6,   638,  9032,    14,  3994,   534,\n",
      "            3,    58, 12567,  3389,     5,  5375,    80,  1311,    10,    61]), tensor([   14,  3994,   534,     3,    58, 12567,  3389,     5,  5375,    80,\n",
      "         1311,    10,    61,   186,   494,   386,  1978,     5,    70,   338]), tensor([ 800,    5, 1487,  118,    6,  372,    9,  129, 1206, 3535,   13,   69,\n",
      "          70,   10,   77,   15,   20,  479,    3,    4])]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE  = 5\n",
    "CONTEXT_SIZE = 20\n",
    "src_batch,trg_batch = [],[]\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    _,text = next(iter(train_iter)) # Take the first text sample of the training data\n",
    "    src,trg = get_sample(context_size = CONTEXT_SIZE,text = tokenizer(text))\n",
    "    src_vocab,trg_vocab = vocab(src),vocab(trg)\n",
    "    src_tensors,trg_tensors = torch.tensor(src_vocab,dtype = torch.int64),torch.tensor(trg_vocab,dtype = torch.int64)\n",
    "\n",
    "    src_batch.append(src_tensors),trg_batch.append(trg_tensors)\n",
    "\n",
    "    print(f\"sample: {i}\")\n",
    "    print(f\"Source: {src_batch}\")\n",
    "    print(f\"Target: {trg_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5029064-9fdf-4397-9d46-acc93c1cb01f",
   "metadata": {},
   "source": [
    "## Create Custom Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b158fd-0d4d-4357-8497-804b1d241473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "    \"\"\"The collate_batch function prepares batches of source and target sequences for training by processing each text sample in a given batch. \n",
    "    It generates source and target sequences using the get_sample function with a specified block size, converts these sequences to indices using a vocabulary, and transforms them into PyTorch tensors. \n",
    "    The sequences are then padded to ensure uniform length across the batch. Finally, it returns the padded source and target batches, ready for training on the specified device (DEVICE).\"\"\"\n",
    "\n",
    "    src_batch,trg_batch = [],[]\n",
    "    for _,text in batch:\n",
    "\n",
    "        token_text = tokenizer(text)\n",
    "\n",
    "        src,trg = get_sample(CONTEXT_SIZE,token_text)\n",
    "\n",
    "        src_indices, trg_indices = vocab(src),vocab(trg)\n",
    "\n",
    "        src_seq,trg_seq = torch.tensor(src_indices,dtype = torch.int64),torch.tensor(trg_indices,dtype = torch.int64)\n",
    "\n",
    "        \n",
    "        src_batch.append(src_seq)\n",
    "        trg_batch.append(trg_seq)\n",
    "\n",
    "\n",
    "\n",
    "    src_batch = pad_sequence(src_batch,padding_value = PAD_IDX, batch_first = False)\n",
    "    trg_batch = pad_sequence(trg_batch,padding_value = PAD_IDX, batch_first = False)\n",
    "    return src_batch,trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04e704-c49e-4322-b144-fd5b12612e87",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2effa0b-58fb-4d28-923c-41de6c415337",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = train_iter,\n",
    "                             batch_size = 1,\n",
    "                             shuffle = True,\n",
    "                             collate_fn = collate_function)\n",
    "test_dataloader = DataLoader(dataset = test_iter,\n",
    "                            batch_size = 1,\n",
    "                            shuffle = True,\n",
    "                            collate_fn = collate_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c3c93-6017-4069-bb19-f2809a1860e0",
   "metadata": {},
   "source": [
    "## Iterating through Data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d717160f-547b-4cdf-ba12-ed3ac275a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 0\n",
      "Source: . and saying that it is the same director as the boogeyman , when a new version of that just\n",
      "Target: and saying that it is the same director as the boogeyman , when a new version of that just came\n",
      "sample: 1\n",
      "Source: cop tying the authority , the criminals and the police together - check . slow motion and/or jerky frame rates\n",
      "Target: tying the authority , the criminals and the police together - check . slow motion and/or jerky frame rates for\n",
      "sample: 2\n",
      "Source: , is the pretentious way the episodes are titled . truly great shows are still funny after many , repeated\n",
      "Target: is the pretentious way the episodes are titled . truly great shows are still funny after many , repeated viewings\n",
      "sample: 3\n",
      "Source: something as an outsider . that may be part of the reason for my disappointment . i was expecting more\n",
      "Target: as an outsider . that may be part of the reason for my disappointment . i was expecting more action\n",
      "sample: 4\n",
      "Source: aware of , such as 1 ) does money buy happiness ? 2 ) should i lie ( to my\n",
      "Target: of , such as 1 ) does money buy happiness ? 2 ) should i lie ( to my parents\n",
      "sample: 5\n",
      "Source: writing clever and sometimes unexpectedly wise and compassionate dialogue . ( no wonder the coen brothers ' next movie is\n",
      "Target: clever and sometimes unexpectedly wise and compassionate dialogue . ( no wonder the coen brothers ' next movie is going\n",
      "sample: 6\n",
      "Source: attempt at sexual suggestion award a scene in which pia zadora , at a picnic , stands between two boys\n",
      "Target: at sexual suggestion award a scene in which pia zadora , at a picnic , stands between two boys who\n",
      "sample: 7\n",
      "Source: the only thing to keep you watching after the head is reattached is the fact that there are so many\n",
      "Target: only thing to keep you watching after the head is reattached is the fact that there are so many boobs\n",
      "sample: 8\n",
      "Source: , who falls in love with sissy , thumbs or not . gus van sant directed and adapted tom robbins\n",
      "Target: who falls in love with sissy , thumbs or not . gus van sant directed and adapted tom robbins '\n",
      "sample: 9\n",
      "Source: ) is one of those million dollar finds - a girl with model looks who don ' t mind getting\n",
      "Target: is one of those million dollar finds - a girl with model looks who don ' t mind getting down\n"
     ]
    }
   ],
   "source": [
    "batch = iter(train_dataloader)\n",
    "\n",
    "for sample in range(10):\n",
    "    src,trg = next(batch)\n",
    "\n",
    "    print(f\"sample: {sample}\")\n",
    "    print(f\"Source: {idx_to_eng(src)}\")\n",
    "    print(f\"Target: {idx_to_eng(trg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3e304-12e5-4159-81c4-f8eed5fcd2fb",
   "metadata": {},
   "source": [
    "## Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8f0d53b-4c07-4eb0-9c68-946f281f6b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4feb8-17a0-4acc-8059-71d64a67cebd",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bd9921b-fa44-4648-9579-9b73f92a91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                emb_dim: int,\n",
    "                dropout: float,\n",
    "                max_len = 5000):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        den = torch.exp(-torch.arange(0,emb_dim,2)*math.log(10000)/emb_dim)\n",
    "        pos  = torch.arange(0,max_len).reshape(max_len,1)\n",
    "\n",
    "        pos_embedding = torch.zeros(size = (max_len,emb_dim))\n",
    "\n",
    "\n",
    "        pos_embedding[:,0::2] = torch.sin(pos*den)\n",
    "        pos_embedding[:,1::2] = torch.cos(pos*den)\n",
    "\n",
    "\n",
    "        # Pos_embedding_shape = [seq_len,emb_dim]\n",
    "\n",
    "        # add the batch_size to the sequence\n",
    "        pos_embedding = pos_embedding.unsqueeze(dim = -2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Positional embedding is a non-learnable parameter. It won't be updated with time\n",
    "        \n",
    "        self.register_buffer('pos_embedding',pos_embedding)\n",
    "\n",
    "\n",
    "    def forward(self,token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9bf74-69fe-4bc9-bb77-9be44027b814",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f28020f2-3e5e-4dd2-9dae-a213e760c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src_seq_len):\n",
    "\n",
    "    mask = (torch.triu(torch.ones(size = (src_seq_len,src_seq_len)))==1).transpose(0,1)\n",
    "    mask = mask.float().masked_fill(mask==1,float(0.0)).masked_fill(mask ==0,float('-inf'))\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "226ff520-d2f2-4d33-8a11-05a53dbec25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf],\n",
       "        [0., 0., -inf],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment\n",
    "triu = torch.triu(torch.ones(size = (3,3))==1).transpose(0,1)\n",
    "mask = triu.float().masked_fill(triu==1,float(0.0)).masked_fill(triu ==0,float('-inf'))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c479e57c-19df-4011-844c-537b2ec8551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(src):\n",
    "    src_seq_len = src.shape[0]\n",
    "    src_mask = create_mask(src_seq_len)\n",
    "    src_padding_mask = (src==PAD_IDX).transpose(0,1)\n",
    "\n",
    "    return src_mask, src_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "249d50d2-d01b-44af-aaeb-58fbe775bbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf],\n",
       "         [0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., 0.]]),\n",
       " tensor([[False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "src_t = torch.rand(5,5)\n",
    "m =generate_mask(src_t)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e107c29-2109-49f1-8a68-926bb14340e0",
   "metadata": {},
   "source": [
    "## Custom GPT Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de643fb9-eabc-4c46-aef9-d222cc887ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                emb_dim: int,\n",
    "                vocab_size: int,\n",
    "                num_head: int,\n",
    "                num_layers: int,\n",
    "                max_seq_len = 500,\n",
    "                dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_encoding = PositionalEmbedding(emb_dim,dropout,max_len = max_seq_len)\n",
    "        self.emb_dim  = emb_dim\n",
    "\n",
    "        # Encoder layers\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model = emb_dim,nhead = num_head,dropout = dropout)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer = encoder_layers,num_layers = num_layers)\n",
    "\n",
    "        self.lm_head = nn.Linear(emb_dim,vocab_size) # produce the final outputs, the final logits over vocabulary\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim>1:\n",
    "                nn.init_xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    def create_mask(self,source):\n",
    "        src_seq_len = source.shape[0]\n",
    "\n",
    "        src_mask = nn.Transformer.generate_square_subsequent_mask(src_seq_len)\n",
    "\n",
    "        src_padding_mask = (src==PAD_IDX).transpose(0,1)\n",
    "\n",
    "        return src_mask, src_padding_mask\n",
    "\n",
    "    def decoder(self,x,src_mask):\n",
    "        \n",
    "\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        seq_len = x.shape[0]\n",
    "\n",
    "\n",
    "        # Add positional encodding to the input embedding\n",
    "        x = self.embedding(x)*math.sqrt(self.emb_dim)\n",
    "\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        if src_mask is None:\n",
    "            src_mask, src_padding_mask = generate_mask(x)\n",
    "\n",
    "\n",
    "        output = self.transformer_encoder(x,src_mask)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x,src_mask= None, key_padding_mask = None):\n",
    "\n",
    "        x.to(DEVICE)\n",
    "\n",
    "        seq_length = x.shape[0]\n",
    "\n",
    "        # add positional embeddings to the embedding\n",
    "        x = self.embedding(x)*math.sqrt(self.emb_dim)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        if src_mask is None:\n",
    "            src_mask, src_padding_mask = generate_mask(x)\n",
    "\n",
    "\n",
    "        output = self.transformer_encoder(x,src_mask, key_padding_mask)\n",
    "\n",
    "        x = self.lm_head(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e18c8ef-8640-45de-811d-9f26517c1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 500\n",
    "num_head = 2\n",
    "num_layers = 2\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = CustomGPTModel(emb_dim = emb_dim,\n",
    "                       vocab_size = vocab_size,\n",
    "                       num_head = num_head,\n",
    "                       num_layers = num_layers).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8fed3-eb84-4694-9567-a1ce028a5e5b",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9242f-048a-40a7-bb71-56a6d18c6f13",
   "metadata": {},
   "source": [
    "#### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b17b5260-372a-422e-ba50-df57d083c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "489cbc1a-6153-4967-aabd-a9480bf068ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompting(prompt=None,block_size=20):\n",
    "\n",
    "    while prompt is None:\n",
    "        prompt = input(\"Prompt can't be empty. Please enter a valid prompt\")\n",
    "\n",
    "    prompt_tokens = tokenizer(prompt)\n",
    "\n",
    "    if len(prompt_tokens)>block_size:\n",
    "        prompt_tokens = prompt_tokens[-block_size:]\n",
    "\n",
    "\n",
    "    prompt_indices = vocab(prompt_tokens)\n",
    "    prompt_tensors = torch.tensor(prompt_indices,dtype = torch.int64).reshape(-1,1).to(DEVICE)\n",
    "\n",
    "    return prompt_tensors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8ddc1c3-3837-4270-a67d-ad34acc32eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Prompt can't be empty. Please enter a valid prompt football is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2233],\n",
       "        [  11]], device='mps:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompting()\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a05ef5de-7b60-4973-8098-451f77403f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   4],\n",
       "        [2290],\n",
       "        [  11]], device='mps:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tensor = prompting('The sky is')\n",
    "prompt_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2bc78-ddb1-4f94-add3-8ec3471c8936",
   "metadata": {},
   "source": [
    "## Output Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c33b2e2d-1be6-4432-b33e-f8bcab5e6bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape:torch.Size([3, 1, 68813])\n",
      "logit prediction dimension:torch.Size([1, 68813])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'concept--the'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def output(prompt):\n",
    "    prompt_logit = prompting(prompt) # prompt_shape : [seq_len,batch_size]\n",
    "    logits = model(prompt_logit)\n",
    "\n",
    "    print(f\"output_shape:{logits.shape}\")\n",
    "\n",
    "    logits = logits.transpose(0,1)\n",
    "\n",
    "    logit_prediction = logits[:,-1]\n",
    "    print(f\"logit prediction dimension:{logit_prediction.shape}\")\n",
    "\n",
    "    next_token_encoded = torch.argmax(logit_prediction,dim = -1).reshape(-1,1)\n",
    "\n",
    "    return next_token_encoded\n",
    "\n",
    "\n",
    "result = output(\"The sky is\")\n",
    "predicted_word = idx_to_eng(result)\n",
    "predicted_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7a283-3315-4662-8ffe-daeb79f2e253",
   "metadata": {},
   "source": [
    "## Autoregressive Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "060cd0d4-7eb9-491e-abfb-d4764770a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_prompt_shape: torch.Size([4, 1])\n",
      "logit shape: torch.Size([3, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the\n",
      "input_prompt_shape: torch.Size([5, 1])\n",
      "logit shape: torch.Size([4, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean\n",
      "input_prompt_shape: torch.Size([6, 1])\n",
      "logit shape: torch.Size([5, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster\n",
      "input_prompt_shape: torch.Size([7, 1])\n",
      "logit shape: torch.Size([6, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly\n",
      "input_prompt_shape: torch.Size([8, 1])\n",
      "logit shape: torch.Size([7, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish\n",
      "input_prompt_shape: torch.Size([9, 1])\n",
      "logit shape: torch.Size([8, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault\n",
      "input_prompt_shape: torch.Size([10, 1])\n",
      "logit shape: torch.Size([9, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade\n",
      "input_prompt_shape: torch.Size([11, 1])\n",
      "logit shape: torch.Size([10, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget\n",
      "input_prompt_shape: torch.Size([12, 1])\n",
      "logit shape: torch.Size([11, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills\n",
      "input_prompt_shape: torch.Size([13, 1])\n",
      "logit shape: torch.Size([12, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural\n",
      "input_prompt_shape: torch.Size([14, 1])\n",
      "logit shape: torch.Size([13, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds\n",
      "input_prompt_shape: torch.Size([15, 1])\n",
      "logit shape: torch.Size([14, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds easiest/worst\n",
      "input_prompt_shape: torch.Size([16, 1])\n",
      "logit shape: torch.Size([15, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds easiest/worst howser\n",
      "input_prompt_shape: torch.Size([17, 1])\n",
      "logit shape: torch.Size([16, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds easiest/worst howser bergen\n",
      "input_prompt_shape: torch.Size([18, 1])\n",
      "logit shape: torch.Size([17, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds easiest/worst howser bergen milestone\n",
      "input_prompt_shape: torch.Size([19, 1])\n",
      "logit shape: torch.Size([18, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds easiest/worst howser bergen milestone stroboscopic\n",
      "input_prompt_shape: torch.Size([20, 1])\n",
      "logit shape: torch.Size([19, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds easiest/worst howser bergen milestone stroboscopic berlusconi\n",
      "input_prompt_shape: torch.Size([21, 1])\n",
      "logit shape: torch.Size([20, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds easiest/worst howser bergen milestone stroboscopic berlusconi oh\n",
      "input_prompt_shape: torch.Size([22, 1])\n",
      "logit shape: torch.Size([21, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds easiest/worst howser bergen milestone stroboscopic berlusconi oh adjurdubois\n",
      "input_prompt_shape: torch.Size([23, 1])\n",
      "logit shape: torch.Size([22, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is concept--the terpsichorean bangster acturly spanish vault accolade microbudget quills oh-so-multicultural kinds easiest/worst howser bergen milestone stroboscopic berlusconi oh adjurdubois undresses\n"
     ]
    }
   ],
   "source": [
    "# Declaring prompt \n",
    "prompt = \"The sky is\"\n",
    "prompt_tokens = prompting(prompt)\n",
    "## By using model output\n",
    "max_new_tokens = 20 # how many words you may allow your model to generate\n",
    "\n",
    "for i in range(max_new_tokens):\n",
    "    \n",
    "\n",
    "    logit = model(prompt_tokens)\n",
    "    logit_reshape = logit.transpose(0,1)\n",
    "\n",
    "    logit_prediction = logit_reshape[:,-1]\n",
    "\n",
    "    next_token_encoded = torch.argmax(logit_prediction, dim = -1).reshape(-1,1)\n",
    "\n",
    "    prompt_tokens = torch.cat((prompt_tokens,next_token_encoded),dim =0)\n",
    "\n",
    "    print(f\"input_prompt_shape: {prompt_tokens.shape}\")\n",
    "    print(f\"logit shape: {logit.shape}\")\n",
    "    print(f\"next_token_shape: {next_token_encoded.shape}\")\n",
    "    print(f\"output:{\" \".join([idx_to_eng(tokens) for tokens in prompt_tokens])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51860a5f-ac89-41d5-88b7-c2afb17b879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_prompt_shape: torch.Size([4, 1])\n",
      "logit shape: torch.Size([3, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger\n",
      "input_prompt_shape: torch.Size([5, 1])\n",
      "logit shape: torch.Size([4, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons\n",
      "input_prompt_shape: torch.Size([6, 1])\n",
      "logit shape: torch.Size([5, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated\n",
      "input_prompt_shape: torch.Size([7, 1])\n",
      "logit shape: torch.Size([6, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran\n",
      "input_prompt_shape: torch.Size([8, 1])\n",
      "logit shape: torch.Size([7, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq\n",
      "input_prompt_shape: torch.Size([9, 1])\n",
      "logit shape: torch.Size([8, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier\n",
      "input_prompt_shape: torch.Size([10, 1])\n",
      "logit shape: torch.Size([9, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski\n",
      "input_prompt_shape: torch.Size([11, 1])\n",
      "logit shape: torch.Size([10, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator\n",
      "input_prompt_shape: torch.Size([12, 1])\n",
      "logit shape: torch.Size([11, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible\n",
      "input_prompt_shape: torch.Size([13, 1])\n",
      "logit shape: torch.Size([12, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants\n",
      "input_prompt_shape: torch.Size([14, 1])\n",
      "logit shape: torch.Size([13, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets\n",
      "input_prompt_shape: torch.Size([15, 1])\n",
      "logit shape: torch.Size([14, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets notices\n",
      "input_prompt_shape: torch.Size([16, 1])\n",
      "logit shape: torch.Size([15, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets notices disdains\n",
      "input_prompt_shape: torch.Size([17, 1])\n",
      "logit shape: torch.Size([16, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets notices disdains crate--this\n",
      "input_prompt_shape: torch.Size([18, 1])\n",
      "logit shape: torch.Size([17, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets notices disdains crate--this toast\n",
      "input_prompt_shape: torch.Size([19, 1])\n",
      "logit shape: torch.Size([18, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets notices disdains crate--this toast whet\n",
      "input_prompt_shape: torch.Size([20, 1])\n",
      "logit shape: torch.Size([19, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets notices disdains crate--this toast whet ticklish\n",
      "input_prompt_shape: torch.Size([21, 1])\n",
      "logit shape: torch.Size([20, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets notices disdains crate--this toast whet ticklish redundant\n",
      "input_prompt_shape: torch.Size([22, 1])\n",
      "logit shape: torch.Size([21, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets notices disdains crate--this toast whet ticklish redundant packing\n",
      "input_prompt_shape: torch.Size([23, 1])\n",
      "logit shape: torch.Size([22, 1, 68813])\n",
      "next_token_shape: torch.Size([1, 1])\n",
      "output:the sky is gangbanger henriksons celebrated tehran gq crummier baranski incinerator --terrible defendants snippets notices disdains crate--this toast whet ticklish redundant packing comprises\n"
     ]
    }
   ],
   "source": [
    "# By using model decoder\n",
    "# Declaring prompt \n",
    "prompt = \"The sky is\"\n",
    "prompt_tokens = prompting(prompt)\n",
    "## By using model output\n",
    "max_new_tokens = 20 # how many words you may allow your model to generate\n",
    "\n",
    "for i in range(max_new_tokens):\n",
    "    \n",
    "\n",
    "    logit = model.decoder(prompt_tokens,src_mask = None)\n",
    "    logit_reshape = logit.transpose(0,1)\n",
    "\n",
    "    logit_prediction = logit_reshape[:,-1]\n",
    "\n",
    "    next_token_encoded = torch.argmax(logit_prediction, dim = -1).reshape(-1,1)\n",
    "\n",
    "    prompt_tokens = torch.cat((prompt_tokens,next_token_encoded),dim =0)\n",
    "\n",
    "    print(f\"input_prompt_shape: {prompt_tokens.shape}\")\n",
    "    print(f\"logit shape: {logit.shape}\")\n",
    "    print(f\"next_token_shape: {next_token_encoded.shape}\")\n",
    "    print(f\"output:{\" \".join([idx_to_eng(tokens) for tokens in prompt_tokens])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff07ab9-fa28-4332-a9ab-76915860c861",
   "metadata": {},
   "source": [
    "# Generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e3f59e8-cf25-419e-8b72-6733b3e16ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(model, prompt=None, max_new_tokens = 15, block_size = BLOCK_SIZE, vocab= vocab, tokenizer=tokenizer):\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    prompt_encoded = prompting(prompt).to(DEVICE)\n",
    "    \n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "\n",
    "        logits = model(prompt_encoded).transpose(0,1)\n",
    "\n",
    "        logit_prediction = logits[:,-1]\n",
    "\n",
    "        encoded_logit = torch.argmax(logit_prediction, dim = -1).reshape(-1,1)\n",
    "\n",
    "\n",
    "        # if the next token is end of text, then stop the generation\n",
    "        if encoded_logit == EOS_IDX:\n",
    "            break\n",
    "\n",
    "        prompt_encoded = torch.cat((prompt_encoded,encoded_logit),dim = 0)[-block_size:]\n",
    "\n",
    "\n",
    "        print(f\"prompt_encoded_shape: {prompt_encoded.shape}\")\n",
    "\n",
    "        \n",
    "\n",
    "    tokens=[idx_to_eng(tokens) for  tokens in prompt_encoded]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6201394c-10b1-412a-8d58-c50994340303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_encoded_shape: torch.Size([3, 1])\n",
      "prompt_encoded_shape: torch.Size([4, 1])\n",
      "prompt_encoded_shape: torch.Size([5, 1])\n",
      "prompt_encoded_shape: torch.Size([6, 1])\n",
      "prompt_encoded_shape: torch.Size([7, 1])\n",
      "prompt_encoded_shape: torch.Size([8, 1])\n",
      "prompt_encoded_shape: torch.Size([9, 1])\n",
      "prompt_encoded_shape: torch.Size([10, 1])\n",
      "prompt_encoded_shape: torch.Size([11, 1])\n",
      "prompt_encoded_shape: torch.Size([12, 1])\n",
      "prompt_encoded_shape: torch.Size([13, 1])\n",
      "prompt_encoded_shape: torch.Size([14, 1])\n",
      "prompt_encoded_shape: torch.Size([15, 1])\n",
      "prompt_encoded_shape: torch.Size([16, 1])\n",
      "prompt_encoded_shape: torch.Size([17, 1])\n"
     ]
    }
   ],
   "source": [
    "prompt_output = generate(model,prompt = \"My love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36e6687a-2b82-4d51-9ee2-d2d33504c4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my love burbridge bad-hairdewed performances stalky bauchau bolivia millimeter tasked quip per√∫ fart-jokes pick-pockets slurp na√Øf use'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d73dac18-8684-4a4c-8526-a4626cfeb692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['was',\n",
       "  'a',\n",
       "  '<pad>',\n",
       "  'to',\n",
       "  'movie',\n",
       "  's',\n",
       "  '.',\n",
       "  '<pad>',\n",
       "  'the',\n",
       "  'that',\n",
       "  ',',\n",
       "  'i',\n",
       "  's',\n",
       "  '<|endoftext|>',\n",
       "  'is',\n",
       "  'that',\n",
       "  'that',\n",
       "  '.',\n",
       "  '<|endoftext|>',\n",
       "  '.']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_lis =[]\n",
    "tokens = torch.randint(0,20,size = (20,1))\n",
    "main_lis.append([idx_to_eng(tok) for tok in tokens])\n",
    "main_lis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b003147-0f03-4cc5-8755-46fdfae432e4",
   "metadata": {},
   "source": [
    "## Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd9ae67-0cc0-4bd9-b784-6166da54b57c",
   "metadata": {},
   "source": [
    "The main difference in training and inferencing lies in the inputs to the decoder.\n",
    "during training, the decoder has the access 0f the ground truth (receiving the exact target sequence tokens incrementally through a technique known as `teacher forcing`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "550dc778-163e-474a-8b34-d5e4d4a9896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src,trg = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d1f3acb-f585-4a10-854f-febbcb17c4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    5],\n",
       "         [  835],\n",
       "         [ 7090],\n",
       "         [    5],\n",
       "         [  291],\n",
       "         [ 3064],\n",
       "         [    4],\n",
       "         [  203],\n",
       "         [    9],\n",
       "         [    4],\n",
       "         [  812],\n",
       "         [    5],\n",
       "         [ 6431],\n",
       "         [ 2091],\n",
       "         [   12],\n",
       "         [   67],\n",
       "         [    5],\n",
       "         [    7],\n",
       "         [55310],\n",
       "         [    4]]),\n",
       " torch.Size([20, 1]),\n",
       " torch.Size([20, 1]),\n",
       " tensor([[  835],\n",
       "         [ 7090],\n",
       "         [    5],\n",
       "         [  291],\n",
       "         [ 3064],\n",
       "         [    4],\n",
       "         [  203],\n",
       "         [    9],\n",
       "         [    4],\n",
       "         [  812],\n",
       "         [    5],\n",
       "         [ 6431],\n",
       "         [ 2091],\n",
       "         [   12],\n",
       "         [   67],\n",
       "         [    5],\n",
       "         [    7],\n",
       "         [55310],\n",
       "         [    4],\n",
       "         [ 9424]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src,src.shape,trg.shape,trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b6321a2-5085-4cdd-aae2-64627f4f0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask,padding_mask =  generate_mask(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15437551-031c-43e6-bdd8-ecbf1b655809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask,padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cd4f13b-38fa-4944-82b3-4ddf8e3a24e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 68813])\n"
     ]
    }
   ],
   "source": [
    "logit = model(src.to(DEVICE),src_mask = mask.to(DEVICE),key_padding_mask = padding_mask.to(DEVICE))\n",
    "print(logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e40770a-d89b-4c18-a456-64f7d3415c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shapetorch.Size([20, 1, 68813])\n",
      "source shape torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"output shape{logit.shape}\")\n",
    "print(f\"source shape {src.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4eb45fb3-2996-4db4-b55d-e524192743ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Target shape: {trg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f79bad9-3d67-4800-9d02-79b01254eb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    5],\n",
       "         [  835],\n",
       "         [ 7090],\n",
       "         [    5],\n",
       "         [  291],\n",
       "         [ 3064],\n",
       "         [    4],\n",
       "         [  203],\n",
       "         [    9],\n",
       "         [    4],\n",
       "         [  812],\n",
       "         [    5],\n",
       "         [ 6431],\n",
       "         [ 2091],\n",
       "         [   12],\n",
       "         [   67],\n",
       "         [    5],\n",
       "         [    7],\n",
       "         [55310],\n",
       "         [    4]]),\n",
       " tensor([[  835],\n",
       "         [ 7090],\n",
       "         [    5],\n",
       "         [  291],\n",
       "         [ 3064],\n",
       "         [    4],\n",
       "         [  203],\n",
       "         [    9],\n",
       "         [    4],\n",
       "         [  812],\n",
       "         [    5],\n",
       "         [ 6431],\n",
       "         [ 2091],\n",
       "         [   12],\n",
       "         [   67],\n",
       "         [    5],\n",
       "         [    7],\n",
       "         [55310],\n",
       "         [    4],\n",
       "         [ 9424]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src,trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1d5f7c7-5876-4190-9d6d-bb51e7e355f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 68813])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "print(logit.reshape(-1,logit.shape[-1]).shape)\n",
    "print(trg.reshape(-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1876f60f-6c1e-4f69-8334-44a4c8381892",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "255f0b83-73bc-4153-b202-d5b602975ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e4f21ee-80ee-441a-867c-56de0b496261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.5584831237793\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(logit.reshape(-1,logit.shape[-1]).to(DEVICE),trg.reshape(-1).to(DEVICE))\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa2d4225-171f-47e0-9902-bc87521a6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,logit,target,device):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src,trg in eval_data:\n",
    "\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            logit = model(src,src_mask = None,key_padding_mask = None).to(device) # logit shape: [seq_len(context size),batch_size,embed_dim] --> [20,1,68813]\n",
    "\n",
    "            total_loss += loss_fn(logit.reshape(-1,logit.shape[-1]),trg.reshape(-1)) \n",
    "            \n",
    "            # logit_shape: [context_size,embed_dim] \n",
    "            # target shape: [context size]\n",
    "            \n",
    "            \n",
    "\n",
    "    return total_loss/ (len(list(eval_data))-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a82ab1-724e-488d-a02a-d8727045552d",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc1175-fd6a-4719-a7df-202932d55da6",
   "metadata": {},
   "source": [
    "#### Optimizer, scheduler, & Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cff755-dd7f-436a-a057-12dd324e74e2",
   "metadata": {},
   "source": [
    "Absolutely ‚Äî you're working with some important hyperparameters here that can significantly affect how well and how fast your model trains. Let's go through them one by one:\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ `weight_decay = 0.01`\n",
    "\n",
    "This is **L2 regularization**.\n",
    "\n",
    "* **What it does:** Adds a penalty to large weights by modifying the loss function:\n",
    "\n",
    "  $$\n",
    "  \\text{Loss}_{\\text{new}} = \\text{Loss}_{\\text{original}} + \\lambda \\sum w^2\n",
    "  $$\n",
    "\n",
    "  Where `Œª` is `weight_decay`.\n",
    "\n",
    "* **Why use it:** It helps prevent **overfitting** by discouraging the model from assigning large weights to any particular input feature.\n",
    "\n",
    "* **Typical range:** `1e-5` to `1e-2`. Your `0.01` is on the strong side ‚Äî that‚Äôs fine for small models, but may be too strong for larger ones.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ `betas = (0.9, 0.999)`\n",
    "\n",
    "These are hyperparameters specific to the **Adam optimizer**.\n",
    "\n",
    "Adam uses **exponentially decaying averages** of past gradients and squared gradients:\n",
    "\n",
    "* **Œ≤‚ÇÅ (0.9):** Decay rate for the first moment (mean of gradients).\n",
    "* **Œ≤‚ÇÇ (0.999):** Decay rate for the second moment (variance of gradients).\n",
    "\n",
    "#### What they mean:\n",
    "\n",
    "* `Œ≤‚ÇÅ = 0.9`: Keeps a memory of the last 10 gradients (roughly), helps stabilize updates.\n",
    "* `Œ≤‚ÇÇ = 0.999`: Keeps a much longer memory (approx. 1000 past squared gradients), helps adapt learning rate per parameter.\n",
    "\n",
    "#### TL;DR:\n",
    "\n",
    "| Beta       | Controls              | Purpose          |\n",
    "| ---------- | --------------------- | ---------------- |\n",
    "| Œ≤‚ÇÅ (0.9)   | Mean of gradients     | Momentum         |\n",
    "| Œ≤‚ÇÇ (0.999) | Variance of gradients | Adaptive scaling |\n",
    "\n",
    "These are **standard default values**. Only tweak if you're doing advanced optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ `scheduler = StepLR(optimizer, step_size=10000, gamma=0.9)`\n",
    "\n",
    "This is a **learning rate scheduler**.\n",
    "\n",
    "#### What it does:\n",
    "\n",
    "* Every **10,000 steps**, it **multiplies** the current learning rate by **0.9** (i.e., decays it by 10%).\n",
    "\n",
    "$$\n",
    "\\text{new\\_lr} = \\text{old\\_lr} \\times \\gamma\n",
    "$$\n",
    "\n",
    "#### Why use it:\n",
    "\n",
    "* Helps the model **start learning fast**, but then **refine slowly** as training progresses.\n",
    "* Avoids overshooting minima in later stages of training.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ How They Work Together\n",
    "\n",
    "During training:\n",
    "\n",
    "1. **Adam** adapts per-parameter updates using running averages.\n",
    "2. **Weight decay** keeps weights small, reducing overfitting.\n",
    "3. **StepLR scheduler** gradually lowers the learning rate to improve convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Example Flow\n",
    "\n",
    "Let‚Äôs say you're training for 50,000 steps:\n",
    "\n",
    "* At step 0: `lr = 0.01`\n",
    "* At step 10,000: `lr = 0.009`\n",
    "* At step 20,000: `lr = 0.0081`\n",
    "* ...\n",
    "* Learning rate gets smaller, allowing more fine-tuned adjustments.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† TL;DR Table\n",
    "\n",
    "| Hyperparameter        | Meaning                  | Purpose                    | Tip                           |\n",
    "| --------------------- | ------------------------ | -------------------------- | ----------------------------- |\n",
    "| `weight_decay=0.01`   | L2 penalty               | Prevent overfitting        | Reduce if underfitting        |\n",
    "| `betas=(0.9, 0.999)`  | Exponential averages     | Stability + adaptivity     | Default is good               |\n",
    "| `gamma=0.9` in StepLR | Learning rate multiplier | Refines training over time | Try `0.95` if decay too sharp |\n",
    "\n",
    "Let me know if you want to try more **adaptive schedulers** like `CosineAnnealingLR` or `ReduceLROnPlateau`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41c09e62-3a06-440f-b3bd-9394594ca2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr = 1e-2,\n",
    "                            weight_decay = 0.01,\n",
    "                            betas = (0.9,0.999))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma = 0.9)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a37533a6-eb75-4147-ae03-4a033fd96288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module,\n",
    "         train_data):\n",
    "\n",
    "    model.train() # enables the model for batch normalization and dropout\n",
    "\n",
    "    total_loss = 0.\n",
    "    log_interval = 10000\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(list(train_data))//BLOCK_SIZE\n",
    "\n",
    "    for batch,srctrg in enumerate(train_data):\n",
    "        src = srctrg[0]\n",
    "        trg = srctrg[1]\n",
    "\n",
    "        src,trg = src.to(DEVICE),trg.to(DEVICE)\n",
    "        #print(f\"Input shape: {src.shape}\")\n",
    "\n",
    "        logits = model(src,src_mask = None)\n",
    "        logits_flat = logits.reshape(-1,logits.shape[-1]).to(DEVICE)\n",
    "        loss = loss_fn(logits_flat,trg.reshape(-1).to(DEVICE))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch % log_interval == 0 and batch >0) or batch==42020:\n",
    "            lr = scheduler.get_last_lr()[0] # get the last learning rate\n",
    "            ms_per_batch = (time.time()-start_time)*1000/log_interval\n",
    "\n",
    "            cur_loss = total_loss/batch\n",
    "            ppl = math.exp(cur_loss)\n",
    "            \n",
    "            print(f\"epoch {epoch:.3d} | {batch//block_size:5d}/{num_batches:5d} batches | \"\n",
    "                 f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "    return total_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd9e7ca4-a343-47fb-abd7-b0fc2a7e9eee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Precision not allowed in integer format specifier",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      8\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model,train_dataloader)\n\u001b[1;32m     10\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model,test_dataloader)\n\u001b[1;32m     11\u001b[0m     val_ppl \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(val_loss)\n",
      "Cell \u001b[0;32mIn[49], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data)\u001b[0m\n\u001b[1;32m     36\u001b[0m         cur_loss \u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m/\u001b[39mbatch\n\u001b[1;32m     37\u001b[0m         ppl \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(cur_loss)\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mblock_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | ms/batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mms_per_batch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcur_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | ppl \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mppl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m         start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "\u001b[0;31mValueError\u001b[0m: Precision not allowed in integer format specifier"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(model,train_dataloader)\n",
    "    val_loss = evaluate(model,test_dataloader)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    elapsed = time.time()-epoch_start_time\n",
    "    print('-'*80)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "        f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('_'*80)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.save_dict(),'model_best_val_loss.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102ccb6-de8a-4e2e-853e-495898a5d3da",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2726c-6455-4005-94e3-2ec3b13bf0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = len(train_losses)\n",
    "\n",
    "# create a figure and a set of subplots\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(num_epochs),train_losses,label = \"Train_losses\")\n",
    "ax.plot(range(num_epochs),val_losses,label = 'Validation losses')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "ax.set_title('Training and Validation Losses')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46340d1-d8b2-407e-8843-2dd3ea44cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kyn1_OsXrzjef0xihlsXmg.pt'\n",
    "model.load_state_dict(torch.load('kyn1_OsXrzjef0xihlsXmg.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3a20d-4750-47e4-af92-f4975854bbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3642ee76-e314-4ee5-8f08-448784d1579d",
   "metadata": {},
   "source": [
    "## Loading GPT2 model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb78a7-3373-464a-a680-a4bc08dd4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer and the model\n",
    "tokenizer1 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# define the input prompt\n",
    "input_text = 'The world is full of'\n",
    "\n",
    "# tokenize the input text and prepare the input for the model \n",
    "input_ids = tokenizer1.encode(input_text,return_tensors= 'pt')\n",
    "\n",
    "\n",
    "# Generate text using the model\n",
    "# set the desired legth of the generated text (max_length),\n",
    "# and other generation parameters like temperature, top_k and top_p\n",
    "\n",
    "max_length = 15\n",
    "temperature = 0.7\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "\n",
    "\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length = max_length,\n",
    "    temperature = temperature,\n",
    "    top_k = top_k,\n",
    "    top_p = top_p,\n",
    "    pad_token_id = tokenizer1.eos_token_id,\n",
    ")\n",
    "\n",
    "# decode the generated text\n",
    "generated_text = tokenizer1.decode(generated_ids[0])\n",
    "\n",
    "# print the input prompt and the generated text\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated_text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df83127-d453-4e0d-abc5-b131bec367ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ebe495e-b43f-4cbd-aa31-5f13c6e565fb",
   "metadata": {},
   "source": [
    "Great question! These three parameters‚Äî**temperature**, **top-k**, and **top-p (nucleus sampling)**‚Äîcontrol how your language model (like GPT-2) generates text. They influence the **creativity vs. predictability** of the output.\n",
    "\n",
    "Let‚Äôs break them down simply:\n",
    "\n",
    "---\n",
    "\n",
    "### üî• `temperature`\n",
    "\n",
    "* Controls **randomness** in token selection.\n",
    "* It's a **scaling factor** applied to the predicted probabilities.\n",
    "* **Lower values (e.g., 0.2‚Äì0.5)** make the model more **confident** and deterministic (boring/repetitive).\n",
    "* **Higher values (e.g., 1.0‚Äì1.5)** make it more **creative**, allowing it to pick less likely words.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```plaintext\n",
    "temperature = 0.1 ‚Üí \"The world is full of hope and love.\"\n",
    "temperature = 1.2 ‚Üí \"The world is full of jellyfish pirates and cheese storms.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ `top_k`\n",
    "\n",
    "* Limits the model to the **top `k` most likely words** at each step.\n",
    "* Randomly samples **only from those `k` words**.\n",
    "* Helps avoid very low-probability tokens (i.e., nonsense).\n",
    "* **Lower `k`** = safer, more predictable.\n",
    "* **Higher `k`** = more varied outputs.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```plaintext\n",
    "top_k = 5 ‚Üí \"The world is full of beauty and grace.\"\n",
    "top_k = 50 ‚Üí \"The world is full of complexity, whispers, and algorithms.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ `top_p` (aka nucleus sampling)\n",
    "\n",
    "* Picks the **smallest set of words** whose combined probability is **‚â• `p`** (e.g., 95%).\n",
    "* Chooses randomly **only from that set**.\n",
    "* More dynamic than top-k since the number of candidate tokens varies depending on the distribution.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```plaintext\n",
    "top_p = 0.9 ‚Üí Usually safe, avoids odd tokens.\n",
    "top_p = 0.7 ‚Üí More conservative, tighter sampling.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è Summary Table\n",
    "\n",
    "| Parameter     | Range   | Effect                          | Recommendation       |\n",
    "| ------------- | ------- | ------------------------------- | -------------------- |\n",
    "| `temperature` | 0.1‚Äì2.0 | Creativity vs. certainty        | 0.7‚Äì1.0 for balance  |\n",
    "| `top_k`       | 0‚Äì100+  | Limit by top-k probabilities    | 40‚Äì100 for diversity |\n",
    "| `top_p`       | 0‚Äì1     | Limit by cumulative probability | 0.9‚Äì0.95 for fluency |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Default/Good Starting Point\n",
    "\n",
    "```python\n",
    "temperature = 0.7\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "```\n",
    "\n",
    "This strikes a **good balance** between fluency and creativity.\n",
    "\n",
    "Let me know if you‚Äôd like to see visualizations or real examples for different settings!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a3c41-211f-4db3-b372-19c3ab5fb1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create an instance with the following parameters:\n",
    "\n",
    "embedding size = 200\n",
    "number of layers = 2\n",
    "number of attention heads = 2\n",
    "dropout probability = 0.2\n",
    "Create a prompt\n",
    "\n",
    "Pass the prompt to model to generate text with a maximum length of 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227d966-9440-4d9b-8bd4-95ac87fc553c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
