{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b52477-d71d-476e-862c-d3c3ceaada45",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf536db5-ac93-4d49-8a48-abbbac662f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import warnings\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28def842-28cf-42e1-9af7-d91b40af9603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.2', '0.17.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__,torchtext.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1043ab3-db0f-4ba8-9969-b25980ec6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = AG_NEWS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e84f4f-511b-441a-bf47-a501db22db9a",
   "metadata": {},
   "source": [
    "## Define Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4dadde-1821-48f0-926d-e3cfd6a45c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(LOSS,ACC):\n",
    "\n",
    "    fig,ax1 = plt.subplots()\n",
    "    color = \"tab:red\"\n",
    "    ax1.plot(LOSS)\n",
    "    ax1.set_xlabel('epochs',color = color)\n",
    "    ax1.set_ylabel('Cost',color = color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy',color = color)\n",
    "    ax2.plot(ACC, color = color)\n",
    "    fig.tight_layout()\n",
    "    ax2.tick_params(axis = 'y', color = color)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89ee45-1b88-419b-a9f3-0cb33a2bfb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(my_embeddings,name,vocab):\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax = fig.add_subplot(111,projection = '3d')\n",
    "\n",
    "    # Plot the datapoints\n",
    "    ax.scatter(my_embeddings[:,0], my_embeddings[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabca79-6e92-4f6d-8c59-87cae60e241e",
   "metadata": {},
   "source": [
    "## Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "039ef22f-4897-47eb-9ebd-4290435b2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \"),\n",
    "    (3,\"he painted the car red\"),\n",
    "    (1,\"he painted the red car\")\n",
    "    ]\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "\n",
    "    for _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset),specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c20743-1c44-404e-9609-2a2676788bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "def label_pipeline(x):\n",
    "    return int(x)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bc09a-d5ee-48d2-ab53-d09be90f8f24",
   "metadata": {},
   "source": [
    "## Zero Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82aeb945-9181-48be-abef-8a00e6086d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1]),\n",
       " tensor([1, 2]),\n",
       " tensor([1, 2, 3]),\n",
       " tensor([1, 2, 3, 4]),\n",
       " tensor([1, 2, 3, 4, 5]),\n",
       " tensor([1, 2, 3, 4, 5, 6]),\n",
       " tensor([1, 2, 3, 4, 5, 6, 7]),\n",
       " tensor([1, 2, 3, 4, 5, 6, 7, 8])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [torch.tensor ([j for j in range (1,i)]) for i in range(2,10)]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37be767b-91e0-4934-a9cd-2937c3b7f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 2, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 2, 3, 0, 0, 0, 0, 0],\n",
      "        [1, 2, 3, 4, 0, 0, 0, 0],\n",
      "        [1, 2, 3, 4, 5, 0, 0, 0],\n",
      "        [1, 2, 3, 4, 5, 6, 0, 0],\n",
      "        [1, 2, 3, 4, 5, 6, 7, 0],\n",
      "        [1, 2, 3, 4, 5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# pad \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_sequence = pad_sequence(sequences,batch_first = True, padding_value = 0)\n",
    "print(padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70b659a3-e7be-4a71-8e68-e19362aaaaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "my_tokens = \"he painted the car red he painted the red car\"\n",
    "\n",
    "my_index = text_pipeline(my_tokens)\n",
    "my_index\n",
    "\n",
    "embedding_dim = 3\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffd513be-ffb4-44f2-99b9-012be34874ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.3464e+00,  1.4493e-03, -2.5309e-01],\n",
       "        [-6.0592e-01,  7.6708e-02,  1.0916e+00],\n",
       "        [ 6.6008e-04,  1.1636e-01,  1.3907e+00],\n",
       "        [ 7.8528e-01,  2.3689e-01,  7.0954e-01],\n",
       "        [-1.7754e+00, -6.0924e-01,  6.7999e-02],\n",
       "        [-1.3510e+00,  1.0454e-01, -1.5773e+00],\n",
       "        [ 1.0209e+00, -1.8551e-01,  1.2583e+00],\n",
       "        [ 4.1363e-01, -5.2709e-01,  9.9344e-01],\n",
       "        [-1.2712e+00, -6.3085e-02,  1.1929e+00],\n",
       "        [ 4.3014e-01, -5.7862e-01,  9.9738e-01],\n",
       "        [ 1.9295e+00, -7.6916e-01,  8.3594e-01],\n",
       "        [-1.0519e+00,  2.0030e-01, -1.5784e+00],\n",
       "        [-3.0331e-01,  3.1823e-01, -1.5905e-02],\n",
       "        [ 5.3973e-02, -1.1571e+00, -1.0002e+00],\n",
       "        [ 5.4749e-01, -8.6171e-01, -1.8179e+00],\n",
       "        [-5.4248e-01, -2.1521e+00, -1.3743e-01],\n",
       "        [-8.7264e-01, -1.4662e+00,  3.9409e-01],\n",
       "        [-3.5706e-01,  7.6975e-01, -8.6329e-01],\n",
       "        [ 2.1589e+00,  1.7368e+00, -3.9517e-01],\n",
       "        [-3.3711e-01,  1.9707e-01, -1.1146e+00],\n",
       "        [ 7.7392e-01, -7.2830e-02, -1.0343e+00],\n",
       "        [ 1.2432e-01,  1.1127e+00,  6.4839e-01],\n",
       "        [-5.0740e-01,  4.0995e-01, -1.2421e+00],\n",
       "        [ 1.1878e+00, -1.5041e-01,  1.2596e-01],\n",
       "        [-1.4511e+00, -9.7536e-01,  6.2316e-01],\n",
       "        [ 1.5273e+00, -8.4952e-02,  2.0693e-01],\n",
       "        [ 7.9981e-01,  2.0783e-01, -2.2166e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3344735-f60b-489b-8987-b3ac25239018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 13, 15, 11, 14, 12, 13, 15, 14, 11]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dd5ccbe-c568-4f67-8b53-bfc7f47de931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.],\n",
       "        [11.],\n",
       "        [12.],\n",
       "        [13.],\n",
       "        [14.],\n",
       "        [15.],\n",
       "        [16.],\n",
       "        [17.],\n",
       "        [18.],\n",
       "        [19.],\n",
       "        [20.],\n",
       "        [21.],\n",
       "        [22.],\n",
       "        [23.],\n",
       "        [24.],\n",
       "        [25.],\n",
       "        [26.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0,vocab_size, dtype = torch.float).unsqueeze(1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92284846-580a-4eb5-819e-34fe9843935c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " 'nlp',\n",
       " 'pytorch',\n",
       " 'analysis',\n",
       " 'entity',\n",
       " 'machine',\n",
       " 'named',\n",
       " 'sentiment',\n",
       " 'translation',\n",
       " 'with',\n",
       " ',',\n",
       " 'car',\n",
       " 'he',\n",
       " 'painted',\n",
       " 'red',\n",
       " 'the',\n",
       " 'basics',\n",
       " 'classification',\n",
       " 'for',\n",
       " 'introduction',\n",
       " 'of',\n",
       " 'recognition',\n",
       " 'techniques',\n",
       " 'text',\n",
       " 'to',\n",
       " 'using',\n",
       " 'vs']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acc25872-22bc-4d86-ac58-12de4be860aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 3\n",
    "pe = torch.zeros(vocab_size,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69b2cb87-97ab-45e9-94bf-fe7c87602917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1007d675-6e84-4c2d-a699-c29d1ae1b6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 3.,  3.,  3.],\n",
       "        [ 4.,  4.,  4.],\n",
       "        [ 5.,  5.,  5.],\n",
       "        [ 6.,  6.,  6.],\n",
       "        [ 7.,  7.,  7.],\n",
       "        [ 8.,  8.,  8.],\n",
       "        [ 9.,  9.,  9.],\n",
       "        [10., 10., 10.],\n",
       "        [11., 11., 11.],\n",
       "        [12., 12., 12.],\n",
       "        [13., 13., 13.],\n",
       "        [14., 14., 14.],\n",
       "        [15., 15., 15.],\n",
       "        [16., 16., 16.],\n",
       "        [17., 17., 17.],\n",
       "        [18., 18., 18.],\n",
       "        [19., 19., 19.],\n",
       "        [20., 20., 20.],\n",
       "        [21., 21., 21.],\n",
       "        [22., 22., 22.],\n",
       "        [23., 23., 23.],\n",
       "        [24., 24., 24.],\n",
       "        [25., 25., 25.],\n",
       "        [26., 26., 26.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.cat((position,position, position),dim = 1)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71e6d63b-003f-4c16-a205-07557be7fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embedings = embedding(torch.tensor(my_index)).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46e888da-9973-49ce-941b-15be529ae131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples, dim = my_embedings.shape\n",
    "samples, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70674bec-37d2-4072-8308-ca7b0f816c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30330598,  0.3182314 , -0.01590507],\n",
       "       [ 0.05397258, -1.157111  , -1.0001758 ],\n",
       "       [-0.5424799 , -2.1520617 , -0.13742737],\n",
       "       [-1.0519241 ,  0.20030454, -1.5783936 ],\n",
       "       [ 0.54749435, -0.8617089 , -1.8179109 ],\n",
       "       [-0.30330598,  0.3182314 , -0.01590507],\n",
       "       [ 0.05397258, -1.157111  , -1.0001758 ],\n",
       "       [-0.5424799 , -2.1520617 , -0.13742737],\n",
       "       [ 0.54749435, -0.8617089 , -1.8179109 ],\n",
       "       [-1.0519241 ,  0.20030454, -1.5783936 ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfce08b3-0db8-4236-beea-7594c0ca245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.0330598e-01,  3.1823140e-01, -1.5905073e-02],\n",
       "       [ 1.0539726e+00, -1.5711105e-01, -1.7583370e-04],\n",
       "       [ 1.4575201e+00, -1.5206170e-01,  1.8625727e+00],\n",
       "       [ 1.9480759e+00,  3.2003045e+00,  1.4216064e+00],\n",
       "       [ 4.5474944e+00,  3.1382911e+00,  2.1820891e+00],\n",
       "       [ 4.6966939e+00,  5.3182316e+00,  4.9840951e+00],\n",
       "       [ 6.0539727e+00,  4.8428888e+00,  4.9998240e+00],\n",
       "       [ 6.4575200e+00,  4.8479385e+00,  6.8625727e+00],\n",
       "       [ 8.5474939e+00,  7.1382914e+00,  6.1820889e+00],\n",
       "       [ 7.9480758e+00,  9.2003050e+00,  7.4216065e+00]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embding = my_embedings + pe[0:samples, :].numpy()\n",
    "pos_embding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "942467f0-9063-4725-904b-1aa4dd5cf088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9480759, 3.2003045, 1.4216064], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embding[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7488f176-db90-43ea-acdf-684da3acd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,embedding_dim,vocab_size=5000,dropout = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout( p = dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, embedding_dim)\n",
    "        position = torch.arange(0,vocab_size, dtype = torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0,embedding_dim, 2).float()\n",
    "            * (-math.log(10000.0)/embedding_dim)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position*div_term)\n",
    "\n",
    "        pe= pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x+ self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0775136-e2d5-4edc-ab67-2164e8d1b3b1",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffc2c84e-5c2b-453f-8206-46b083bde0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embdings = embedding(torch.tensor(my_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "539cb393-aa8b-462a-904b-a4b116d4abc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3033,  0.3182, -0.0159],\n",
       "        [ 0.0540, -1.1571, -1.0002],\n",
       "        [-0.5425, -2.1521, -0.1374],\n",
       "        [-1.0519,  0.2003, -1.5784],\n",
       "        [ 0.5475, -0.8617, -1.8179],\n",
       "        [-0.3033,  0.3182, -0.0159],\n",
       "        [ 0.0540, -1.1571, -1.0002],\n",
       "        [-0.5425, -2.1521, -0.1374],\n",
       "        [ 0.5475, -0.8617, -1.8179],\n",
       "        [-1.0519,  0.2003, -1.5784]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6917c84-a9e2-4319-bab8-fa2163e861b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 13, 15, 11, 14, 12, 13, 15, 14, 11])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(my_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2222ac73-61bb-475d-8197-dbef48881c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(27, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1aed803-6b5c-4f06-bd43-188a82901b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embdings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad703bf3-ea57-43b8-9c42-c8bc0c6e4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=3,\n",
    "    nhead = 1,\n",
    "    dim_feedforward=1,\n",
    "    dropout = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8acf2834-5f7a-4f75-973f-026071521d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3975,  1.3741, -0.9766],\n",
       "        [ 1.3787, -0.4165, -0.9622],\n",
       "        [ 0.9005, -1.3946,  0.4942],\n",
       "        [ 0.1794,  1.1251, -1.3046],\n",
       "        [ 1.2589, -0.0714, -1.1875],\n",
       "        [-0.3975,  1.3741, -0.9766],\n",
       "        [ 1.3787, -0.4165, -0.9622],\n",
       "        [ 0.9005, -1.3946,  0.4942],\n",
       "        [ 1.2589, -0.0714, -1.1875],\n",
       "        [ 0.1794,  1.1251, -1.3046]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = encoder_layer(my_embdings)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb05962f-96d2-4a3b-a6eb-60af55f4b2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9868e-08, -3.9736e-08,  4.9671e-08,  3.9736e-08,  0.0000e+00,\n",
       "         1.9868e-08, -3.9736e-08,  4.9671e-08,  0.0000e+00,  0.0000e+00],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.mean(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da85e605-9246-477e-8110-41aca8451844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn.in_proj_weight torch.Size([9, 3])\n",
      "self_attn.in_proj_bias torch.Size([9])\n",
      "self_attn.out_proj.weight torch.Size([3, 3])\n",
      "self_attn.out_proj.bias torch.Size([3])\n",
      "linear1.weight torch.Size([1, 3])\n",
      "linear1.bias torch.Size([1])\n",
      "linear2.weight torch.Size([3, 1])\n",
      "linear2.bias torch.Size([3])\n",
      "norm1.weight torch.Size([3])\n",
      "norm1.bias torch.Size([3])\n",
      "norm2.weight torch.Size([3])\n",
      "norm2.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "params_dict = encoder_layer.state_dict()\n",
    "\n",
    "# Print the parameter names and shapes\n",
    "for name, param in params_dict.items():\n",
    "    print(name,param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1db2c59-7785-414e-bb97-877cae417648",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 3\n",
    "q_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][0:embed_dim].t()\n",
    "k_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][embed_dim:2*embed_dim].t()\n",
    "v_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][2*embed_dim:3*embed_dim].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4398715-db18-408d-bac5-7bc4963b22bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3882,  0.0439,  0.4021],\n",
       "         [ 0.1002, -0.1283, -0.1279],\n",
       "         [ 0.2260, -0.6599, -0.4037]]),\n",
       " tensor([[-0.6860, -0.5836,  0.0396],\n",
       "         [-0.2887,  0.4315,  0.5349],\n",
       "         [-0.2767,  0.1194, -0.6030]]),\n",
       " tensor([[-0.4280, -0.0283, -0.4744],\n",
       "         [-0.3591, -0.6797, -0.0774],\n",
       "         [-0.3855,  0.2779,  0.6169]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj_weight,k_proj_weight,v_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc5a0cc5-e328-4c6c-bdeb-fe272642c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = my_embdings @ q_proj_weight\n",
    "K = my_embdings @ k_proj_weight\n",
    "V = my_embdings @ v_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69306873-8c96-4e8a-b2ef-3d015f7de56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1460, -0.0436, -0.1562],\n",
       "        [-0.3630,  0.8108,  0.5736],\n",
       "        [-0.0362,  0.3429,  0.1127],\n",
       "        [ 0.0717,  0.9697,  0.1887],\n",
       "        [-0.7097,  1.3343,  1.0644],\n",
       "        [ 0.1460, -0.0436, -0.1562],\n",
       "        [-0.3630,  0.8108,  0.5736],\n",
       "        [-0.0362,  0.3429,  0.1127],\n",
       "        [-0.7097,  1.3343,  1.0644],\n",
       "        [ 0.0717,  0.9697,  0.1887]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c415a360-2537-411d-af34-f86efad07078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0128,  0.0660,  0.2011, -0.0119, -0.0047, -0.0128,  0.0660,  0.2011,\n",
       "         -0.0047, -0.0119],\n",
       "        [ 0.1765, -0.4291, -0.8712,  0.3459, -0.2865,  0.1765, -0.4291, -0.8712,\n",
       "         -0.2865,  0.3459],\n",
       "        [ 0.0702, -0.1416, -0.2168,  0.1445, -0.1449,  0.0702, -0.1416, -0.2168,\n",
       "         -0.1449,  0.1445],\n",
       "        [ 0.1982, -0.3418, -0.4278,  0.4430, -0.4214,  0.1982, -0.3418, -0.4278,\n",
       "         -0.4214,  0.4430],\n",
       "        [ 0.2944, -0.7444, -1.5764,  0.5685, -0.4502,  0.2944, -0.7444, -1.5764,\n",
       "         -0.4502,  0.5685],\n",
       "        [-0.0128,  0.0660,  0.2011, -0.0119, -0.0047, -0.0128,  0.0660,  0.2011,\n",
       "         -0.0047, -0.0119],\n",
       "        [ 0.1765, -0.4291, -0.8712,  0.3459, -0.2865,  0.1765, -0.4291, -0.8712,\n",
       "         -0.2865,  0.3459],\n",
       "        [ 0.0702, -0.1416, -0.2168,  0.1445, -0.1449,  0.0702, -0.1416, -0.2168,\n",
       "         -0.1449,  0.1445],\n",
       "        [ 0.2944, -0.7444, -1.5764,  0.5685, -0.4502,  0.2944, -0.7444, -1.5764,\n",
       "         -0.4502,  0.5685],\n",
       "        [ 0.1982, -0.3418, -0.4278,  0.4430, -0.4214,  0.1982, -0.3418, -0.4278,\n",
       "         -0.4214,  0.4430]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = Q@K.T/np.sqrt(embed_dim)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9cc94ffa-5708-4ae2-b8f8-b05d1cece6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7399,  0.3077, -0.3537],\n",
       "        [ 0.6669, -0.0094, -0.3993],\n",
       "        [ 0.7053,  0.1675, -0.3762],\n",
       "        [ 0.6913,  0.0489, -0.3471],\n",
       "        [ 0.6447, -0.1506, -0.3951],\n",
       "        [ 0.7399,  0.3077, -0.3537],\n",
       "        [ 0.6669, -0.0094, -0.3993],\n",
       "        [ 0.7053,  0.1675, -0.3762],\n",
       "        [ 0.6447, -0.1506, -0.3951],\n",
       "        [ 0.6913,  0.0489, -0.3471]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = nn.Softmax(dim = 1)(scores) @ V\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40e406cf-cf42-488b-a57d-ff43bde3c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b157294-ac01-4cbf-93e1-6933225ba958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.self_attn.in_proj_weight torch.Size([9, 3])\n",
      "layers.0.self_attn.in_proj_bias torch.Size([9])\n",
      "layers.0.self_attn.out_proj.weight torch.Size([3, 3])\n",
      "layers.0.self_attn.out_proj.bias torch.Size([3])\n",
      "layers.0.linear1.weight torch.Size([1, 3])\n",
      "layers.0.linear1.bias torch.Size([1])\n",
      "layers.0.linear2.weight torch.Size([3, 1])\n",
      "layers.0.linear2.bias torch.Size([3])\n",
      "layers.0.norm1.weight torch.Size([3])\n",
      "layers.0.norm1.bias torch.Size([3])\n",
      "layers.0.norm2.weight torch.Size([3])\n",
      "layers.0.norm2.bias torch.Size([3])\n",
      "layers.1.self_attn.in_proj_weight torch.Size([9, 3])\n",
      "layers.1.self_attn.in_proj_bias torch.Size([9])\n",
      "layers.1.self_attn.out_proj.weight torch.Size([3, 3])\n",
      "layers.1.self_attn.out_proj.bias torch.Size([3])\n",
      "layers.1.linear1.weight torch.Size([1, 3])\n",
      "layers.1.linear1.bias torch.Size([1])\n",
      "layers.1.linear2.weight torch.Size([3, 1])\n",
      "layers.1.linear2.bias torch.Size([3])\n",
      "layers.1.norm1.weight torch.Size([3])\n",
      "layers.1.norm1.bias torch.Size([3])\n",
      "layers.1.norm2.weight torch.Size([3])\n",
      "layers.1.norm2.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "params_dict = transformer_encoder.state_dict()\n",
    "for name, param in params_dict.items():\n",
    "    print(name,param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7031ae-a71e-45bd-bd36-d887cb205d8f",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "afcb3c55-1524-4063-ab19-611e8bc3dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Bank dataset\n",
    "train_iter = AG_NEWS(split = 'train')\n",
    "\n",
    "# AG_NEWS is an iterable dataset, that should be used with an iterator\n",
    "y,text = next(iter(train_iter))\n",
    "\n",
    "y,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d40c4a45-e227-4954-b024-ccc58d8eecf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Business'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "ag_news_label[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f0ecd48-f13c-4ea7-9636-bf0b6dc4bb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(set([label for label,text in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d53e398-1e76-41d3-8f5c-d40d914ed119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter),specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1658948-bd96-4e9b-9444-8cd64cbb01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(list(vocab.get_itos()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db865802-f90f-411c-8064-86fc502bf994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95811"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4522e6ea-1c6e-41b4-b739-8480b474fc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2120, 12544]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['age','hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3f518fb-4f7e-4f47-93a5-38dad6e9f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50706"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['shit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec016fde-63a3-42be-af9a-a90a34c8420d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45696"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['fuck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce0ee485-6f9c-4f36-a4c4-f1830dde6c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25731"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['damn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "54bd4c18-d01a-4a33-977f-1b9d9640dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# Convert the training and testing iterators to map-style datasets\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# now randomly split the training data into train and validation dataset\n",
    "train_length = int(0.95*len(train_dataset))\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "train_dataset, validation_dataset = random_split(train_dataset,[train_length,len(train_dataset)-train_length] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d1c8ede-40c3-495a-ac32-4a92dc1a4e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114000, 6000, 7600)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(validation_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b07f938-fcf5-4349-9017-bac5dfcbef3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c168d-01bc-4d1f-925b-995362005292",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af6817-8207-4497-9cbb-047c517b8d86",
   "metadata": {},
   "source": [
    "### Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ed70985a-46f5-4b41-864a-0ba4efc37fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    label_list, text_list = [],[]\n",
    "\n",
    "    for label, text in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        text_list.append(torch.tensor(text_pipeline(text), dtype = torch.int64))\n",
    "\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype = torch.int64)\n",
    "    text_list = pad_sequence(text_list,batch_first = True)\n",
    "\n",
    "    return label_list.to(device),text_list.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0d9d16c1-134f-41eb-91a5-ad2ab7f8f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    validation_dataset, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e4b203d2-3950-4b64-8aa0-41d7f21ff53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label, sequence = next(iter(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eac9e0de-8d4a-47b7-82ff-a1859a385f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 1, 2, 3, 0, 1, 0, 2, 2, 2, 2, 3, 3, 3, 1, 3, 3, 0, 1, 3, 1, 1, 3, 1,\n",
       "         1, 3, 3, 3, 2, 0, 0, 1, 2, 0, 2, 0, 3, 3, 1, 3, 1, 0, 1, 0, 2, 3, 1, 3,\n",
       "         3, 0, 0, 3, 2, 3, 3, 3, 0, 2, 2, 0, 3, 0, 1, 2], device='mps:0'),\n",
       " tensor([[ 5405,    82,  1778,  ...,     0,     0,     0],\n",
       "         [ 9126,    59,    11,  ...,     0,     0,     0],\n",
       "         [25627,     4, 14139,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 2180,  9071,    59,  ...,     0,     0,     0],\n",
       "         [13242,   540,  2946,  ...,     0,     0,     0],\n",
       "         [  318,   294,    43,  ...,     0,     0,     0]], device='mps:0'))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label,sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad408127-5c35-4deb-bb98-8320324092f3",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b60f81a-2de8-4510-bb76-8301d975ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    \"\"\"Text classifier based on a pytorch Transformer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_class,\n",
    "        embedding_dim = 100,\n",
    "        nhead = 5,\n",
    "        dim_feedforward = 2048,\n",
    "        num_layers = 6,\n",
    "        dropout = 0.1,\n",
    "        activation = 'relu',\n",
    "        classifier_dropout = 0.1\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            embedding_dim = embedding_dim,\n",
    "            dropout = dropout,\n",
    "            vocab_size = vocab_size\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead = nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout = dropout\n",
    "\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(embedding_dim,num_class)\n",
    "        self.d_model = embedding_dim\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim = 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "88d0c219-3998-4711-a724-2be6f189fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cd22e055-f36f-4d31-8cf9-91b26f8a3b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 3, 3, 0, 1, 1, 3, 0, 0, 2, 1, 2, 1, 2, 0, 2, 2, 1, 2, 2, 2, 3, 0, 1,\n",
       "         1, 0, 0, 3, 3, 0, 1, 2, 0, 1, 2, 3, 0, 2, 0, 0, 2, 1, 3, 0, 3, 1, 3, 0,\n",
       "         0, 0, 2, 1, 2, 2, 0, 2, 2, 2, 3, 2, 2, 3, 3, 0], device='mps:0'),\n",
       " tensor([[  383,  4256,  9606,  ...,     0,     0,     0],\n",
       "         [  333,   263,    12,  ...,     0,     0,     0],\n",
       "         [ 2991,  1150,  2534,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [   77, 12350,   127,  ...,     0,     0,     0],\n",
       "         [14034,     8,  2843,  ...,     0,     0,     0],\n",
       "         [ 3899,     3,   162,  ...,     0,     0,     0]], device='mps:0'))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f4d6f653-420e-4107-a476-0693543c5be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(95811, 100)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=100, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emsize = 64\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = Net(vocab_size = vocab_size, num_class = 4).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dd2e6fef-371c-418a-8687-aaab859c05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "14f78fad-c877-48f4-907f-af3870d3a86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 4]), torch.Size([64, 79]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label.shape,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e1454188-fee2-400a-8a11-89fe65041447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,text_pipeline):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device) # Add the batch size\n",
    "\n",
    "        output = model(text)\n",
    "\n",
    "        return ag_news_label[output.argmax(1).item()+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0b913c34-e39f-4b4c-852e-1ca6be8da551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bf83d44f-099b-466a-8b2f-cbec0a6456eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sci/Tec'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I like sports and stuff\",text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "935ee493-11e8-42ce-8c55-9b181699825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model_eval):\n",
    "\n",
    "    model_eval.eval()\n",
    "\n",
    "    total_acc, total_count = 0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for idx, (label,text) in enumerate(dataloader):\n",
    "\n",
    "            predicted_label = model_eval(text.to(device))\n",
    "\n",
    "            total_acc += (predicted_label.argmax(1)==label).sum().item()\n",
    "\n",
    "            total_count += label.size(0)\n",
    "\n",
    "\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b35a2988-7fd4-4503-a04d-afe899e02996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label = [1,2,3,4,3,2,1,1]\n",
    "label = [0,1,3,4,2,2,2,2]\n",
    "\n",
    "eval_value = evaluate(pred_label,label)\n",
    "eval_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31021ee-ae23-4d85-9909-f21ee42e352a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "483b2324-ab2b-450e-ad59-b9a1d1d0cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer , 1.0, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "43adf757-7748-40cb-a692-e03ddfeaf149",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 7.03 GB, other allocations: 2.01 GB, max allowed: 9.07 GB). Tried to allocate 60.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m label,text \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device), text\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m model(text)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predicted_label, label)\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[108], line 50\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(x)\n\u001b[0;32m---> 50\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(x)\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:391\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    388\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 391\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(output, src_mask\u001b[38;5;241m=\u001b[39mmask, is_causal\u001b[38;5;241m=\u001b[39mis_causal, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    394\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[0;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:730\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 730\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x))))\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 7.03 GB, other allocations: 2.01 GB, max allowed: 9.07 GB). Tried to allocate 60.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "EPOCHS =3\n",
    "cum_loss_list = []\n",
    "acc_epoch = []\n",
    "acc_old = 0\n",
    "\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    model.train()\n",
    "\n",
    "    cum_loss = 0\n",
    "\n",
    "    for idx, (label, text) in enumerate(train_dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        label,text = label.to(device), text.to(device)\n",
    "\n",
    "        predicted_label = model(text)\n",
    "\n",
    "        loss = loss_fn(predicted_label, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss: {cum_loss}\")\n",
    "\n",
    "    cum_loss_list.append(cum_loss)\n",
    "\n",
    "    acc = evaluate(valid_dataloader,model)\n",
    "\n",
    "    acc_epoch.append(acc)\n",
    "\n",
    "    if acc> acc_old:\n",
    "        add_old = acc\n",
    "\n",
    "        torch.save(model.state_dict(),'my_model.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca60796-d666-4eb3-af6b-93f557be3a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
