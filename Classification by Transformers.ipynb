{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b52477-d71d-476e-862c-d3c3ceaada45",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf536db5-ac93-4d49-8a48-abbbac662f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import warnings\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28def842-28cf-42e1-9af7-d91b40af9603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.2', '0.17.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__,torchtext.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1043ab3-db0f-4ba8-9969-b25980ec6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = AG_NEWS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e84f4f-511b-441a-bf47-a501db22db9a",
   "metadata": {},
   "source": [
    "## Define Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4dadde-1821-48f0-926d-e3cfd6a45c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(LOSS,ACC):\n",
    "\n",
    "    fig,ax1 = plt.subplots()\n",
    "    color = \"tab:red\"\n",
    "    ax1.plot(LOSS)\n",
    "    ax1.set_xlabel('epochs',color = color)\n",
    "    ax1.set_ylabel('Cost',color = color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy',color = color)\n",
    "    ax2.plot(ACC, color = color)\n",
    "    fig.tight_layout()\n",
    "    ax2.tick_params(axis = 'y', color = color)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89ee45-1b88-419b-a9f3-0cb33a2bfb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(my_embeddings,name,vocab):\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax = fig.add_subplot(111,projection = '3d')\n",
    "\n",
    "    # Plot the datapoints\n",
    "    ax.scatter(my_embeddings[:,0], my_embeddings[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabca79-6e92-4f6d-8c59-87cae60e241e",
   "metadata": {},
   "source": [
    "## Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "039ef22f-4897-47eb-9ebd-4290435b2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \"),\n",
    "    (3,\"he painted the car red\"),\n",
    "    (1,\"he painted the red car\")\n",
    "    ]\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "\n",
    "    for _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset),specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c20743-1c44-404e-9609-2a2676788bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "def label_pipeline(x):\n",
    "    return int(x)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bc09a-d5ee-48d2-ab53-d09be90f8f24",
   "metadata": {},
   "source": [
    "## Zero Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82aeb945-9181-48be-abef-8a00e6086d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1]),\n",
       " tensor([1, 2]),\n",
       " tensor([1, 2, 3]),\n",
       " tensor([1, 2, 3, 4]),\n",
       " tensor([1, 2, 3, 4, 5]),\n",
       " tensor([1, 2, 3, 4, 5, 6]),\n",
       " tensor([1, 2, 3, 4, 5, 6, 7]),\n",
       " tensor([1, 2, 3, 4, 5, 6, 7, 8])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [torch.tensor ([j for j in range (1,i)]) for i in range(2,10)]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37be767b-91e0-4934-a9cd-2937c3b7f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 2, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 2, 3, 0, 0, 0, 0, 0],\n",
      "        [1, 2, 3, 4, 0, 0, 0, 0],\n",
      "        [1, 2, 3, 4, 5, 0, 0, 0],\n",
      "        [1, 2, 3, 4, 5, 6, 0, 0],\n",
      "        [1, 2, 3, 4, 5, 6, 7, 0],\n",
      "        [1, 2, 3, 4, 5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# pad \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_sequence = pad_sequence(sequences,batch_first = True, padding_value = 0)\n",
    "print(padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70b659a3-e7be-4a71-8e68-e19362aaaaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "my_tokens = \"he painted the car red he painted the red car\"\n",
    "\n",
    "my_index = text_pipeline(my_tokens)\n",
    "my_index\n",
    "\n",
    "embedding_dim = 3\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffd513be-ffb4-44f2-99b9-012be34874ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.3464e+00,  1.4493e-03, -2.5309e-01],\n",
       "        [-6.0592e-01,  7.6708e-02,  1.0916e+00],\n",
       "        [ 6.6008e-04,  1.1636e-01,  1.3907e+00],\n",
       "        [ 7.8528e-01,  2.3689e-01,  7.0954e-01],\n",
       "        [-1.7754e+00, -6.0924e-01,  6.7999e-02],\n",
       "        [-1.3510e+00,  1.0454e-01, -1.5773e+00],\n",
       "        [ 1.0209e+00, -1.8551e-01,  1.2583e+00],\n",
       "        [ 4.1363e-01, -5.2709e-01,  9.9344e-01],\n",
       "        [-1.2712e+00, -6.3085e-02,  1.1929e+00],\n",
       "        [ 4.3014e-01, -5.7862e-01,  9.9738e-01],\n",
       "        [ 1.9295e+00, -7.6916e-01,  8.3594e-01],\n",
       "        [-1.0519e+00,  2.0030e-01, -1.5784e+00],\n",
       "        [-3.0331e-01,  3.1823e-01, -1.5905e-02],\n",
       "        [ 5.3973e-02, -1.1571e+00, -1.0002e+00],\n",
       "        [ 5.4749e-01, -8.6171e-01, -1.8179e+00],\n",
       "        [-5.4248e-01, -2.1521e+00, -1.3743e-01],\n",
       "        [-8.7264e-01, -1.4662e+00,  3.9409e-01],\n",
       "        [-3.5706e-01,  7.6975e-01, -8.6329e-01],\n",
       "        [ 2.1589e+00,  1.7368e+00, -3.9517e-01],\n",
       "        [-3.3711e-01,  1.9707e-01, -1.1146e+00],\n",
       "        [ 7.7392e-01, -7.2830e-02, -1.0343e+00],\n",
       "        [ 1.2432e-01,  1.1127e+00,  6.4839e-01],\n",
       "        [-5.0740e-01,  4.0995e-01, -1.2421e+00],\n",
       "        [ 1.1878e+00, -1.5041e-01,  1.2596e-01],\n",
       "        [-1.4511e+00, -9.7536e-01,  6.2316e-01],\n",
       "        [ 1.5273e+00, -8.4952e-02,  2.0693e-01],\n",
       "        [ 7.9981e-01,  2.0783e-01, -2.2166e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3344735-f60b-489b-8987-b3ac25239018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 13, 15, 11, 14, 12, 13, 15, 14, 11]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dd5ccbe-c568-4f67-8b53-bfc7f47de931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.],\n",
       "        [11.],\n",
       "        [12.],\n",
       "        [13.],\n",
       "        [14.],\n",
       "        [15.],\n",
       "        [16.],\n",
       "        [17.],\n",
       "        [18.],\n",
       "        [19.],\n",
       "        [20.],\n",
       "        [21.],\n",
       "        [22.],\n",
       "        [23.],\n",
       "        [24.],\n",
       "        [25.],\n",
       "        [26.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0,vocab_size, dtype = torch.float).unsqueeze(1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92284846-580a-4eb5-819e-34fe9843935c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " 'nlp',\n",
       " 'pytorch',\n",
       " 'analysis',\n",
       " 'entity',\n",
       " 'machine',\n",
       " 'named',\n",
       " 'sentiment',\n",
       " 'translation',\n",
       " 'with',\n",
       " ',',\n",
       " 'car',\n",
       " 'he',\n",
       " 'painted',\n",
       " 'red',\n",
       " 'the',\n",
       " 'basics',\n",
       " 'classification',\n",
       " 'for',\n",
       " 'introduction',\n",
       " 'of',\n",
       " 'recognition',\n",
       " 'techniques',\n",
       " 'text',\n",
       " 'to',\n",
       " 'using',\n",
       " 'vs']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acc25872-22bc-4d86-ac58-12de4be860aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 3\n",
    "pe = torch.zeros(vocab_size,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69b2cb87-97ab-45e9-94bf-fe7c87602917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1007d675-6e84-4c2d-a699-c29d1ae1b6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 3.,  3.,  3.],\n",
       "        [ 4.,  4.,  4.],\n",
       "        [ 5.,  5.,  5.],\n",
       "        [ 6.,  6.,  6.],\n",
       "        [ 7.,  7.,  7.],\n",
       "        [ 8.,  8.,  8.],\n",
       "        [ 9.,  9.,  9.],\n",
       "        [10., 10., 10.],\n",
       "        [11., 11., 11.],\n",
       "        [12., 12., 12.],\n",
       "        [13., 13., 13.],\n",
       "        [14., 14., 14.],\n",
       "        [15., 15., 15.],\n",
       "        [16., 16., 16.],\n",
       "        [17., 17., 17.],\n",
       "        [18., 18., 18.],\n",
       "        [19., 19., 19.],\n",
       "        [20., 20., 20.],\n",
       "        [21., 21., 21.],\n",
       "        [22., 22., 22.],\n",
       "        [23., 23., 23.],\n",
       "        [24., 24., 24.],\n",
       "        [25., 25., 25.],\n",
       "        [26., 26., 26.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.cat((position,position, position),dim = 1)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71e6d63b-003f-4c16-a205-07557be7fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embedings = embedding(torch.tensor(my_index)).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46e888da-9973-49ce-941b-15be529ae131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples, dim = my_embedings.shape\n",
    "samples, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70674bec-37d2-4072-8308-ca7b0f816c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30330598,  0.3182314 , -0.01590507],\n",
       "       [ 0.05397258, -1.157111  , -1.0001758 ],\n",
       "       [-0.5424799 , -2.1520617 , -0.13742737],\n",
       "       [-1.0519241 ,  0.20030454, -1.5783936 ],\n",
       "       [ 0.54749435, -0.8617089 , -1.8179109 ],\n",
       "       [-0.30330598,  0.3182314 , -0.01590507],\n",
       "       [ 0.05397258, -1.157111  , -1.0001758 ],\n",
       "       [-0.5424799 , -2.1520617 , -0.13742737],\n",
       "       [ 0.54749435, -0.8617089 , -1.8179109 ],\n",
       "       [-1.0519241 ,  0.20030454, -1.5783936 ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfce08b3-0db8-4236-beea-7594c0ca245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.0330598e-01,  3.1823140e-01, -1.5905073e-02],\n",
       "       [ 1.0539726e+00, -1.5711105e-01, -1.7583370e-04],\n",
       "       [ 1.4575201e+00, -1.5206170e-01,  1.8625727e+00],\n",
       "       [ 1.9480759e+00,  3.2003045e+00,  1.4216064e+00],\n",
       "       [ 4.5474944e+00,  3.1382911e+00,  2.1820891e+00],\n",
       "       [ 4.6966939e+00,  5.3182316e+00,  4.9840951e+00],\n",
       "       [ 6.0539727e+00,  4.8428888e+00,  4.9998240e+00],\n",
       "       [ 6.4575200e+00,  4.8479385e+00,  6.8625727e+00],\n",
       "       [ 8.5474939e+00,  7.1382914e+00,  6.1820889e+00],\n",
       "       [ 7.9480758e+00,  9.2003050e+00,  7.4216065e+00]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embding = my_embedings + pe[0:samples, :].numpy()\n",
    "pos_embding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "942467f0-9063-4725-904b-1aa4dd5cf088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9480759, 3.2003045, 1.4216064], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embding[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0775136-e2d5-4edc-ab67-2164e8d1b3b1",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffc2c84e-5c2b-453f-8206-46b083bde0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embdings = embedding(torch.tensor(my_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "539cb393-aa8b-462a-904b-a4b116d4abc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3033,  0.3182, -0.0159],\n",
       "        [ 0.0540, -1.1571, -1.0002],\n",
       "        [-0.5425, -2.1521, -0.1374],\n",
       "        [-1.0519,  0.2003, -1.5784],\n",
       "        [ 0.5475, -0.8617, -1.8179],\n",
       "        [-0.3033,  0.3182, -0.0159],\n",
       "        [ 0.0540, -1.1571, -1.0002],\n",
       "        [-0.5425, -2.1521, -0.1374],\n",
       "        [ 0.5475, -0.8617, -1.8179],\n",
       "        [-1.0519,  0.2003, -1.5784]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6917c84-a9e2-4319-bab8-fa2163e861b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 13, 15, 11, 14, 12, 13, 15, 14, 11])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(my_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2222ac73-61bb-475d-8197-dbef48881c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(27, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1aed803-6b5c-4f06-bd43-188a82901b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_embdings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad703bf3-ea57-43b8-9c42-c8bc0c6e4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=3,\n",
    "    nhead = 1,\n",
    "    dim_feedforward=1,\n",
    "    dropout = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8acf2834-5f7a-4f75-973f-026071521d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3975,  1.3741, -0.9766],\n",
       "        [ 1.3787, -0.4165, -0.9622],\n",
       "        [ 0.9005, -1.3946,  0.4942],\n",
       "        [ 0.1794,  1.1251, -1.3046],\n",
       "        [ 1.2589, -0.0714, -1.1875],\n",
       "        [-0.3975,  1.3741, -0.9766],\n",
       "        [ 1.3787, -0.4165, -0.9622],\n",
       "        [ 0.9005, -1.3946,  0.4942],\n",
       "        [ 1.2589, -0.0714, -1.1875],\n",
       "        [ 0.1794,  1.1251, -1.3046]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = encoder_layer(my_embdings)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb05962f-96d2-4a3b-a6eb-60af55f4b2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9868e-08, -3.9736e-08,  4.9671e-08,  3.9736e-08,  0.0000e+00,\n",
       "         1.9868e-08, -3.9736e-08,  4.9671e-08,  0.0000e+00,  0.0000e+00],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.mean(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da85e605-9246-477e-8110-41aca8451844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn.in_proj_weight torch.Size([9, 3])\n",
      "self_attn.in_proj_bias torch.Size([9])\n",
      "self_attn.out_proj.weight torch.Size([3, 3])\n",
      "self_attn.out_proj.bias torch.Size([3])\n",
      "linear1.weight torch.Size([1, 3])\n",
      "linear1.bias torch.Size([1])\n",
      "linear2.weight torch.Size([3, 1])\n",
      "linear2.bias torch.Size([3])\n",
      "norm1.weight torch.Size([3])\n",
      "norm1.bias torch.Size([3])\n",
      "norm2.weight torch.Size([3])\n",
      "norm2.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "params_dict = encoder_layer.state_dict()\n",
    "\n",
    "# Print the parameter names and shapes\n",
    "for name, param in params_dict.items():\n",
    "    print(name,param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1db2c59-7785-414e-bb97-877cae417648",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 3\n",
    "q_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][0:embed_dim].t()\n",
    "k_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][embed_dim:2*embed_dim].t()\n",
    "v_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][2*embed_dim:3*embed_dim].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4398715-db18-408d-bac5-7bc4963b22bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3882,  0.0439,  0.4021],\n",
       "         [ 0.1002, -0.1283, -0.1279],\n",
       "         [ 0.2260, -0.6599, -0.4037]]),\n",
       " tensor([[-0.6860, -0.5836,  0.0396],\n",
       "         [-0.2887,  0.4315,  0.5349],\n",
       "         [-0.2767,  0.1194, -0.6030]]),\n",
       " tensor([[-0.4280, -0.0283, -0.4744],\n",
       "         [-0.3591, -0.6797, -0.0774],\n",
       "         [-0.3855,  0.2779,  0.6169]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj_weight,k_proj_weight,v_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc5a0cc5-e328-4c6c-bdeb-fe272642c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = my_embdings @ q_proj_weight\n",
    "K = my_embdings @ k_proj_weight\n",
    "V = my_embdings @ v_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69306873-8c96-4e8a-b2ef-3d015f7de56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1460, -0.0436, -0.1562],\n",
       "        [-0.3630,  0.8108,  0.5736],\n",
       "        [-0.0362,  0.3429,  0.1127],\n",
       "        [ 0.0717,  0.9697,  0.1887],\n",
       "        [-0.7097,  1.3343,  1.0644],\n",
       "        [ 0.1460, -0.0436, -0.1562],\n",
       "        [-0.3630,  0.8108,  0.5736],\n",
       "        [-0.0362,  0.3429,  0.1127],\n",
       "        [-0.7097,  1.3343,  1.0644],\n",
       "        [ 0.0717,  0.9697,  0.1887]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c415a360-2537-411d-af34-f86efad07078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0128,  0.0660,  0.2011, -0.0119, -0.0047, -0.0128,  0.0660,  0.2011,\n",
       "         -0.0047, -0.0119],\n",
       "        [ 0.1765, -0.4291, -0.8712,  0.3459, -0.2865,  0.1765, -0.4291, -0.8712,\n",
       "         -0.2865,  0.3459],\n",
       "        [ 0.0702, -0.1416, -0.2168,  0.1445, -0.1449,  0.0702, -0.1416, -0.2168,\n",
       "         -0.1449,  0.1445],\n",
       "        [ 0.1982, -0.3418, -0.4278,  0.4430, -0.4214,  0.1982, -0.3418, -0.4278,\n",
       "         -0.4214,  0.4430],\n",
       "        [ 0.2944, -0.7444, -1.5764,  0.5685, -0.4502,  0.2944, -0.7444, -1.5764,\n",
       "         -0.4502,  0.5685],\n",
       "        [-0.0128,  0.0660,  0.2011, -0.0119, -0.0047, -0.0128,  0.0660,  0.2011,\n",
       "         -0.0047, -0.0119],\n",
       "        [ 0.1765, -0.4291, -0.8712,  0.3459, -0.2865,  0.1765, -0.4291, -0.8712,\n",
       "         -0.2865,  0.3459],\n",
       "        [ 0.0702, -0.1416, -0.2168,  0.1445, -0.1449,  0.0702, -0.1416, -0.2168,\n",
       "         -0.1449,  0.1445],\n",
       "        [ 0.2944, -0.7444, -1.5764,  0.5685, -0.4502,  0.2944, -0.7444, -1.5764,\n",
       "         -0.4502,  0.5685],\n",
       "        [ 0.1982, -0.3418, -0.4278,  0.4430, -0.4214,  0.1982, -0.3418, -0.4278,\n",
       "         -0.4214,  0.4430]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = Q@K.T/np.sqrt(embed_dim)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9cc94ffa-5708-4ae2-b8f8-b05d1cece6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7399,  0.3077, -0.3537],\n",
       "        [ 0.6669, -0.0094, -0.3993],\n",
       "        [ 0.7053,  0.1675, -0.3762],\n",
       "        [ 0.6913,  0.0489, -0.3471],\n",
       "        [ 0.6447, -0.1506, -0.3951],\n",
       "        [ 0.7399,  0.3077, -0.3537],\n",
       "        [ 0.6669, -0.0094, -0.3993],\n",
       "        [ 0.7053,  0.1675, -0.3762],\n",
       "        [ 0.6447, -0.1506, -0.3951],\n",
       "        [ 0.6913,  0.0489, -0.3471]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = nn.Softmax(dim = 1)(scores) @ V\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40e406cf-cf42-488b-a57d-ff43bde3c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b157294-ac01-4cbf-93e1-6933225ba958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.self_attn.in_proj_weight torch.Size([9, 3])\n",
      "layers.0.self_attn.in_proj_bias torch.Size([9])\n",
      "layers.0.self_attn.out_proj.weight torch.Size([3, 3])\n",
      "layers.0.self_attn.out_proj.bias torch.Size([3])\n",
      "layers.0.linear1.weight torch.Size([1, 3])\n",
      "layers.0.linear1.bias torch.Size([1])\n",
      "layers.0.linear2.weight torch.Size([3, 1])\n",
      "layers.0.linear2.bias torch.Size([3])\n",
      "layers.0.norm1.weight torch.Size([3])\n",
      "layers.0.norm1.bias torch.Size([3])\n",
      "layers.0.norm2.weight torch.Size([3])\n",
      "layers.0.norm2.bias torch.Size([3])\n",
      "layers.1.self_attn.in_proj_weight torch.Size([9, 3])\n",
      "layers.1.self_attn.in_proj_bias torch.Size([9])\n",
      "layers.1.self_attn.out_proj.weight torch.Size([3, 3])\n",
      "layers.1.self_attn.out_proj.bias torch.Size([3])\n",
      "layers.1.linear1.weight torch.Size([1, 3])\n",
      "layers.1.linear1.bias torch.Size([1])\n",
      "layers.1.linear2.weight torch.Size([3, 1])\n",
      "layers.1.linear2.bias torch.Size([3])\n",
      "layers.1.norm1.weight torch.Size([3])\n",
      "layers.1.norm1.bias torch.Size([3])\n",
      "layers.1.norm2.weight torch.Size([3])\n",
      "layers.1.norm2.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "params_dict = transformer_encoder.state_dict()\n",
    "for name, param in params_dict.items():\n",
    "    print(name,param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7031ae-a71e-45bd-bd36-d887cb205d8f",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "afcb3c55-1524-4063-ab19-611e8bc3dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Bank dataset\n",
    "train_iter = AG_NEWS(split = 'train')\n",
    "\n",
    "# AG_NEWS is an iterable dataset, that should be used with an iterator\n",
    "y,text = next(iter(train_iter))\n",
    "\n",
    "y,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d40c4a45-e227-4954-b024-ccc58d8eecf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Business'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "ag_news_label[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f0ecd48-f13c-4ea7-9636-bf0b6dc4bb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(set([label for label,text in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53e398-1e76-41d3-8f5c-d40d914ed119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "vocab = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1658948-bd96-4e9b-9444-8cd64cbb01b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db865802-f90f-411c-8064-86fc502bf994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522e6ea-1c6e-41b4-b739-8480b474fc55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
