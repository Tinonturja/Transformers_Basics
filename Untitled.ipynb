{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d91c21-c2a7-4df5-b19e-77ea6537aacb",
   "metadata": {},
   "source": [
    "## BERT's BASICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56029f77-92b9-4867-9e49-ab9da569ba04",
   "metadata": {},
   "source": [
    "BERT's architecture allows for fine tuning specific tasks like:\n",
    "* Text summarization\n",
    "* Question Answering\n",
    "* Sentiment Analysis\n",
    "\n",
    "Uses only `encoder_only` architecture to process entire sequences of text simultaneously\n",
    "`MLM` involves randomly masking some of the input tokens and training BERT to predict the original masked tones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8744ce8-0ab4-45ac-a153-504c3132caeb",
   "metadata": {},
   "source": [
    "For prediction:\n",
    "\n",
    "    * Encoder outputs a set of contextual embeddings\n",
    "    * Contextual embeddings are passed through another layer and converted into a set of logits.\n",
    "    * Masked word is identified by selecting the word corresponding to the index with the highest logit value. \n",
    "\n",
    "Encoder models have access to the entire sequence.\n",
    "\n",
    "The training method is `bidirectional`\n",
    "\n",
    "    * It enables the model to understand the context from both sides of any given word in a sentence.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d24efd-888f-4b51-8499-42d0e8a2e4f7",
   "metadata": {},
   "source": [
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb2d094-c4f8-4f6d-a1e2-a7ef3129368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "# New\n",
    "from torch.nn import Transformer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf768b5-95d6-4272-944c-cc3c915c453d",
   "metadata": {},
   "source": [
    "### Pretraining objectives\n",
    "\n",
    "Pretraining objectives are crucial components of the pretraining process for transformers. These objectives define the tasks that the model is trained on during the pretraining phase, allowing it to learn meaningful contextual representations of language. Two commonly used pretraining objectives are masked language modeling (MLM) and next sentence prediction (NSP).\n",
    "\n",
    "1. Masked Language Modeling (MLM):\n",
    "   Masked language modeling involves randomly masking some words in a sentence and training the model to predict the masked words based on the context provided by the surrounding words(i.e., words that appear either before or after the masked word). The objective is to enable the model to learn contextual understanding and fill in missing information.\n",
    "\n",
    "   Here's how MLM works:\n",
    "   - Given an input sentence, a certain percentage of the words are randomly chosen and replaced with a special [MASK] token.\n",
    "   - The model's task is to predict the original words that were masked, given the context of the surrounding words.\n",
    "   - During training, the model learns to understand the relationship between the masked words and the rest of the sentence, effectively capturing the contextual information.\n",
    "\n",
    "2. Next Sentence Prediction (NSP):\n",
    "   Next sentence prediction involves training the model to predict whether two sentences are consecutive in the original text or randomly chosen from the corpus. This objective helps the model learn sentence-level relationships and understand the coherence between sentences.\n",
    "\n",
    "   Here's how NSP works:\n",
    "   - Given a pair of sentences, the model is trained to predict whether the second sentence follows the first sentence in the original text or if it is randomly selected from the corpus.\n",
    "   - The model learns to capture the relationships between sentences and understand the flow of information in the text.\n",
    "\n",
    "   NSP is particularly useful for tasks that involve understanding the relationship between multiple sentences, such as question answering or document classification. By training the model to predict the coherence of sentence pairs, it learns to capture the semantic connections between them.\n",
    "\n",
    "It's important to note that different pretrained models may use variations or combinations of these objectives, depending on the specific architecture and training setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685fcfa-b11a-41f3-aa1a-d41c497a02f0",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a005c6-33bc-43ba-a8eb-10a45eda4f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-31 11:50:54--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
      "connected. to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 88958506 (85M) [application/zip]\n",
      "Saving to: ‘BERT_dataset.zip’\n",
      "\n",
      "BERT_dataset.zip    100%[===================>]  84.84M  2.23MB/s    in 41s     \n",
      "\n",
      "2025-07-31 11:51:42 (2.06 MB/s) - ‘BERT_dataset.zip’ saved [88958506/88958506]\n",
      "\n",
      "Archive:  BERT_dataset.zip\n",
      "   creating: /Users/tinonturjamajumder/Generative AI Language Modelling with Transformers_3/bert_dataset\n",
      "  inflating: bert_dataset/.DS_Store  \n",
      "  inflating: bert_dataset/bert_train_data.csv  \n",
      "  inflating: bert_dataset/bert_test_data_sampled.csv  \n",
      "  inflating: bert_dataset/bert_test_data.csv  \n",
      "  inflating: bert_dataset/bert_train_data_sampled.csv  \n"
     ]
    }
   ],
   "source": [
    "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
    "!unzip BERT_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d22a77fb-1328-4e52-9fce-4c636e8e618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCSVDataset(Dataset):\n",
    "\n",
    "    def __init__(self,filename):\n",
    "        self.data = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        try:\n",
    "            bert_input = torch.tensor(json.loads(row[\"BERT Input\"]),dtype = torch.long)\n",
    "            bert_label = torch.tensor(json.loads(row['BERT Label']),dtype = torch.long)\n",
    "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')],dtype = torch.long)\n",
    "            is_next = torch.tensor(row['Is Next'],dtype = torch.long)\n",
    "            original_text = row['Original Text']\n",
    "\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            \n",
    "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
    "            print(f\"BERT Input: {row['BERT Input']}'\")\n",
    "            print(f\"BERT Label: {row[\"BERT Label\"]}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            original_text,\n",
    "            add_special_tokens = True,\n",
    "            max_length = 512,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "\n",
    "        return(bert_input,bert_label,segment_label,is_next,input_ids,attention_mask,original_text)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466a92d9-19a7-40e0-8097-3428635a9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "def collate_batch(batch):\n",
    "    bert_inputs_batch,bert_label_batch,bert_segment_batch,is_next_batch,input_ids_batch,attention_mask_batch,original_text_batch =  [], [], [], [],[],[],[]\n",
    "\n",
    "\n",
    "    for bert_inputs,bert_label,bert_segment,is_next,input_ids,attention_mask,original_text in batch:\n",
    "\n",
    "        bert_inputs_batch.append(torch.tensor(bert_inputs,dtype = torch.long))\n",
    "        bert_label_batch.append(torch.tensor(bert_label,dtype = torch.long))\n",
    "        bert_segment_batch.append(torch.tensor(bert_segment),dtype= torch.long)\n",
    "        is_next_batch.append(is_next)\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        original_text_batch.append(original_text)\n",
    "        \n",
    "\n",
    "    # pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch,padding_value = PAD_IDX,batch_first = False)\n",
    "    bert_labels_final = pad_sequence(bert_label_batch,padding_value = PAD_IDX, batch_first = False)\n",
    "    segments_label_final = pad_sequence(bert_segment_batch,padding_value = PAD_IDX, batch_first = False)\n",
    "    is_nexts_final = torch.tensor(is_next_batch,dtype = torch.long)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segments_label_final,is_nexts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082499b1-b015-476d-a300-6ebb1d67a569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a628f51-9702-4bce-92f9-2ffdf132a7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4f5d3-51e8-45fe-9f4e-426fe8125aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3430830-7d55-490e-8800-3a9b91e15336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
