{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84668e49-0501-476a-b5c0-8658dfb17ecf",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493033c8-0f7b-42c6-86f3-73498db094fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Levenshtein\n",
      "  Downloading levenshtein-0.27.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\n",
      "  Downloading rapidfuzz-3.13.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Downloading levenshtein-0.27.1-cp312-cp312-macosx_11_0_arm64.whl (156 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp312-cp312-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.27.1 rapidfuzz-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d7f0a-6fd6-4c9a-90b9-b28942ee67c1",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de860c-194c-4492-9088-3baa269e00ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2a2e403-a7b4-413f-81d3-1bf8ea2a85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "# new\n",
    "from Levenshtein import distance # use to detect the distance between the predicted and ac\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495ffaa-c86e-4d24-b22c-423b3af8325d",
   "metadata": {},
   "source": [
    "## Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b461ea-f41c-49fa-8341-4fb367e6f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac4542-fae9-4e0d-a97f-4b42c3d52fa0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1b89d4-b883-4795-8817-6d849eb477c1",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "`learning_rate:` This is the step size at each iteration while moving towarda minimum of the loss function. \n",
    "\n",
    "`batch_size:` the number of samples that will be propagated through the network in one forward/backward pass. here it's 64\n",
    "\n",
    "`max_iters:` the total number of training iterations we plan to run. Set to 5000 to allow the model ample opportunity to learn from the data\n",
    "\n",
    "`eval_interval and eval_iters:` Parameters defining how frequently we evaluate the model's performance on a set number of batches to approximate loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be180f4d-7973-4a33-81c5-9b007836ce27",
   "metadata": {},
   "source": [
    "## Architecture Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a76bf-f784-4830-a2a6-5e2c9a3d0035",
   "metadata": {},
   "source": [
    "max_vocab_size: maximum number of tokens in our vocabulary. It's set to 256, meaning that we will only consider the most frequent 256 tokens.\n",
    "\n",
    "vocab_size: the actual number of tokens in the vocabulary, which may be less than the max due to the variable length of tokens in subword tokenization like BPE(Byte Pair Encodding)\n",
    "\n",
    "block_size: the length of input token the model is designed to handle. Here it's 16\n",
    "\n",
    "n_embed:embedding size\n",
    "\n",
    "num_heads: number of head in multi-head attention\n",
    "\n",
    "n_layer: 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa75c7-610e-4a6c-8355-23fbc9b4a00a",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6399c-9416-4231-bd8f-88b8669b7943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
