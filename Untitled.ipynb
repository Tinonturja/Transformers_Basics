{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d91c21-c2a7-4df5-b19e-77ea6537aacb",
   "metadata": {},
   "source": [
    "## BERT's BASICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56029f77-92b9-4867-9e49-ab9da569ba04",
   "metadata": {},
   "source": [
    "BERT's architecture allows for fine tuning specific tasks like:\n",
    "* Text summarization\n",
    "* Question Answering\n",
    "* Sentiment Analysis\n",
    "\n",
    "Uses only `encoder_only` architecture to process entire sequences of text simultaneously\n",
    "`MLM` involves randomly masking some of the input tokens and training BERT to predict the original masked tones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8744ce8-0ab4-45ac-a153-504c3132caeb",
   "metadata": {},
   "source": [
    "For prediction:\n",
    "\n",
    "    * Encoder outputs a set of contextual embeddings\n",
    "    * Contextual embeddings are passed through another layer and converted into a set of logits.\n",
    "    * Masked word is identified by selecting the word corresponding to the index with the highest logit value. \n",
    "\n",
    "Encoder models have access to the entire sequence.\n",
    "\n",
    "The training method is `bidirectional`\n",
    "\n",
    "    * It enables the model to understand the context from both sides of any given word in a sentence.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d24efd-888f-4b51-8499-42d0e8a2e4f7",
   "metadata": {},
   "source": [
    "### Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb2d094-c4f8-4f6d-a1e2-a7ef3129368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "# New\n",
    "from torch.nn import Transformer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf768b5-95d6-4272-944c-cc3c915c453d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pretraining objectives\n",
    "\n",
    "Pretraining objectives are crucial components of the pretraining process for transformers. These objectives define the tasks that the model is trained on during the pretraining phase, allowing it to learn meaningful contextual representations of language. Two commonly used pretraining objectives are masked language modeling (MLM) and next sentence prediction (NSP).\n",
    "\n",
    "1. Masked Language Modeling (MLM):\n",
    "   Masked language modeling involves randomly masking some words in a sentence and training the model to predict the masked words based on the context provided by the surrounding words(i.e., words that appear either before or after the masked word). The objective is to enable the model to learn contextual understanding and fill in missing information.\n",
    "\n",
    "   Here's how MLM works:\n",
    "   - Given an input sentence, a certain percentage of the words are randomly chosen and replaced with a special [MASK] token.\n",
    "   - The model's task is to predict the original words that were masked, given the context of the surrounding words.\n",
    "   - During training, the model learns to understand the relationship between the masked words and the rest of the sentence, effectively capturing the contextual information.\n",
    "\n",
    "2. Next Sentence Prediction (NSP):\n",
    "   Next sentence prediction involves training the model to predict whether two sentences are consecutive in the original text or randomly chosen from the corpus. This objective helps the model learn sentence-level relationships and understand the coherence between sentences.\n",
    "\n",
    "   Here's how NSP works:\n",
    "   - Given a pair of sentences, the model is trained to predict whether the second sentence follows the first sentence in the original text or if it is randomly selected from the corpus.\n",
    "   - The model learns to capture the relationships between sentences and understand the flow of information in the text.\n",
    "\n",
    "   NSP is particularly useful for tasks that involve understanding the relationship between multiple sentences, such as question answering or document classification. By training the model to predict the coherence of sentence pairs, it learns to capture the semantic connections between them.\n",
    "\n",
    "It's important to note that different pretrained models may use variations or combinations of these objectives, depending on the specific architecture and training setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685fcfa-b11a-41f3-aa1a-d41c497a02f0",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a005c6-33bc-43ba-a8eb-10a45eda4f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-04 16:28:52--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
      "connected. to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 88958506 (85M) [application/zip]\n",
      "Saving to: ‘BERT_dataset.zip’\n",
      "\n",
      "BERT_dataset.zip    100%[===================>]  84.84M  1.81MB/s    in 46s     \n",
      "\n",
      "2025-08-04 16:29:40 (1.86 MB/s) - ‘BERT_dataset.zip’ saved [88958506/88958506]\n",
      "\n",
      "Archive:  BERT_dataset.zip\n",
      "  inflating: bert_dataset/.DS_Store  \n",
      "  inflating: bert_dataset/bert_train_data.csv  \n",
      "  inflating: bert_dataset/bert_test_data_sampled.csv  \n",
      "  inflating: bert_dataset/bert_test_data.csv  \n",
      "  inflating: bert_dataset/bert_train_data_sampled.csv  \n"
     ]
    }
   ],
   "source": [
    "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
    "!unzip -o BERT_dataset.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22a77fb-1328-4e52-9fce-4c636e8e618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCSVDataset(Dataset):\n",
    "\n",
    "    def __init__(self,filename):\n",
    "        self.data = pd.read_csv(filename)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        try:\n",
    "            bert_input = torch.tensor(json.loads(row[\"BERT Input\"]),dtype = torch.long)\n",
    "            bert_label = torch.tensor(json.loads(row['BERT Label']),dtype = torch.long)\n",
    "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')],dtype = torch.long)\n",
    "            is_next = torch.tensor(row['Is Next'],dtype = torch.long)\n",
    "            original_text = row['Original Text']\n",
    "\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            \n",
    "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
    "            print(f\"BERT Input: {row['BERT Input']}'\")\n",
    "            print(f\"BERT Label: {row[\"BERT Label\"]}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            original_text,\n",
    "            add_special_tokens = True,\n",
    "            max_length = 512,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_input['input_ids'].squeeze()\n",
    "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
    "\n",
    "        return(bert_input,bert_label,segment_label,is_next,input_ids,attention_mask,original_text)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466a92d9-19a7-40e0-8097-3428635a9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "def collate_batch(batch):\n",
    "    bert_inputs_batch,bert_label_batch,bert_segment_batch,is_next_batch,input_ids_batch,attention_mask_batch,original_text_batch =  [], [], [], [],[],[],[]\n",
    "\n",
    "\n",
    "    for bert_inputs,bert_label,bert_segment,is_next,input_ids,attention_mask,original_text in batch:\n",
    "\n",
    "        bert_inputs_batch.append(torch.tensor(bert_inputs,dtype = torch.long))\n",
    "        bert_label_batch.append(torch.tensor(bert_label,dtype = torch.long))\n",
    "        bert_segment_batch.append(torch.tensor(bert_segment,dtype= torch.long))\n",
    "        is_next_batch.append(is_next)\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        original_text_batch.append(original_text)\n",
    "        \n",
    "\n",
    "    # pad the sequences in the batch\n",
    "    bert_inputs_final = pad_sequence(bert_inputs_batch,padding_value = PAD_IDX,batch_first = False)\n",
    "    bert_labels_final = pad_sequence(bert_label_batch,padding_value = PAD_IDX, batch_first = False)\n",
    "    segments_label_final = pad_sequence(bert_segment_batch,padding_value = PAD_IDX, batch_first = False)\n",
    "    is_nexts_final = torch.tensor(is_next_batch,dtype = torch.long)\n",
    "\n",
    "    return bert_inputs_final, bert_labels_final, segments_label_final,is_nexts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082499b1-b015-476d-a300-6ebb1d67a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset_path = 'bert_dataset/bert_train_data.csv'\n",
    "test_dataset_path = 'bert_dataset/bert_test_data.csv'\n",
    "\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             batch_size = BATCH_SIZE,\n",
    "                             shuffle= True,\n",
    "                             collate_fn = collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                             shuffle= True,\n",
    "                             collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0ac165-8e6b-4b8f-ae0c-05a08828cb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    1,    16,    94, 12615,     5,  1026,    22,     5,   201,    18,\n",
       "           110,   405,   611,    15,     5,    26,    13,     3,    22,  1580,\n",
       "          5045,   254,     3,     3,    11,  1501,    88,     8,   108,   283,\n",
       "             3,     3,   611,    15,  8209,  2184,    15,   838,     6,     2,\n",
       "             3,     3,     3,   492,    11,  1135,    50, 11862,    11,     3,\n",
       "            59,   444,    10,   672,     3,    57,     3,   866, 29740,   212,\n",
       "             3,   784,  1004,  1307,   150,     3,     5,  2678,     3,     8,\n",
       "          1513,     3,    15,     5,     3,  1595,     6,     2,     0,     0]),\n",
       " tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,    26,     0,   611,     0,     0,\n",
       "             0,     0,    82,    52,     0,     0,    88,     0,     0,   283,\n",
       "            11,    37,   611,     0,     0,  2184,    15,     0,     0,     0,\n",
       "            15,   829,    64,     0,     0,     0,     0,     0,     0, 13903,\n",
       "             0,     0,    10,     0,    31,     0, 64535,     0,     0,     0,\n",
       "            53,     0,     0,     0,     0,    21,     0,     0,   340,     0,\n",
       "             0, 60330,     0,     0,  2362,     0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 0, 0]),\n",
       " tensor(0),\n",
       " tensor([  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
       "          2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
       "          2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
       "          2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
       "          1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
       "          2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
       "          6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
       "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
       "          5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
       "         14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
       "          1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
       "          2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
       "         25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107,  2004,  1996,\n",
       "          5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,  2163,  1012,\n",
       "          1999,  2090,  4851,  8801,  1998,  6623,  7939,  4697,  3619,  1997,\n",
       "          8947,  2055,  2037, 10740,  2006,  4331,  1010,  2016,  2038,  3348,\n",
       "          2007,  2014,  3689,  3836,  1010, 19846,  1010,  1998,  2496,  2273,\n",
       "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2054,\n",
       "          8563,  2033,  2055,  1045,  2572,  8025,  1011,  3756,  2003,  2008,\n",
       "          2871,  2086,  3283,  1010,  2023,  2001,  2641, 26932,  1012,  2428,\n",
       "          1010,  1996,  3348,  1998, 16371, 25469,  5019,  2024,  2261,  1998,\n",
       "          2521,  2090,  1010,  2130,  2059,  2009,  1005,  1055,  2025,  2915,\n",
       "          2066,  2070, 10036,  2135,  2081, 22555,  2080,  1012,  2096,  2026,\n",
       "          2406,  3549,  2568,  2424,  2009, 16880,  1010,  1999,  4507,  3348,\n",
       "          1998, 16371, 25469,  2024,  1037,  2350, 18785,  1999,  4467,  5988,\n",
       "          1012,  2130, 13749,  7849, 24544,  1010, 15835,  2037,  3437,  2000,\n",
       "          2204,  2214,  2879,  2198,  4811,  1010,  2018,  3348,  5019,  1999,\n",
       "          2010,  3152,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "          1028,  1045,  2079,  4012,  3549,  2094,  1996, 16587,  2005,  1996,\n",
       "          2755,  2008,  2151,  3348,  3491,  1999,  1996,  2143,  2003,  3491,\n",
       "          2005,  6018,  5682,  2738,  2084,  2074,  2000,  5213,  2111,  1998,\n",
       "          2191,  2769,  2000,  2022,  3491,  1999, 26932, 12370,  1999,  2637,\n",
       "          1012,  1045,  2572,  8025,  1011,  3756,  2003,  1037,  2204,  2143,\n",
       "          2005,  3087,  5782,  2000,  2817,  1996,  6240,  1998, 14629,  1006,\n",
       "          2053, 26136,  3832,  1007,  1997,  4467,  5988,  1012,  2021,  2428,\n",
       "          1010,  2023,  2143,  2987,  1005,  1056,  2031,  2172,  1997,  1037,\n",
       "          5436,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample dataset\n",
    "train_dataset = iter(train_dataset)\n",
    "one_sample = next(train_dataset)\n",
    "two_sample = next(train_dataset)\n",
    "one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e46df5ab-ef36-4577-8b4d-7ca2b0ea48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer_dataset = BERTCSVDataset(train_dataset_path)\n",
    "tokenizer = tokenizer_dataset.tokenizer\n",
    "\n",
    "print(f\"vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7095cf-27c2-4483-8db1-8d2abefc2819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert_input_shape:torch.Size([80])\n",
      "Bert_label_shape:torch.Size([80])\n",
      "Segment_label_shape:torch.Size([80])\n",
      "is_next_shape:torch.Size([])\n",
      "input_ids_shape:torch.Size([512])\n",
      "Attention_mask_shape:torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the sample of a dataset\n",
    "print(f\"Bert_input_shape:{one_sample[0].shape}\")\n",
    "print(f\"Bert_label_shape:{one_sample[1].shape}\")\n",
    "print(f\"Segment_label_shape:{one_sample[2].shape}\")\n",
    "print(f\"is_next_shape:{one_sample[3].shape}\")\n",
    "print(f\"input_ids_shape:{one_sample[4].shape}\")\n",
    "print(f\"Attention_mask_shape:{one_sample[5].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ddd7b2-aa30-47ed-aec2-5443c03b271e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a77a0f3a-873b-48db-8170-1537302faa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,     1],\n",
       "         [ 4737,    49],\n",
       "         [  133,    34],\n",
       "         [    9,    47],\n",
       "         [   65,   119],\n",
       "         [   60, 24373],\n",
       "         [  300,    18],\n",
       "         [   15,    64],\n",
       "         [   50,    20],\n",
       "         [  222,    32],\n",
       "         [    3,     3],\n",
       "         [ 6862,     3],\n",
       "         [ 9762,    25],\n",
       "         [  753,    15],\n",
       "         [   81,    17],\n",
       "         [  379,     3],\n",
       "         [    3,     3],\n",
       "         [   12, 12060],\n",
       "         [   19, 22723],\n",
       "         [   60,  3204],\n",
       "         [    3,    13],\n",
       "         [   83,    32],\n",
       "         [    6,    65],\n",
       "         [    2,     3],\n",
       "         [   16,     3],\n",
       "         [ 1074,     2],\n",
       "         [   14,     0],\n",
       "         [    9,     0],\n",
       "         [  973,     0],\n",
       "         [   56, 38159],\n",
       "         [   10,    11],\n",
       "         [  314,     3],\n",
       "         [   21,     9],\n",
       "         [    9,   327],\n",
       "         [  730,     3],\n",
       "         [    3,   435],\n",
       "         [    6,     7],\n",
       "         [    2,     8],\n",
       "         [    0,     3],\n",
       "         [    0,    64],\n",
       "         [    0,    78],\n",
       "         [    0,    11],\n",
       "         [    0,  1596],\n",
       "         [    0,    23],\n",
       "         [    0,     5],\n",
       "         [    0, 65947],\n",
       "         [    0,     3],\n",
       "         [    0,     5],\n",
       "         [    0, 59140],\n",
       "         [    0,     7],\n",
       "         [    0,     8],\n",
       "         [    0,     5],\n",
       "         [    0,  1266],\n",
       "         [    0,     3],\n",
       "         [    0,    11],\n",
       "         [    0,  1985],\n",
       "         [    0,     6],\n",
       "         [    0,     2]]),\n",
       " tensor([[    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [   65,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    8, 85950],\n",
       "         [    0,     7],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,  1584],\n",
       "         [   33,    10],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [82234,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,  1053],\n",
       "         [    0,     6],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0, 72574],\n",
       "         [    0,     9],\n",
       "         [    9,     0],\n",
       "         [    0,     8],\n",
       "         [  231,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0, 41118],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0, 74930],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0, 35702],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0],\n",
       "         [    0,     0]]),\n",
       " tensor([[1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [2, 1],\n",
       "         [2, 1],\n",
       "         [2, 0],\n",
       "         [2, 0],\n",
       "         [2, 0],\n",
       "         [2, 2],\n",
       "         [2, 2],\n",
       "         [2, 2],\n",
       "         [2, 2],\n",
       "         [2, 2],\n",
       "         [2, 2],\n",
       "         [2, 2],\n",
       "         [2, 2],\n",
       "         [2, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2],\n",
       "         [0, 2]]),\n",
       " tensor([1, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sample_dataloader = next(iter(train_dataloader))\n",
    "one_sample_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a628f51-9702-4bce-92f9-2ffdf132a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self,vocab_size,emb_dim, dropout =0.1,train = True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def forward(self,tokens):\n",
    "        return self.embedding(tokens.long())*math.sqrt(self.emb_dim)\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,emb_dim:int, dropout:float, maxlen: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        den = torch.exp(-torch.arange(0,emb_dim,2)*math.log(10000)/emb_dim)\n",
    "\n",
    "        pos = torch.arange(0,maxlen).reshape(maxlen,1)\n",
    "        pos_embedding = torch.zeros(size = (maxlen,emb_dim))\n",
    "\n",
    "        pos_embedding[:,0::2]= torch.sin(pos*den)\n",
    "        pos_embedding[:,1::2]= torch.cos(pos*den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2) # add batch size\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding',pos_embedding)\n",
    "\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :]) # take all the rows upto the embedding size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76d4f5d3-51e8-45fe-9f4e-426fe8125aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_dim, dropout = 0.1,train = True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, emb_dim)\n",
    "        self.pos_encoding = PositionalEncoding(emb_dim, dropout)\n",
    "        self.segment_embedding = nn.Embedding(3,emb_dim)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "\n",
    "    def forward(self,bert_inputs, segment_labels = False):\n",
    "        my_embeddings = self.token_embedding(bert_inputs)\n",
    "\n",
    "        if self.train:\n",
    "            x = self.dropout(my_embeddings + self.pos_encoding(my_embeddings)+self.segment_embedding(segment_labels))\n",
    "        else:\n",
    "            x = my_embeddings + self.pos_encoding(my_embeddings)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c7398-becb-40f3-a6d5-f1957574332c",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987095f-4caa-4308-9013-7f686d940a1e",
   "metadata": {},
   "source": [
    "1. Initialization\n",
    "2. Embedding Layer\n",
    "3. Transformer Encoder\n",
    "4. Next Sentence Prediction\n",
    "5. Masked Language Modeling\n",
    "6. Forward Pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7495d2f3-f974-4d83-8f93-a74962ae5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 147161\n",
    "batch = 2\n",
    "count = 0\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    bert_inputs, bert_labels, segment_labels,is_nexts = [b.to(device) for b in batch]\n",
    "    count +=1\n",
    "    if count ==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e39b2654-2bd9-49ab-831e-0ed157572d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_nexts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb36e5e5-bd7e-41ff-99b3-001997e91579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "740042aa-9ef4-44c7-b7c8-114b01d39eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a73de7d-bb36-4efd-930f-7c6f2cef7172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_nexts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cbfe0be-4675-4052-aacb-40d8b8fc59bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,    16,    12,   155,     3,   338,   138,   583,    15,     9,\n",
       "        17277,   338,   536,     6,     2,     0,     0,    18,   121,   308,\n",
       "            7,    16,    36,    59,   666, 19956,    53,     3,  3635,    12,\n",
       "           19,   173,     6,     2,     0,     0,     0,     0,     0,     0],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b440fd2-73fb-45dd-8a15-054ddccbcded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0], device='mps:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_labels[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7de685ff-68ce-4d21-88d2-6c27b624a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT input token embeddings: torch.Size([40, 2, 10])\n",
      "Token embeddings for the 0th token of the first sample:tensor([-2.9955,  0.2391, -0.7194,  2.1648, -0.9531, -0.9986,  0.1961, -2.3026,\n",
      "        -3.4024,  4.0960], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "Token embeddings for the 1th token of the first sample:tensor([-4.1424, -6.7488,  4.8567,  2.5696, -3.7761, -2.6144,  1.6663, -8.3851,\n",
      "         0.9357,  3.2129], device='mps:0', grad_fn=<SliceBackward0>) \n",
      "Token embeddings for the 2th token of the first sample:tensor([-0.7929, -0.6224, -7.1462,  1.3423, -4.7221, -1.0516,  3.6903,  4.1848,\n",
      "         1.5964, -4.6210], device='mps:0', grad_fn=<SliceBackward0>) \n"
     ]
    }
   ],
   "source": [
    "## Instantiate the token embedding\n",
    "token_embedding = TokenEmbedding(VOCAB_SIZE,emb_dim = EMBEDDING_DIM).to(device)\n",
    "\n",
    "\n",
    "# get the token embeddings for the sample input\n",
    "t_embeddings = token_embedding(bert_inputs.to(device)) # shpae : {seq_len,batch_size,emb_dim}\n",
    "\n",
    "# print the dimensionality of the bert input token embeddings\n",
    "print(f\"BERT input token embeddings: {t_embeddings.shape}\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Token embeddings for the {i}th token of the first sample:{t_embeddings[i,0,:]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce40c5b5-fbb3-4188-805a-b64bfdea816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of the inputs after the positional encodings: torch.Size([40, 2, 10])\n",
      "Positional embeddings for the 0th token of the first samples: tensor([-2.9955e+00,  1.2391e+00, -7.1940e-01,  3.1648e+00, -9.5311e-01,\n",
      "         1.4390e-03,  1.9608e-01, -1.3026e+00, -3.4024e+00,  5.0960e+00],\n",
      "       device='mps:0', grad_fn=<SliceBackward0>)\n",
      "Positional embeddings for the 1th token of the first samples: tensor([-3.3009, -6.2085,  5.0145,  3.5570, -3.7509, -1.6147,  1.6702, -7.3851,\n",
      "         0.9364,  4.2129], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "Positional embeddings for the 2th token of the first samples: tensor([ 0.1164, -1.0385, -6.8345,  2.2925, -4.6719, -0.0529,  3.6983,  5.1848,\n",
      "         1.5977, -3.6210], device='mps:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## apply positional encoding to the bert input\n",
    "positional_encoding = PositionalEncoding(emb_dim = EMBEDDING_DIM,dropout = 0).to(device)\n",
    "\n",
    "bert_positional_value = positional_encoding(t_embeddings)\n",
    "\n",
    "# print the dimensionality of the tembeddings after passing it through the positional embeddings\n",
    "print(f\"Dimensionality of the inputs after the positional encodings: {bert_positional_value.shape}\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Positional embeddings for the {i}th token of the first samples: {bert_positional_value[i,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c06c9e81-c7e1-43d7-ad6b-d039d5141cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of the inputs after the segment encodings: torch.Size([40, 2, 10])\n",
      "segment embeddings for the 0th token of the first samples: tensor([ 1.2616,  0.6381, -0.8654,  0.4832, -1.6175, -0.9290, -1.1129,  1.5232,\n",
      "        -0.3162,  0.3977], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "segment embeddings for the 1th token of the first samples: tensor([ 1.2616,  0.6381, -0.8654,  0.4832, -1.6175, -0.9290, -1.1129,  1.5232,\n",
      "        -0.3162,  0.3977], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "segment embeddings for the 2th token of the first samples: tensor([ 1.2616,  0.6381, -0.8654,  0.4832, -1.6175, -0.9290, -1.1129,  1.5232,\n",
      "        -0.3162,  0.3977], device='mps:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Segment encoding\n",
    "segment_embedding = nn.Embedding(3,EMBEDDING_DIM).to(device)\n",
    "segmented_bert = segment_embedding(segment_labels)\n",
    "\n",
    "# print the dimensionality of the tembeddings after passing it through the positional embeddings\n",
    "print(f\"Dimensionality of the inputs after the segment encodings: {segmented_bert.shape}\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"segment embeddings for the {i}th token of the first samples: {segmented_bert[i,0,:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cd3ad3b-4c8b-4102-bcfb-c5b33e862f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]], device='mps:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b765f6b-eb8f-429d-aac7-4ea4b33042a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after combining token, position and segment: torch.Size([40, 2, 10])\n",
      "combined embeddings for the 0th token of the first samples: tensor([-4.7294,  2.1163, -2.3042,  5.8128, -3.5237, -1.9261, -0.7208, -2.0820,\n",
      "        -7.1209,  9.5897], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "combined embeddings for the 1th token of the first samples: tensor([ -6.1817, -12.3192,   9.0058,   6.6098,  -9.1445,  -5.1580,   2.2236,\n",
      "        -14.2469,   1.5560,   7.8234], device='mps:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "combined embeddings for the 2th token of the first samples: tensor([  0.5850,  -1.0227, -14.8460,   4.1179, -11.0115,  -2.0335,   6.2756,\n",
      "         10.8928,   2.8779,  -7.8442], device='mps:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create the combined embedding vectors\n",
    "combined_vectors = (t_embeddings+bert_positional_value+segmented_bert).to(device)\n",
    "\n",
    "print(f\"Shape after combining token, position and segment: {combined_vectors.shape}\")\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"combined embeddings for the {i}th token of the first samples: {combined_vectors[i,0,:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7cba2-e475-457e-9e03-735a3465e91c",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f256884-8a1e-43d9-bb32-41e7cd9322a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,d_model=768,n_layers = 12,nheads=12,dropout = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.nheads = nheads\n",
    "\n",
    "\n",
    "        # Embedding layer that combines tokens embeddings and segment emebeddings\n",
    "        \n",
    "        self.bert_embedding = BERTEmbedding(vocab_size = vocab_size,emb_dim = d_model,dropout = dropout)\n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(d_model = d_model,nhead = nheads,dropout = dropout, batch_first = False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layers,num_layers = n_layers)\n",
    "\n",
    "        # linear layer for next sentence prediction\n",
    "        self.nextsentenceprediction = nn.Linear(d_model,2)\n",
    "\n",
    "        # linear layer for masked language modeling\n",
    "        self.maskedlanguagemodelling = nn.Linear(d_model,vocab_size)\n",
    "\n",
    "    def forward(self,bert_inputs,segment_labels):\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        bert_inputs = input tokens of bert inputs\n",
    "        segment_labels = segment ids for distinguishing different segments in the input\n",
    "        mask: Attention mask to prevent attention to padding tokens\n",
    "        \n",
    "        return: Prediction for next sentence task and masked modelling task\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        padding_mask = (bert_inputs == PAD_IDX).transpose(0,1)\n",
    "\n",
    "        # generate embeddings from inputs and segment labels\n",
    "        my_bert_embedding = self.bert_embedding(bert_inputs,segment_labels)\n",
    "\n",
    "\n",
    "        ## Pass embeddings through the transformer\n",
    "        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask = padding_mask)\n",
    "\n",
    "        ## Next sentence prediction\n",
    "        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[0,:])\n",
    "\n",
    "        # Masked Language Modeling: Predict all tokens in the sequence\n",
    "        masked_language = self.maskedlanguagemodelling(transformer_encoder_output)\n",
    "\n",
    "        return next_sentence_prediction,masked_language\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4685b-65a7-4537-9a46-b5d97b9dabc4",
   "metadata": {},
   "source": [
    "#### Create an instance of our BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a82c13d3-51af-40bd-8669-fd0ec6106a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "d_model = EMBEDDING_DIM\n",
    "n_layers = 2\n",
    "nheads = 2\n",
    "\n",
    "dropout = 0.1\n",
    "\n",
    "# create the instance of the BERT Model\n",
    "\n",
    "model = BERT(vocab_size = vocab_size, d_model = d_model,n_layers=n_layers,nheads = nheads,dropout =dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8de2d34-9ae7-4f96-b5b3-87b6e0168be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = (bert_inputs ==PAD_IDX).transpose(0,1).to(device)\n",
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6c6773e-ff3f-408c-9a70-52238bb40122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True,  True, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2858dae5-fd53-4881-be12-6253d6bd9f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ac11f79-0ff6-4184-ba66-d13a2ec5b7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     1],\n",
       "        [   16,     3],\n",
       "        [   12,   100],\n",
       "        [  155,    12],\n",
       "        [    3,     3],\n",
       "        [  338,   132],\n",
       "        [  138,   259],\n",
       "        [  583,     3],\n",
       "        [   15,  9052],\n",
       "        [    9,    48],\n",
       "        [17277,   143],\n",
       "        [  338,     3],\n",
       "        [  536,    94],\n",
       "        [    6,     6],\n",
       "        [    2,     2],\n",
       "        [    0,     0],\n",
       "        [    0,     0],\n",
       "        [   18,     0],\n",
       "        [  121,     0],\n",
       "        [  308,     0],\n",
       "        [    7,    17],\n",
       "        [   16,    13],\n",
       "        [   36,     9],\n",
       "        [   59,   867],\n",
       "        [  666,    24],\n",
       "        [19956,    11],\n",
       "        [   53,     3],\n",
       "        [    3,    31],\n",
       "        [ 3635,     3],\n",
       "        [   12,    16],\n",
       "        [   19,  1431],\n",
       "        [  173,     3],\n",
       "        [    6,   100],\n",
       "        [    2,     3],\n",
       "        [    0,     3],\n",
       "        [    0,  1743],\n",
       "        [    0,     5],\n",
       "        [    0,  2852],\n",
       "        [    0,     3],\n",
       "        [    0,     2]], device='mps:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0b99ea4-360e-4788-9138-0a4d2b120c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer =nn.TransformerEncoderLayer(d_model = EMBEDDING_DIM,nhead = 2,dropout = dropout,batch_first = False)\n",
    "\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=n_layers).to(device)\n",
    "\n",
    "transformer_encoder_output = transformer_encoder(combined_vectors,src_key_padding_mask = padding_mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9b7bd96-eec5-4f33-a8b0-88ee14ac656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 2, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c05b48d-b978-49f5-bf4e-6a908169000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_sentence_prediction = nn.Linear(d_model,2).to(device)\n",
    "next_predicted_sentence = next_sentence_prediction(transformer_encoder_output[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a29f9ab-7ed9-4159-b6b0-ba7d5583b6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_predicted_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f94bdf3-0145-48c5-a5ce-1cfa118fa7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm shape: torch.Size([40, 2, 30522])\n"
     ]
    }
   ],
   "source": [
    "masked_language = nn.Linear(d_model,vocab_size).to(device)\n",
    "\n",
    "mlm = masked_language(transformer_encoder_output)\n",
    "\n",
    "print(f\"mlm shape: {mlm.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f9ad9-433f-45dd-890d-280ba426b5be",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60f17af7-9426-4fb2-b721-41433add9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 10\n",
    "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "loss_fn_nsp = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52b44819-4db9-4137-b859-940331ce5360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader,loss_fn_mlm, loss_fn_nsp,device):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_next_sentence_loss = 0\n",
    "    total_mlm_loss = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader: # batch --> bert_inputs,bert_labels,segment_labels, is_next\n",
    "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
    "\n",
    "            # Forward Pass\n",
    "            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
    "\n",
    "            # Calculate loss for next sentence prediction\n",
    "            # Ensure is_nexts is of the correct shape for crossentropyloss\n",
    "            next_loss = loss_fn_nsp(next_sentence_prediction,is_nexts.view(-1))\n",
    "\n",
    "            # Calculate loss for predicting masked tokens\n",
    "            # Flatten both masked language predictions and bert labels to match input requirements\n",
    "            \n",
    "            mlm_loss = loss_fn_mlm(masked_language.view((-1),masked_language.size(-1)),bert_labels.view(-1))\n",
    "\n",
    "            # Sum up the two losses\n",
    "            loss = next_loss + mlm_loss\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            else:\n",
    "                total_loss +=loss.item()\n",
    "                total_next_sentence_loss += next_loss.item()\n",
    "                total_mask_loss +=mlm_loss.item()\n",
    "                total_batches +=1\n",
    "\n",
    "\n",
    "        avg_loss = total_loss/ (total_batches +1)\n",
    "        avg_next_sentence_loss = total_next_sentence_loss /(total_batches +1)\n",
    "        avg_mask_loss = total_mask_loss / (total_batches +1)\n",
    "\n",
    "        print(f\"Average Loss: {avg_loss:.4f},Average next sentence loss: {avg_next_sentence_loss:.4f}, Average Mask Loss : {avg_mask_loss:.4f}\")\n",
    "\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9a04d-ad11-4f79-ab45-761587b2e5ba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7db03340-91ae-40f9-841e-d609768c56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "\n",
    "train_dataset_path = 'bert_dataset/bert_train_data.csv'\n",
    "test_dataset_path = 'bert_dataset/bert_test_data.csv'\n",
    "\n",
    "train_dataset = BERTCSVDataset(train_dataset_path)\n",
    "test_dataset = BERTCSVDataset(test_dataset_path)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size = BATCH_SIZE, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3505367f-8701-4725-8a55-e9c848906d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b3bf8948134fc896b9d083db7bc6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24b4a15e50d49a8a6890f85bbafdc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/56847 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Average training loss: 4.7688\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'total_mask_loss' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Average training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Evaluation after each epoch\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model,test_dataloader,loss_fn_nsp, loss_fn_mlm,device)\n\u001b[1;32m     68\u001b[0m test_loss\u001b[38;5;241m.\u001b[39mappend(eval_loss)\n",
      "Cell \u001b[0;32mIn[39], line 32\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, loss_fn_mlm, loss_fn_nsp, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m         total_next_sentence_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m next_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 32\u001b[0m         total_mask_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mmlm_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     33\u001b[0m         total_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m/\u001b[39m (total_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'total_mask_loss' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay = 0.01, betas = (0.9,0.999))\n",
    "\n",
    "# Training loop setup\n",
    "num_epochs = 1\n",
    "total_steps = num_epochs* len(train_dataloader)\n",
    "\n",
    "# Define the number of warmup steps\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "\n",
    "## Create the learning rate scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps=warmup_steps,\n",
    "                                           num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "# list to store losses for plotting\n",
    "train_loss,test_loss =[],[]\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc = \"Training Epochs\"):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for step,batch in enumerate(tqdm(train_dataloader, desc = f\"Epoch {epoch+1}\")):\n",
    "\n",
    "        bert_inputs, bert_labels, segment_labels, is_next = [b.to(device) for b in batch]\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        # forward pass\n",
    "        next_sentence, masked_predict = model(bert_inputs, segment_labels)\n",
    "\n",
    "        # loss calculation\n",
    "        nsp_loss = loss_fn_nsp(next_sentence, is_next.view(-1))\n",
    "\n",
    "        mlm_loss = loss_fn_mlm(masked_predict.view(-1,masked_predict.size(-1)),bert_labels.view(-1))\n",
    "\n",
    "        loss = nsp_loss + mlm_loss\n",
    "        # backward calculation\n",
    "        loss.backward()\n",
    "\n",
    "        # clipping the grad\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm = 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss +=loss.item()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "        else:\n",
    "            total_loss +=loss.item()\n",
    "\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)+1\n",
    "    train_loss.append(avg_train_loss)\n",
    "    print(f\"Epoch: {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluation after each epoch\n",
    "    eval_loss = evaluate(model,test_dataloader,loss_fn_nsp, loss_fn_mlm,device)\n",
    "    test_loss.append(eval_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ed8f3-0dbf-41b9-b9bd-e55a77a14224",
   "metadata": {},
   "source": [
    "## Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3e1e509-85af-4d68-aa9c-1bbf7578afb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs), train_loss, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs), test_loss, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/pyplot.py:3903\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mscatter)\n\u001b[1;32m   3885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[1;32m   3886\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3901\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3902\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PathCollection:\n\u001b[0;32m-> 3903\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mscatter(\n\u001b[1;32m   3904\u001b[0m         x,\n\u001b[1;32m   3905\u001b[0m         y,\n\u001b[1;32m   3906\u001b[0m         s\u001b[38;5;241m=\u001b[39ms,\n\u001b[1;32m   3907\u001b[0m         c\u001b[38;5;241m=\u001b[39mc,\n\u001b[1;32m   3908\u001b[0m         marker\u001b[38;5;241m=\u001b[39mmarker,\n\u001b[1;32m   3909\u001b[0m         cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[1;32m   3910\u001b[0m         norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[1;32m   3911\u001b[0m         vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[1;32m   3912\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax,\n\u001b[1;32m   3913\u001b[0m         alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[1;32m   3914\u001b[0m         linewidths\u001b[38;5;241m=\u001b[39mlinewidths,\n\u001b[1;32m   3915\u001b[0m         edgecolors\u001b[38;5;241m=\u001b[39medgecolors,\n\u001b[1;32m   3916\u001b[0m         plotnonfinite\u001b[38;5;241m=\u001b[39mplotnonfinite,\n\u001b[1;32m   3917\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m   3918\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3919\u001b[0m     )\n\u001b[1;32m   3920\u001b[0m     sci(__ret)\n\u001b[1;32m   3921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[1;32m   1474\u001b[0m             ax,\n\u001b[1;32m   1475\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args),\n\u001b[1;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: sanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/axes/_axes.py:4787\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4785\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n\u001b[0;32m-> 4787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4790\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal.classic_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m          mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "plt.scatter(range(1,num_epochs), train_loss, label = 'Training Loss', color = 'blue')\n",
    "plt.scatter(range(1,num_epochs), test_loss, label = 'Testing Loss', color = 'orange')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Testing Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255dd92-d53e-489f-899a-081584589f48",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b63fb211-c4c9-4c0b-8b8c-ff7bffadec1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 10, 2, 2, 0.1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size,d_model, n_layers,nheads,dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f68af712-21f6-4a90-8717-7253144c7b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-04 22:41:36--  https://f-courses-data.s3.us.cloud-object-storage.appdomain.cloud/H04Cs7O75aOfmJ4YP2HdPw.pt\n",
      "Resolving f-courses-data.s3.us.cloud-object-storage.appdomain.cloud (f-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(88898) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.45.118.108\n",
      "connected. to f-courses-data.s3.us.cloud-object-storage.appdomain.cloud (f-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... \n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2025-08-04 22:41:38 ERROR 404: Not Found.\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'H04Cs7O75aOfmJ4YP2HdPw.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m BERT(vocab_size,d_model,n_layers,nheads,dropout)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwget \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://f-courses-data.s3.us.cloud-object-storage.appdomain.cloud/H04Cs7O75aOfmJ4YP2HdPw.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH04Cs7O75aOfmJ4YP2HdPw.pt\u001b[39m\u001b[38;5;124m'\u001b[39m,map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'H04Cs7O75aOfmJ4YP2HdPw.pt'"
     ]
    }
   ],
   "source": [
    "model = BERT(vocab_size,d_model,n_layers,nheads,dropout)\n",
    "!wget 'https://f-courses-data.s3.us.cloud-object-storage.appdomain.cloud/H04Cs7O75aOfmJ4YP2HdPw.pt'\n",
    "model.load_state_dict(torch.load('H04Cs7O75aOfmJ4YP2HdPw.pt',map_location=torch.device('cpu')))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f86f0ae-d837-46e4-b085-506433c8a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer with the BERT model's vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def predict_nsp(sentence1,sentence2,model,tokenizer):\n",
    "    # tokenize sentences with special tokens\n",
    "    tokens = tokenizer.encode_plus(sentence1,sentence2,return_tensors = 'pt').to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
    "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nsp_prediction,_ = model(tokens_tensor, segment_tensor)\n",
    "\n",
    "\n",
    "        # Select the first element (first sequence) of the logits tensor\n",
    "        first_logits = nsp_prediction[0].unsqueeze(0) # adds an extra dimension, making it [1,2]\n",
    "        logits = torch.softmax(first_logits, dim = 1)\n",
    "        prediction = torch.argmax(logits, dim = 1).item()\n",
    "\n",
    "    return \"second sentence follows the first\" if prediction==1 else \"Second sentence does not follow the first\"\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb44e6df-9c9b-496a-bdc3-250f2a4b04c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second sentence follows the first\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The cat sat on the mat\"\n",
    "sentence2 = \"It was a sunny day\"\n",
    "\n",
    "print(predict_nsp(sentence1,sentence2,model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c100488-b917-450b-bf7c-0ec45bce6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mlm(sentence, model, tokenizer):\n",
    "\n",
    "    inputs = tokenizer(sentence, return_tensors = \"pt\").to(device)\n",
    "\n",
    "    tokens_tensor = inputs.input_ids.to(device)\n",
    "\n",
    "    model.to(device)\n",
    "    # Create dummy segment labels filled with zeros, assuming it's needed by yoour model\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Forward pass through the model, now correctly handling the output tuple\n",
    "        output_tuple = model(tokens_tensor, segment_labels)\n",
    "\n",
    "        # Assuming the second element of the tuple contains the MLM logits\n",
    "        predictions = output_tuple[1]\n",
    "\n",
    "        # Identify the position of the [MASK] token\n",
    "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple = True)[1]\n",
    "\n",
    "        # get the predicted index for the [MASK] token from the MLM logits\n",
    "        predicted_index = torch.argmax(predictions[0,mask_token_index.item().item(), :], dim = -1)\n",
    "        \n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
    "\n",
    "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token,1)\n",
    "\n",
    "    return predicted_sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ddd7a7ad-5661-4979-8784-6161a3440bde",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cat sat on the [MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_mlm(sentence, model, tokenizer))\n",
      "Cell \u001b[0;32mIn[63], line 13\u001b[0m, in \u001b[0;36mpredict_mlm\u001b[0;34m(sentence, model, tokenizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create dummy segment labels filled with zeros, assuming it's needed by yoour model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Forward pass through the model, now correctly handling the output tuple\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     output_tuple \u001b[38;5;241m=\u001b[39m model(tokens_tensor, segment_labels)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Assuming the second element of the tuple contains the MLM logits\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m output_tuple[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 39\u001b[0m, in \u001b[0;36mBERT.forward\u001b[0;34m(self, bert_inputs, segment_labels)\u001b[0m\n\u001b[1;32m     36\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m (bert_inputs \u001b[38;5;241m==\u001b[39m PAD_IDX)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# generate embeddings from inputs and segment labels\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m my_bert_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_embedding(bert_inputs,segment_labels)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m## Pass embeddings through the transformer\u001b[39;00m\n\u001b[1;32m     43\u001b[0m transformer_encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(my_bert_embedding,src_key_padding_mask \u001b[38;5;241m=\u001b[39m padding_mask)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mBERTEmbedding.forward\u001b[0;34m(self, bert_inputs, segment_labels)\u001b[0m\n\u001b[1;32m     13\u001b[0m my_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(bert_inputs)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain:\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(my_embeddings \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(my_embeddings)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_embedding(segment_labels))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m my_embeddings \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(my_embeddings)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "sentence = \"The cat sat on the [MASK]\"\n",
    "print(predict_mlm(sentence, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d66889-a5d3-40d3-85b5-db00239aeac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b909d-0c1a-476d-a499-3dc3fc171228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab207fa4-0ced-499b-80a0-9706a8aa827d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c43a42-2e80-440d-a549-ec991215be3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486a2d7-c938-41ee-badb-046e3ef17d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
