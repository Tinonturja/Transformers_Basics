{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "610a8600-585f-498f-8838-7d8baf0ee3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6b83f5-b441-432e-a1e0-57f6b7bfdd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2025.4.26)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a6139d-bf50-42e3-af74-b83e12ed0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchtext.datasets import multi30k,Multi30k\n",
    "from torch.nn.utils.rnn import pad_sequence # New\n",
    "\n",
    "from torchdata.datapipes.iter import IterableWrapper,Mapper # New\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time # New\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def warn(*args,**kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff2a5e0-5e8b-4695-9474-8134cb6941e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_xh = torch.tensor(-10.0)\n",
    "w_hh = torch.tensor(10.0)\n",
    "b_h = torch.tensor(0.0)\n",
    "x_t =1\n",
    "h_prev = torch.tensor(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d55b9f0-db79-48db-9ab0-ca3f9e199b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1,1,-1,-1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79d23f-db35-4c51-967a-87d42c9a1985",
   "metadata": {},
   "source": [
    "Assuming that you start from the intial state $h = 0$,  with the above input vector $x$, the state vector $h$ should look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7357e06-640e-48b1-8bce-7112451359a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=[-1,-1,0,1,0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1f43c7-5ea9-4320-a9f7-c7261c690e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1\n",
      "h_t-1 -1\n",
      "h_t:-1.0\n",
      "\n",
      "t = 2\n",
      "h_t-1 -1.0\n",
      "h_t:-1.0\n",
      "\n",
      "t = 3\n",
      "h_t-1 -1.0\n",
      "h_t:0.0\n",
      "\n",
      "t = 4\n",
      "h_t-1 0.0\n",
      "h_t:1.0\n",
      "\n",
      "t = 5\n",
      "h_t-1 1.0\n",
      "h_t:0.0\n",
      "\n",
      "t = 6\n",
      "h_t-1 0.0\n",
      "h_t:-1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the predicted state values\n",
    "H_hat = []\n",
    "\n",
    "# Loop through each data point in the input sequence X\n",
    "t =1\n",
    "\n",
    "for x in X:\n",
    "    # assign the current data point to x_t\n",
    "    print(\"t =\",t)\n",
    "    x_t = x\n",
    "\n",
    "    # Print the value of the previous state (h at time t-1)\n",
    "    print(\"h_t-1\",h_prev.item())\n",
    "\n",
    "\n",
    "    # Compute the current state (h at time t) using the RNN formaula with tanh activation\n",
    "    h_t = torch.tanh(x_t*w_xh +h_prev * w_hh +b_h)\n",
    "\n",
    "\n",
    "    # update the h_prev to the current state value for the next iter\n",
    "    h_prev = h_t\n",
    "\n",
    "    # print the current input value (x at time t)\n",
    "    print(f\"h_t:{h_t.item()}\\n\")\n",
    "\n",
    "    ## Append the current state value to the H_hat list after converting it to the integer\n",
    "    H_hat.append(int(h_t.item()))\n",
    "\n",
    "    t+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518c8370-cec7-4991-ab00-a5f00f1c59e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-1, -1, 0, 1, 0, -1], [-1, -1, 0, 1, 0, -1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_hat,H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f760df4-5366-4de9-a0c8-6c5156dc4fb1",
   "metadata": {},
   "source": [
    "## Sequence2Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f84cb-f8f5-4137-94b6-1ab19e4688c8",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe6bfaef-6dde-4811-808f-4023c4b7b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,vocab_len,emb_dim,hid_dim,n_layers,dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim,hid_dim,n_layers,dropout = dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self,input_batch):\n",
    "\n",
    "        # input_batch = [src_len, batch_size]\n",
    "\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "\n",
    "        embed = embed.to(device)\n",
    "\n",
    "        # outputs = [src_len, batch_size, hid_dim*n directions]\n",
    "        # hidden = [n_layers * n directions, batch_size, hid_dim]\n",
    "        # cell = [n_layers * n directions, batch_size, hid dim]\n",
    "\n",
    "        outputs, (hidden,cell) = self.lstm(embed)\n",
    "\n",
    "        return hidden,cell\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e38ae4-5da7-4411-8ad2-dc94614e47a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Encoder(torch.nn.Module):\\n\\n    def __init__(self,vocab_size,embed_dim,hid_dim,n_layers,dropout_prob):\\n\\n        super().__init__()\\n\\n        self.dropout = nn.Dropout(dropout_prob)\\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\\n        self.lstm = nn.LSTM(embed_dim,hid_dim,n_layers,dropout = dropout_prob)\\n\\n\\n    def forward(self,input_batch):\\n        # input batch size = [each_sentence_length,batch_size]\\n        embed = self.dropout(self.embedding(input_batch))\\n\\n        embed = embed.to(device)\\n\\n        outputs, (hidden,cell) = self.lstm(embed)\\n\\n        return hidden,cell'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,embed_dim,hid_dim,n_layers,dropout_prob):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim,hid_dim,n_layers,dropout = dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self,input_batch):\n",
    "        # input batch size = [each_sentence_length,batch_size]\n",
    "        embed = self.dropout(self.embedding(input_batch))\n",
    "\n",
    "        embed = embed.to(device)\n",
    "\n",
    "        outputs, (hidden,cell) = self.lstm(embed)\n",
    "\n",
    "        return hidden,cell'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654c0e1-34d5-4a6a-af41-c1ac3f9130d4",
   "metadata": {},
   "source": [
    "Now create an encoder instance to see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "22fbdd4c-5206-4824-82cf-50709c142a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = 8\n",
    "emb_dim = 10\n",
    "hid_dim = 8\n",
    "n_layers = 1\n",
    "dropout_prob = 0.5\n",
    "device = torch.device('cpu')\n",
    "\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa9093e8-59da-4361-af36-8e3d92297e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input (src) tensor: torch.Size([5, 1])\n",
      "Hidden tensor from encoder: tensor([[[-0.5584,  0.1026, -0.0183,  0.0380, -0.0150, -0.0235,  0.0213,\n",
      "           0.0561]]], grad_fn=<StackBackward0>)\n",
      "Cell tensor from encodder: tensor([[[-0.8698,  0.3432, -0.0218,  0.0572, -0.0221, -0.1568,  0.0439,\n",
      "           0.0870]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "src_batch = torch.tensor([[0,3,4,2,1]]) # Input of single token\n",
    "# you need to transpose the input tensor as the encoder LSTM is in sequence_first mode\n",
    "src_batch = src_batch.t().to(device)\n",
    "\n",
    "## Input sequence has two options: 1) batch first & 2) Sequence first\n",
    "### In case of Sequence first input tensor should be transposed, otherwise not\n",
    "\n",
    "print(f\"Shape of input (src) tensor: {src_batch.shape}\")\n",
    "\n",
    "hidden_t, cell_t = encoder_t(src_batch)\n",
    "print(f\"Hidden tensor from encoder: {hidden_t}\\nCell tensor from encodder: {cell_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f79126-657c-4434-a7c1-1cb7759e1d43",
   "metadata": {},
   "source": [
    "The encoder takes the input sentences which consists of a sequence of words or tokens. The encoder LSTM process the entire input sequence and updates it hidden states at each time step. The hidden states of the LSTM network act as a form of memory and capture the contextual information of the input sequence. After processing the entire input sequence, the final hidden state of the encoder LSTM captures the summarized representation of the input sequences's context. This final hidden state is sometimes referred to as the `\"Context vector\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7971619e-4218-486d-8810-0ba290478eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 8]), torch.Size([1, 1, 8]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_t.shape,cell_t.shape,src_batch.shape \n",
    "\n",
    "# Shape verification\n",
    "# input_shape = [5,1] --> one sentence consists of 5 words--> batch_size =1, src_len = 5\n",
    "# output_shape = [] --> src_len, batch_size, hid_dim*n_directions --> 5,1,8\n",
    "# hidden_shape = [] --> n_layers*n_direction, batch_size , hid_dim --> 1,1,8\n",
    "# cell_shape = [] --> 1,1,8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fb15e-3b0e-4cdd-ba54-812fb23c735f",
   "metadata": {},
   "source": [
    "## Decoder Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c818b-4630-47c0-9288-6e3166a0cd06",
   "metadata": {},
   "source": [
    "The init method contains the following parameters:\n",
    "\n",
    "`num_layers` is the number of layers in LSTM\n",
    "\n",
    "`hid_dim:` is the dimensionality of the hidden state\n",
    "\n",
    "`emb_dim:` features of embedding vector (is the dimensionality of the embedding layer)\n",
    "\n",
    "`dropout:` dropout random neuron to prevent overfitting\n",
    "\n",
    "`output_dim:` Number of possible outputs (target vocabulary length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8288d-9d34-4920-83d1-89880e46973e",
   "metadata": {},
   "source": [
    "Decoder contains the following layers:\n",
    "\n",
    "`embedding:`\n",
    "\n",
    "`lstm` An LSTM layer that takes the embedded input and produces hidden states of size hid_dim\n",
    "\n",
    "`fc_out:` A linear layer that maps the LSTM output to the output dimension output_dim\n",
    "\n",
    "`softmax:` A long softmax activation function applied to the output to obtain a probability distribution over the output values \n",
    "\n",
    "`dropout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3e3ea685-9ffd-41f1-94cc-8ed3c75c9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,emb_dim,hid_dim,n_layers,output_dim,dropout_prob):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(emb_dim,hid_dim, n_layers,dropout = dropout_prob)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim,output_dim)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input,hidden,cell):\n",
    "\n",
    "        \n",
    "        # input = [batch_size]\n",
    "\n",
    "        # hidden_dim = [n_layers * n_directions, batch_size, hid_dim]\n",
    "\n",
    "        # cell_dim = [n_layers * n_directions, batch_size, hid_dim]\n",
    "\n",
    "        # n_direction in decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch_size, hid dim]\n",
    "        # context = [n layers, batch_size, hid_dim]\n",
    "\n",
    "        if input.dim() ==1:\n",
    "            input = input.unsqueeze(0)\n",
    "        elif input.dim()==0:\n",
    "            input = input.view(1,1)\n",
    "            \n",
    "        # input = [1,batch_size, emb dim] # creates the input a 3D shaped array\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        print(f\"embeded shape: {embedded.shape}\")\n",
    "\n",
    "        # embedded = [1,batch_size, emb_dim]\n",
    "\n",
    "        output, (hidden,cell)  = self.lstm(embedded, (hidden,cell))\n",
    "\n",
    "        # output = [seq_len, batch_size, hid_dim * n_direction]\n",
    "\n",
    "        # hidden = [n_layers * n_direction,batch_size,hid_dim]\n",
    "\n",
    "        # cell = [n_layers * n_direction,batch_size,hid_dim]\n",
    "\n",
    "        prediction_logit = self.fc_out(output.squeeze(0))\n",
    "\n",
    "        prediction  = self.softmax(prediction_logit)\n",
    "\n",
    "        # prediction = [batch_size, output_dim]\n",
    "\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "90d8cc9e-3d71-4dc0-8b43-7eaece627274",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 6\n",
    "emb_dim = 10\n",
    "hid_dim = 8\n",
    "n_layers = 1\n",
    "dropout = 0.5\n",
    "decoder_t = Decoder(emb_dim, hid_dim,n_layers, output_dim,dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ee164cd5-180f-483e-8864-280e9d8e5d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_t = torch.tensor([0]).to(device)\n",
    "\n",
    "input_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6c61ac1f-7e0e-4b19-8b8d-1f612a20923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeded shape: torch.Size([1, 1, 10])\n",
      "Prediction: tensor([[-2.1600, -1.8589, -1.5200, -2.2193, -1.5989, -1.6130]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "Hidden: tensor([[[-0.0759, -0.3959, -0.3013, -0.1028, -0.1011, -0.1888,  0.2365,\n",
      "           0.0567]]], grad_fn=<StackBackward0>)\n",
      "Cell: tensor([[[-0.4043, -0.5529, -0.7236, -0.2843, -0.1467, -0.3228,  0.3428,\n",
      "           0.1406]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction, hidden, cell  = decoder_t(input_t,hidden_t,cell_t)\n",
    "\n",
    "print(f\"Prediction: {prediction}\\nHidden: {hidden}\\nCell: {cell}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da03a7-0431-469a-bc91-221e11d1ac7b",
   "metadata": {},
   "source": [
    "Now that you have instances of both encoder and decoder, you are ready to connect them (the red box in the diagram below). First, let's see how you can pass the Hidden and Cell (the pink cell within the red box) from encoder (the green boxes container) to decoder (the orange boxes container). Looking at the diagram, you can see that the decoder also receives an input which is the previous word that it has predicted. For the first decoder cell, this input is `<bos>` token. Each decoder cell outputs a prediction and updates the cell and state to pass to the next decoder cell. prediction is a probability distribution over possible target tokens (length of target vocab).\n",
    "\n",
    "![connection](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/ED_connection.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b7ed5-4d17-4a87-b1ae-425eaeb4bcad",
   "metadata": {},
   "source": [
    "## Encoder-Decoder connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9c111890-3e69-47fe-98e3-dcf132437064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeded shape: torch.Size([1, 1, 10])\n",
      "embeded shape: torch.Size([1, 1, 10])\n",
      "embeded shape: torch.Size([1, 1, 10])\n",
      "embeded shape: torch.Size([1, 1, 10])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-2.0548, -1.7087, -1.5305, -2.2095, -1.8622, -1.5641]],\n",
      "\n",
      "        [[-2.0555, -1.7544, -1.5032, -2.2261, -1.8190, -1.5779]],\n",
      "\n",
      "        [[-1.9344, -1.8208, -1.4639, -2.2149, -1.8498, -1.6305]],\n",
      "\n",
      "        [[-1.9220, -1.6845, -1.4960, -2.1895, -2.0248, -1.6089]]],\n",
      "       grad_fn=<CopySlices>) torch.Size([5, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "# trg = [trg_len, batch_size]\n",
    "# teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
    "# if teacher_forcing_ratio is 0.75 you use ground truth inputs 75% of the time\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)\n",
    "\n",
    "batch_size =trg.shape[1]\n",
    "trg_len = trg.shape[0]\n",
    "\n",
    "trg_vocab_size = decoder_t.output_dim\n",
    "\n",
    "\n",
    "# tensor to store decoder outputs\n",
    "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size) # words_per_sentence, number_of_sentence, output_vocab_size\n",
    "\n",
    "\n",
    "# send to device\n",
    "hidden_t = hidden_t.to(device)\n",
    "cell_t = cell_t.to(device)\n",
    "\n",
    "# first input to the decoder is the <bos> tokens\n",
    "input = trg[0,: ]\n",
    "\n",
    "\n",
    "for t in range(1, trg_len):\n",
    "\n",
    "    # loop through the trg len and generate tokens\n",
    "    # decoder receives previous generated token, cell and hidden\n",
    "    # decoder outputs it prediction (probability distribution for the next token) and updates hidden and cell\n",
    "\n",
    "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
    "\n",
    "    # place predictions in a tensor holding predictions for each token\n",
    "    outputs_t[t] = output_t\n",
    "\n",
    "    # decide if you are going to use teacher forcing or not\n",
    "    teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "    # get the highest predicted token from your predictions\n",
    "    top1 = output_t.argmax(1)\n",
    "\n",
    "    # if teacher forcing, use actual next token as next input\n",
    "    # if not, use predicted token\n",
    "    # input = trg[i] if teacher_force else top1\n",
    "    input = trg[t] if teacher_force else top1\n",
    "\n",
    "\n",
    "print(outputs_t,outputs_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5a2618a1-cdbd-4be0-a980-2ae897305c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_  = trg[0,:]\n",
    "input_,trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "84ff3e7b-f2a7-4f35-907b-253901d41852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3, 4],\n",
       "         [5, 6, 7, 8]]),\n",
       " torch.Size([2, 4]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_chk = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "r_chk,r_chk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "68b1ae78-b7c5-47e4-9db9-b3c4fc1ec4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_chk[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d2336a28-3daa-4218-870f-47adac444686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-2.0548, -1.7087, -1.5305, -2.2095, -1.8622, -1.5641]],\n",
       " \n",
       "         [[-2.0555, -1.7544, -1.5032, -2.2261, -1.8190, -1.5779]],\n",
       " \n",
       "         [[-1.9344, -1.8208, -1.4639, -2.2149, -1.8498, -1.6305]],\n",
       " \n",
       "         [[-1.9220, -1.6845, -1.4960, -2.1895, -2.0248, -1.6089]]],\n",
       "        grad_fn=<CopySlices>),\n",
       " torch.Size([5, 1, 6]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_vocab_size,outputs_t,outputs_t.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6b53b88f-ffb1-4a8a-a6f9-8c8092720c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg = torch.tensor([[0],[2],[3],[5],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1d5f8c3d-925c-413e-a041-6db7d8467eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [2],\n",
       "         [3],\n",
       "         [5],\n",
       "         [1]]),\n",
       " torch.Size([5, 1]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg,trg.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d2420f2e-6c12-458d-8fd1-a9e8b8b9a78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "620a5fe7-4aec-4c18-a018-228a9e411291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self,encoder, decoder, device, trg_vocab):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder \n",
    "        self.device = device\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim\n",
    "        \"Hidden dimensions of encoder and decoder must be equal\"\n",
    "        assert encoder.n_layers == decoder.n_layers\n",
    "        \"Encoder and decoder must have equal number of layers\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio =0.5):\n",
    "\n",
    "        ## src = [src len, batch_size]\n",
    "        ## trg = [trg len, batch_size]\n",
    "        ## teacher_forcing_ratio is 0.75, you use ground truth inputs 75% of the time\n",
    "\n",
    "        trg_batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        ## tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, trg_batch_size, trg_vocab_size).to(device)\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        hidden = hidden.to(device)\n",
    "        cell  = cell.to(device)\n",
    "\n",
    "        # first input to the decoder is the <bos> tokens\n",
    "        input = trg[0,:] # skip the first row, that represents the first token\n",
    "\n",
    "        for t in range(1,trg_len): # Passing through each words\n",
    "\n",
    "            # loop through the trg len and generate tokens\n",
    "            # decoder receives previous generated token, cell and hidden\n",
    "            # decoder outputs it prediction probability distribution for next token and updates hidden and cel\n",
    "\n",
    "            output,hidden,cell = self.decoder(input,hidden,cell)\n",
    "\n",
    "            # place prediction in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            # decide if you are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            top1 = output.argmax()\n",
    "\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "714fc863-5a0b-45cc-b6f6-5f32ce009914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, loss_fn, optimizer, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # wrap iterator with tqdm for progress logging\n",
    "    train_iterator = tqdm(iterator, desc = \"Training\",leave = False)\n",
    "\n",
    "    for i, (src,trg) in enumerate(iterator):\n",
    "\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # trg [trg_len, batch_size]\n",
    "        # output [trg_len, batch_size, output_dim]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1,output_dim)\n",
    "\n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "        # trg = [(trg_len-1)* batch_size]\n",
    "        # output = [(trg len-1)* batch_size, output dim]\n",
    "\n",
    "        loss = loss_fn(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(),clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # update tqdm progress bar with the current loss\n",
    "        train_iterator.set_postfix(loss = loss.item())\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "    return epoch_loss /len(list(iterator))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef348ab5-2b7b-4855-abe8-ba194b47c4cf",
   "metadata": {},
   "source": [
    "## Data Evaluation / Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f8ccb-c63e-4e5c-9111-2fc88c5e9328",
   "metadata": {},
   "source": [
    "You also need to define a function to evaluate the model. Let's go through the code and understand its components:\n",
    "\n",
    "1. `evaluate(model, iterator, criterion)` takes three arguments:\n",
    "   - `model` is the neural network model that will be evaluated.\n",
    "   - `iterator` is an iterable object that provides the evaluation data in batches.\n",
    "   - `criterion` is the loss function that measures the model's performance.\n",
    "* Note that evaluate function do not perform any optimization on the model.\n",
    "\n",
    "2. The function starts by setting the model to evaluation mode with `model.eval()`.\n",
    "\n",
    "3. It initializes a variable `epoch_loss` to keep track of the accumulated loss during the evaluation.\n",
    "\n",
    "4. The function enters a `with torch.no_grad()` block, which ensures that no gradients are computed during the evaluation. This saves memory and speeds up the evaluation process since gradients are not needed for parameter updates.\n",
    "\n",
    "5. The function iterates over the evaluation data provided by the `iterator`. Each iteration retrieves a batch of input sequences (`src`) and target sequences (`trg`).\n",
    "\n",
    "6. The input sequences (`src`) and target sequences (`trg`) are moved to the appropriate device (e.g., GPU) using `src = src.to(device)` and `trg = trg.to(device)`.\n",
    "\n",
    "7. The model is then called with `output = model(src, trg, 0)` to obtain the model's predictions for the target sequences. The third argument `0` is passed to indicate that teacher forcing is turned off during evaluation.  During evaluation, teacher forcing is typically turned off to evaluate the model's ability to generate sequences based on its own predictions.\n",
    "\n",
    "8. The `output` tensor has dimensions `[trg len, batch size, output dim]`. To calculate the loss, the tensor is reshaped to `[trg len - 1, batch size, output dim]` to remove the initial `<bos>` (beginning of sequence) token, which is not used for calculating the loss.\n",
    "\n",
    "9. The target sequences (`trg`) are also reshaped to `[trg len - 1]` by removing the initial `<bos>` token and making it a contiguous tensor. This matches the shape of the reshaped `output` tensor.\n",
    "\n",
    "10. The loss between the reshaped `output` and `trg` tensors is calculated using the specified `criterion`.\n",
    "\n",
    "11. The current batch loss (`loss.item()`) is added to the `epoch_loss` variable.\n",
    "\n",
    "12. After all the batches have been processed, the function returns the average loss per batch for the entire evaluation, calculated as `epoch_loss / len(list(iterator))`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b38d6f6e-0770-45ba-bc99-ad726c588647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(iterator, model ,loss_fn):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    test_iterator = tqdm(iterator,desc= \"Testing\",leave = False)\n",
    "\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (src,trg) in enumerate(test_iterator):\n",
    "    \n",
    "            src, trg = src.to(device),trg.to(device)\n",
    "    \n",
    "            output = model(src, trg,0) # set the teacher forcing ratio as 0\n",
    "    \n",
    "            # trg_dim -> [trg_len,batch_size]\n",
    "            # output -> [trg_len,batch_size,output_dim]\n",
    "    \n",
    "            # to calculate for loss calculation, we should reshape the trg and output dimention\n",
    "            output_dim = output[-1]\n",
    "    \n",
    "            # trg = [trg_len-1*batch_size]\n",
    "            # output = [trg_len-1*batch_size, output_dim]\n",
    "            trg = trg[1:].contiguous().view(-1)\n",
    "    \n",
    "    \n",
    "            output = output[1:].view(-1,output_dim)\n",
    "    \n",
    "            loss = loss_fn(output,trg)\n",
    "\n",
    "            test_iterator.set_postfix(loss = loss.item())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        return epoch_loss/len(list(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130eea98-4df1-420e-a4f7-d4f7384fc425",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee0530-abfd-4921-a0a8-ce0ac67d8432",
   "metadata": {},
   "source": [
    "Get a translation dataset called Multi30K, collate it (tokenization, numericallization, and adding BOS/EOS and padding) and create iterable batches of src and trg tensors\n",
    "\n",
    "This leverages the predefined collate_fn to efficiently curate and ready batches for training the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "349b5706-5543-44f8-b33b-42b3e442e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!brew install wget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "40e0ae98-abc8-4d8e-bfd0-2d81b802f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "df4cf091-151b-4dbe-9776-ee6454278569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Multi30K_de_en_dataloader import get_translation_dataloaders\n",
    "\n",
    "train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8879f7d3-2950-43b6-91f7-c35a1e69a08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    2,     2,     2,     2],\n",
       "         [    3,  5510,  5510, 12642],\n",
       "         [    1,     3,     3,     8],\n",
       "         [    1,     1,     1,  1701],\n",
       "         [    1,     1,     1,     3]]),\n",
       " tensor([[   2,    2,    2,    2],\n",
       "         [   3, 6650,  216,    6],\n",
       "         [   1, 4623,  110, 3398],\n",
       "         [   1,  259, 3913,  202],\n",
       "         [   1,  172, 1650,  109],\n",
       "         [   1, 9953, 3823,   37],\n",
       "         [   1,  115,   71,    3],\n",
       "         [   1,  692, 2808,    1],\n",
       "         [   1, 3428, 2187,    1],\n",
       "         [   1,    5,    5,    1],\n",
       "         [   1,    3,    3,    1]]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src,trg = next(iter(train_dataloader))\n",
    "\n",
    "src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c30bef93-cf2b-428d-9643-bfb7317e7b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([11, 4]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape, trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3a5ce2a8-d813-4c90-bab7-b7f666ec3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Multi30K_de_en_dataloader import index_to_german,index_to_eng,vocab_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f67d302f-a340-4790-aa7c-c1bca46f89d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________\n",
      "german\n",
      "<bos> Personen mit schwarzen Hüten in der Innenstadt . <eos>\n",
      "<bos> Eine Gruppe Menschen protestiert in einer Stadt . <eos>\n",
      "<bos> Eine Gruppe teilt ihre politischen Ansichten mit . <eos>\n",
      "<bos> Mehrere Personen sitzen an einem felsigen Strand . <eos>\n",
      "____________________\n",
      "english\n",
      "<bos> People in black hats gathered together downtown . <eos> <pad> <pad> <pad>\n",
      "<bos> A group of people protesting in a city . <eos> <pad> <pad>\n",
      "<bos> A group is letting their political opinion be known . <eos> <pad>\n",
      "<bos> A group of people are sitting on a rocky beach . <eos>\n",
      "___________________\n",
      "german\n",
      "<bos> Zwei sitzende Personen mit Hüten und Sonnenbrillen . <eos>\n",
      "<bos> Ein kleiner Junge mit Hut beim Angeln . <eos>\n",
      "<bos> Diese zwei Frauen haben Spaß im Giorgio's . <eos>\n",
      "<bos> Zwei kleine Kinder schlafen auf dem Sofa . <eos>\n",
      "____________________\n",
      "english\n",
      "<bos> Two people sitting in hats and shades . <eos> <pad> <pad> <pad>\n",
      "<bos> A young boy in a hat is fishing by himself . <eos>\n",
      "<bos> These two women is at Giorgio 's having fun . <eos> <pad>\n",
      "<bos> Two young children are asleep on a couch . <eos> <pad> <pad>\n",
      "___________________\n",
      "german\n",
      "<bos> Zwei junge Mädchen marschieren in einem Umzug . <eos>\n",
      "<bos> Eine Frau läuft vor einer gestreiften Wand . <eos>\n",
      "<bos> Ein Mann fährt Jet-Ski auf dem Ozean . <eos>\n",
      "<bos> Die städtischen Straßenbahnen an einem sonnigen Tag . <eos>\n",
      "____________________\n",
      "english\n",
      "<bos> Two young girls walk in a parade . <eos> <pad> <pad> <pad> <pad>\n",
      "<bos> A woman is running in front of a striped wall . <eos> <pad>\n",
      "<bos> A man rides a jet ski across the ocean . <eos> <pad> <pad>\n",
      "<bos> The urban trolly 's of a city on a sunny day . <eos>\n"
     ]
    }
   ],
   "source": [
    "data_itr = iter(train_dataloader)\n",
    "\n",
    "# moving forward in the dataset to reach sequences of longer length for illustration purpose. \n",
    "\n",
    "for n in range(1000):\n",
    "    german,english = next(data_itr)\n",
    "\n",
    "    # source and target, from german to convert English\n",
    "\n",
    "\n",
    "for n in range(3):\n",
    "    german,english = next(data_itr)\n",
    "    german = german.T\n",
    "\n",
    "    english = english.T\n",
    "\n",
    "    print(\"___________________\")\n",
    "    print(\"german\")\n",
    "\n",
    "    for g in german:\n",
    "        print(index_to_german(g))\n",
    "\n",
    "    print(\"____________________\")\n",
    "    print(\"english\")\n",
    "\n",
    "    for e in english:\n",
    "        print(index_to_eng(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674593f1-9a32-40b7-a4e5-a36996ddd0f3",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390769f-9ff5-4fd3-b680-a7a9140f06e2",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5e12b791-e8d4-47ac-99df-4843c4a822d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.mps.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930023d-13cc-4989-83b6-2426da33fbce",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b547818-3b08-49ea-af0a-9f89e6cab2c2",
   "metadata": {},
   "source": [
    "enc = Encoder(INPUT_DIM ,ENC_EMB_DIM, HID_DIM , N_LAYERS, ENC_DROPOUT)\n",
    "This line creates an instance of the Encoder class, which represents the encoder component of the Seq2Seq model.\n",
    "\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "line creates an instance of the Decoder class, which represents the decoder component of the Seq2Seq model\n",
    "\n",
    "model = Seq2Seq(enc,dec,device, trg_vocab = vocab_transformation['en']).to(device)\n",
    "This line creates an instance of the seq2seq class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d2ec5cc7-c76c-4d5d-bb72-a0c90de04526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19214"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transform[\"de\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6cbe0fdd-18f6-415b-9e24-f4bf5c2e0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_vocab = vocab_transform[\"de\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "af1d0690-7e99-45ff-b9a9-a09feaefa585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2501"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_vocab['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e5c35c6d-89f9-4831-82d1-75a82788f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform['de'])\n",
    "OUTPUT_DIM = len(vocab_transform['en'])\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.3\n",
    "DEC_DROPOUT = 0.3\n",
    "\n",
    "\n",
    "enc = Encoder(INPUT_DIM,ENC_EMB_DIM,HID_DIM,N_LAYERS,ENC_DROPOUT)\n",
    "dec = Decoder(DEC_EMB_DIM,HID_DIM,N_LAYERS,OUTPUT_DIM,DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc,dec,device,trg_vocab = vocab_transform['en'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2bb97483-24f2-45a8-86e6-9e7de5e6e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_vocab = vocab_transform['de']\n",
    "english_vocab = vocab_transform['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dce906a2-983b-49ec-932d-05a56501b0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19214, 10837)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transform['de']),len(vocab_transform['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "22f3d5f2-38dd-4350-bee6-2c5a0e85f407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7638, 0, 48, 231, 0]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_vocab(['meine','bruder','ist','sehr','klug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "84b3d80f-4af0-4e13-ad38-72966eea212f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2227, 2025, 10, 268, 10187]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_vocab(['my','brother','is','very','smart'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bed1ab-704a-483b-9152-310d039c04a1",
   "metadata": {},
   "source": [
    "def init_weights(m): take model as input. The purpose of this function is to initialize the weights of the nn module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "76183661-b07c-4e8a-b9ba-77cd322367f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(19214, 128)\n",
       "    (lstm): LSTM(128, 256, dropout=0.3)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(10837, 128)\n",
       "    (lstm): LSTM(128, 256, dropout=0.3)\n",
       "    (fc_out): Linear(in_features=256, out_features=10837, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (trg_vocab): Vocab()\n",
       ")"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "\n",
    "    for name,param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08,0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "916870a0-79c9-42e6-a75a-f8c94c8fa6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odel has 7422165 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"The odel has {count_parameters(model):} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "061bdae2-7a44-4e17-9c40-cbdd9d9c7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b2b8e433-06bd-451e-b867-6ee0ab491c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time-start_time\n",
    "    elapsed_mins = int(elapsed_time/60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins,elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "89f12641-0efe-45c4-96c7-1cfedcb223a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebddc7e1ff1b44dfba6c71cc669475d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeded shape: torch.Size([1, 4, 128])\n",
      "embeded shape: torch.Size([1, 1, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 1, 256), got [1, 4, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m     17\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 19\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model,train_dataloader, loss_fn,optimizer,CLIP)\n\u001b[1;32m     20\u001b[0m     train_ppl \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(train_loss)\n\u001b[1;32m     22\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39mevaluate(valid_dataloader,model,loss_fn)\n",
      "Cell \u001b[0;32mIn[163], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, loss_fn, optimizer, clip)\u001b[0m\n\u001b[1;32m     14\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src, trg)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# trg [trg_len, batch_size]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# output [trg_len, batch_size, output_dim]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[162], line 44\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m trg[\u001b[38;5;241m0\u001b[39m,:] \u001b[38;5;66;03m# skip the first row, that represents the first token\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,trg_len): \u001b[38;5;66;03m# Passing through each words\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# loop through the trg len and generate tokens\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# decoder receives previous generated token, cell and hidden\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# decoder outputs it prediction probability distribution for next token and updates hidden and cel\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     output,hidden,cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;28minput\u001b[39m,hidden,cell)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# place prediction in a tensor holding predictions for each token\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     outputs[t] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[150], line 50\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden, cell)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeded shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedded\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# embedded = [1,batch_size, emb_dim]\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m output, (hidden,cell)  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(embedded, (hidden,cell))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# output = [seq_len, batch_size, hid_dim * n_direction]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# hidden = [n_layers * n_direction,batch_size,hid_dim]\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# cell = [n_layers * n_direction,batch_size,hid_dim]\u001b[39;00m\n\u001b[1;32m     58\u001b[0m prediction_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:874\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    871\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m--> 874\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:790\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    785\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    786\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    787\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    788\u001b[0m                        ):\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m--> 790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    791\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    793\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:259\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    257\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[0;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 1, 256), got [1, 4, 256]"
     ]
    }
   ],
   "source": [
    "torch.mps.empty_cache()\n",
    "\n",
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "train_PPLs = []\n",
    "valid_PPLs = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model,train_dataloader, loss_fn,optimizer,CLIP)\n",
    "    train_ppl = math.exp(train_loss)\n",
    "\n",
    "    valid_loss =evaluate(valid_dataloader,model,loss_fn)\n",
    "\n",
    "    valid_ppl = math.exp(valid_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time,end_time)\n",
    "\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'RNN-TR-model.pth')\n",
    "\n",
    "\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_PPLs.append(train_ppl)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_PPLs.append(valid_ppl)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b765e6-7381-4b6f-a60f-eb7d36ffd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!install tqdm\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e538a-727b-49a8-9665-35a02316ed68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429833c-da93-485e-89da-14ec3fd51c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d15502-2b29-4f94-ad68-b5e69248fad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d7fd1-4b74-45f4-9450-f9b41a9166d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669015ee-d68a-4e89-b07c-5ebb4564ad11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43bca8-0185-41bf-a662-999679ab8356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
